# *Librerias*

## Indice:

* [os](#os)
* [pathlib](#pathlib--path)
* [1. math](#1-math)
* [2. statistics](#2-statistics-"st")
* [3. random](#3-random-"rd")
* [4. numpy](#4-numpy-"np")
* [5. re](#5-re-regex)
* [6. pandas](#6-pandas-"pd")
* [7. openpyxl](#7-openpyxl)
* [8. matplotlib](#8-matplotlib-"plt")
* [9. seaborn](#9-seaborn-"sns")
* [10. plotly](#10-plotly-"px")
* [11. collections](#11-collections-"clt")
* [12. selenium](#12-selenium)
* [13. BeautifulSoup](#13-beautifulsoup-"bs")
* [14. scrapy](#14-scrapy-"scp")
* [15. request](#15-request)
* [16. scipy](#16-scipy-stats)
* [17. TextBlob](#17-textblob-"tb")
* [18. nltk](#18-nltk)
* [19. spacy](#19-spacy)
* [20. sklearn](#20-sklearn)
* [21. TensorFlow](#21-tensorflow)


## [os](#indice)
El módulo os de Python le permite a usted realizar operaciones dependiente del Sistema Operativo como crear una carpeta, listar contenidos de una carpeta, conocer acerca de un proceso, finalizar un proceso, etc. Esta librería trabaja retornando valores de **Strings** con el **shell** activado o en modo True (el cual puede ser un riesgo latente).

```python
import os
```

Es importante aclarar que python tiene como carácter reservado el " **\\** " por ello debemos reemplazar el símbolo " **/** " o digitar un doble " **\\** **\\** " para anularlo u lea como un string. Otra manera es también pegar la dirección tan cual antecediéndolo con un r-String. Aquí las siguientes soluciones:

 ``"C:/Users/Foster-PC/"``<br>
 ``"C:\\Users\\Foster-PC\\"``<br>
 ``r"C:\Users\Foster-PC\"`` #Esta es la solucionas más sencilla pegamos la dirección y le agregamos r'

Otra manera de convertir el carácter a " **/** ".

```python
(Variable_path).replace("\\","/")
```

Para mover carpetas y archivos a una ruta en especifica usamos:
```python
import shutil
shutil.move('C:/Users/Foster-PC/Desktop/Archivo.py', 'C:/Users/Foster-PC/Desktop/Nueva Carpeta') #Carpeta Existente.
```
* CMD: 
  * Ejecutar un script de python:
  ```cmd
          C:\Users\Foster-PC\Documents\Visual Studio Code\Learning>py nombre.py
  ```
  * Comandos del sistema:
    * **`cls`** : Limpia la consola. También puede usarse `clear`
    * **`cd`** : Entrar y movernos a un directorio. Para entrar a carpetas cuyo nombre tiene espacios en blancos se debe escribir el nombre entre comillas. ***>cd "Tutoriales Internet"***
      ```
                >cd ("Tutoriales Internet/Tutorial 0") #para ingresar por más directorios a la vez.
      
      ```
    * **`cd../`** : Salir de un directorio. Ejm: ***cd../../../*** (Regresara 3 directorios hacia atrás.); ***cd/*** (Saldrá hasta la raíz "C:")
      ```python
                ('./../practica') # De la ubicación actual ("./") Retrocede un directorio ("../")
      
      ```
    * **`pwd`** : Muestra el directorio donde nos encontramos actualmente.
    * **`dir`** : Muestra el contenido de un directorio archivos y carpetas (DIR).
    * **`tree`** : Muestra el contenido y sub carpetas en forma de árbol raíz.
    * **`mkdir`** : Crea una carpeta en el directorio actual donde nos ubicamos. También puede usarse `md`
    * **`rmdir`** : remueve una carpeta en el directorio actual donde nos ubicamos. También puede usarse `rd`
    * **`del`** : Elimina un archivo.abc o una carpeta en el directorio actual donde nos ubicamos.
  
___
### Clase, Función:

* **`os.system()`** : Ejecuta comandos al sistema (cmd). Ejm: ('dir'). A diferencia de la librería ``subprocess`` aquí si se puede ejecutar los comandos tal cual se digitan con sus espacios. Es necesario ejecutarlo en archivos `.py` para ver el resultado en la consola. Usa el path actual por defecto:
```
               os.system('py --version')           
```
* **`os.listdir()`** : Crea una lista con los archivos encontrados en un directorio. Podemos indicar dentro del paréntesis que carpeta queremos que haga la lista de sus archivos contenidos. Arranca buscando del directoria actual donde nos encontramos. Esta función es muy usada para enlistar archivos.
```python
          os.listdir('C:/Users/Foster-PC/Documents/Visual Studio Code/Learning/Beat Data - Tutorials/sales')
          #Creara una lista con todos los archivos .csv dentro de la ruta indicada.
```
Otra manera partiendo de la Ruta de Trabajo
```python
          files = os.listdir('../files') #Lista con el nombre de cada archivo.csv
          #los 2 puntos indican una carpeta antes
          df=pd.DataFrame() #vacío
          for x in files:
              file=pd.read_csv('files/'+x) #Directorio de cada archivo.
              df=pd.concat([file,df]) #añade cada Dataframe creado.
```

* **`os.getcwd()`** : Muestra la ruta del archivo python en el que estamos trabajando (Ruta de trabajo). Lo muestra en Formato **" \ "** por ello debemos utilizar un `.replace("\\","/")` para usarlo en una cadena lógica de comandos.
* **`os.chdir()`** : Este método cambia el directorio de trabajo actual a la ruta dada. 
  ```python
          path = "C:/GoogleDrive/06/archivos_unificar

          os.chdir(path)
          # %pwd o os.getcwd() para verificar que nuestra ruta se ha movido.
  ```
* **`os.path.join()`** : La función `.join()` es usada para que diferentes sistemas operativos puedan correr el código tanto como en windows ( \ ) como en Mac o Linux ( / ). En conclusion Esta función **arroja una ruta** como resultado.
```python
          os.path.join(os.getcwd(), 'Carpeta') #Del directorio añadirá '/Carpeta' a la ruta.
          #Esto no crea una carpeta con el nombre 'Carpeta'.
```
* **`os.mkdir()`** : Creará una nueva carpeta, utiliza el path o directorio actual por defecto. Necesitaremos indicarle la ruta adentro incluyendo la carpeta que queremos crear considerando usar " / "
```python
          os.mkdir(os.path.join('./../../','Nueva_Carpeta')) #leerá una ruta adentro donde creara la carpeta.
          os.mkdir('C:/Users/Foster-PC/Desktop/Nueva Carpeta') #Podemos poner la ruta entera también.
```
* **`os.makedirs()`** : Creará varias carpetas según la ruta que le especifiquemos. Utiliza el path o directorio actual por defecto. Necesitaremos indicarle la ruta adentro incluyendo la carpeta que queremos crear considerando usar " / ".
```python
          os.makedirs(os.path.join('C:/Users/Foster-PC/Desktop','Nueva Carpeta','Carpeta Hijo'))
          #creara "Carpeta Hijo" dentro de "Nueva Carpeta". Creara a ambas.
```
* **`os.rename()`** : Renombrará a una carpeta. (nombre del archivo, nuevo nombre).
```python
          for file in os.listdir():
            if file.endswith('.csv'):
              os.rename(file, f'2021_{file}')
```
* **`os.path.exists()`** : Muestra con un booleano (True or False) si un archivo.py o una carpeta existe.
* **`os.path.abspath()`** : Muestra la ruta absoluta de la carpeta o archivo.py en la ruta de trabajo que le indiquemos.
* **`os.path.realpath()`** : Muestra la ruta real donde se encuentra la carpeta o archivo.py que le indiquemos. (resuelve los enlaces simbólicos) Por ello suelen usar:
```python
          os.path.abspath(os.path.realpath(__file__))
          #Muestra la ruta incluyendo las carpetas y el archivo.py

```
* **`os.path.splitext()`** : Dividida a un archivo por su nombre y extension.
  ```python
            archivo, ext = os.path.splitext(archivo)
  ```

___
### Métodos:
___
### Propiedad:

* **`os.extsep`** : Referencia al "." (lo ubica) que divide la extensión de un archivo.
  ```python
            print("hola.abc".split(os.extsep))
            #Cortara por la extension del archivo "."
  ```

## [pathlib > Path](#indice)
Esta Librería, al igual que con `os` permite trabajar con nuestro directorio y comandos del sistema, ya sea creando carpetas en nuestro sistema, etc. La diferencia con `os` es que `pathlib` los resultado que arroja no son de formato `str` sino que son de formato `path`.

Es importante aclarar que esta librería leerá las rutas de los directorios con el siguiente carácter " **/** " :

 ``C:/Users/Foster-PC/``

Python 3 incluye el módulo ``pathlib`` para manipular rutas de sistemas de archivos de forma agnóstica en cualquier sistema operativo. El módulo ``pathlib`` es similar al os.path, pero ``pathlib`` ofrece una interfaz de nivel más alto, y, a menudo, más conveniente, que os.path. [Tutorial pathlib](https://www.digitalocean.com/community/tutorials/how-to-use-the-pathlib-module-to-manipulate-filesystem-paths-in-python-3-es) <br><br>

```python
from pathlib import Path
```
La contraparte de la función ´os.path.join()´ es PurePath para tener compatibilidad de código con diferentes sistemas operativos.
La diferencia es que como `PurePath()` es una clase esta la podemos usar para encapsular todo los comandos bajo la clase `Path()`, haciendo de esta manera todo nuestro código con `Path()` compatible con cualquier sistema operativo.
```python
from pathlib import PurePath
```
___
### Clase, Función:

Normalmente suelen guardarse en una variable la clase `Path('ruta')` con la ruta de la carpeta o archivo.py a trabajar.

* **`PurePath(Path())`** : Convertirá el código `Path` compatible con cualquier sistema operativo.

* **`Path('C:Ruta/')`** : Esta clase representa **una ruta** del sistema de archivos. Indicaremos los directorios en donde queremos que se situé el `Path`. El **Path** se encontrara por defecto en nuestra Ruta de trabajo. Con respecto a las funciones y métodos no importa donde este ubicado el `Path()` siempre se ejecutaran los comando de la ruta que indiquemos.
* **`Path.cwd()`** : Muestra la ruta del archivo python en el que estamos trabajando. Ruta de trabajo

___
### Métodos:
Muchos de los métodos arrojaran como resultado un generador de objeto. Para poder ver el resultado necesitamos encerrar el código en una **clase** **`list()`**.

* **`.iterdir()`** : Crea una lista con los archivos encontrados en un directorio (Path() toma por defecto el path actual de nuestro trabajo). Podemos indicar dentro del paréntesis de la clase **`Path('Carpeta_Prueba')`** que carpeta queremos que haga la lista de sus archivos contenidos. Arranca buscando del directoria actual donde nos encontramos. Debido a que el resultado es formato ``Path`` necesitamos pasarlo a formato ``list``. Esta método devuelve en formato path una lista con los elementos a diferencia de `os.listdir` que devuelve ya los elementos dentro de una lista. 
```python
              list(Path('Carpeta_prueba').iterdir()) #list es solo para verlo por pantalla.
              list(Path.iterdir(Path('C:/Users/Foster-PC/Documents/Visual Studio Code/Learning/Tutoriales Internet/Tutorial 0')))
              #Esta es otra manera de indicar el Path. Ejecutara entorno a la ruta.
```
* **`.joinpath()`** : Esta función **arroja una ruta** como resultado. Puede usarse con la función ``Path`` o ``PurePath`` con este ultimo funcionará el código en varios sistemas operativos. En conclusion Esta función **arroja una ruta** como resultado uniendo (Path(), "nombre") *(Por mas que creamos una ruta con nombres con directorios que no existan no creara las carpetas hasta que usemos el método mkdir() o mkdir(parents=True) para mas de 1 carpeta.)*
```python
          print(Path.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # Path.joinpath(Path(),'Nueva Carpeta')
          print(PurePath.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # PurePath.joinpath(Path(),'Nueva Carpeta')
          print(PurePath.joinpath(Path.cwd(),'Carpeta'))
```

* **`.mkdir()`** : Crea una carpeta. Propiedades:
  *  ``(exist_ok= True or False)`` : Indicara si ya existe una carpeta con ese nombre, el código no se rompa y no arroje error sino lo omita.
    ```python
              print(Path('Nueva_carpeta').mkdir(exist_ok=True)) #usando el path actual.

              print(Path('C:/Users/Foster-PC/Desktop/Nueva Carpeta').mkdir(exist_ok=True))
               #Especificando el path con el nombre de la carpeta a crear.
              print(PurePath.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # PurePath.joinpath(Path(),'Nueva Carpeta')
    ```

  *  ``(parents= True or False)`` : Creara mas de una carpeta con el parámetro en `True`. Para lo cual nuestra ruta debe contener una carpeta nueva con otra que sea hijo (dentro de carpeta nueva).
    ```python
              print(Path.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta','Carpeta Hijo').mkdir(exist_ok=True,parents=True))
    ```
* **`Path.rename()`** : Cambia el nombre de path1 a path2.
```python
          Path.rename(Path('C:/Users/Foster-PC/Desktop/Nueva Carpeta'),Path('C:/Users/Foster-PC/Desktop/New Directory'))
```
* **`Path.rmdir()`** : elimina la carpeta situado en el path indicado.
```python
          Path.rmdir(Path('C:/Users/Foster-PC/Desktop/New Directory'))
```
* **`Path.exists(Path())`** : Indica con un booleano si el path indicado existe.
* **`Path.resolve(Path())`** : Arroja la ruta absoluta de la carpeta o archivo.py indicado previamente en la ruta.
* **`Path.stem(Path())`** : Arroja solo el nombre, sin la extension, de la carpeta o archivo.py indicado previamente en la ruta.
* **`Path.suffix(Path())`** : Arroja solo la extension de los archivo.py indicado previamente en la ruta.
* **`Path.stat(Path()).st_size`** : Arroja el tamaño de los archivo.py indicado previamente en la ruta.

___
### Propiedad:

## [1. math](#indice)
Es una libreria que brinda funciones para operaciones matemáticas clásicas como: **`.log`**

## [2. statistics ("*st*")](#indice)
Es una libreria que brinda funciones para calculos estadisticos como: **`.mean`**, **`.median`**, **`.mode`**, **`.stdev`**, **`.pstdev`**, **`.variance`**, **`.pvariance`**, etc. 

## [3. random ("*rd*")](#indice)
Es una libreria que ofrece generadores de números pseudo-aleatorios para varias distribuciones:

* **`.randrange()`** : Selecciona un item de manera aleatoria indicado (1,7)
* **`.randint()`** : Crea números aleatorios en el rango especificado. Ejemplo para un Array de numpy:
```python
    Array_1 = np.random.randint(1, 100, size=1000)
    #Creara un array con valores del 1-100 con 1000 datos dentro del array.
```
* **`.uniform()`** : Crea números aleatorios de números decimales en el rango especificado. Ej: (1, 100, size=1000)

## [4. numpy ("*np*")](#indice)
**Numeric Python**. Es una libreria que brinda la funcion de trabajar con matrices, algebra lineal, estadistica para analisis de datos.<br>
Esta libreria solo permite trabajar con datos homogeneos bien sea con numeros o con letras. Y trabaja solo con indices "0" ahi es donde entra `pandas`.
Incluye la libreria **random**.

* Cabe resaltar que para numpy cada cierre de **"`[]`"** es una fila.
* El punto de origen de las matrices es la coordenada **(0,0)** Empezando por fila y columna.
  
### Tipos de Data:
* **`np.int64`**
* **`np.float32`**
* **`np.complex`**
* **`np.bool`**
* **`np.object`**
* **`np.string`**
* **`np.unicode_`**
---     
### Indexado:
*  Con **[x,y]** traera de un dato entre filas y columnas (coordenada)
*  Con **[[x,y]]** traera las filas específicas señaladas.
*  Con **[x:y]** traera de un rango continuo de solo filas.
*  Con **[x:y,0]** traera de un rango de filas la primer columna.
*  Con **[x:y,[0,2]]** Delimitara por columnas (traera 2 primeras columnas contando con la columna "0") 
---
### Clase, Funciones:
Ciertas funciones pueden pasar a ser métodos depende a quien esten modificando.

* **`np.array()`** : Crea una matriz de una sola dimension o una fila **([ ])**
* **`np.mat()`** : Crea una matriz de 2 dimensiones *('1 2;3 4')*
* **`np.arange()`** : Funciona exactamente igual que la clase `range`. Es un intervalo dado.
* **`np.linspace()`** : A diferencia de `arange` indicamos la cantidad de divisiones de nuestro array. Es un intervalo Especificado.
* **`np.unique()`** : Retorna valores unicos y de manera ordenada.
* **`np.where()`** : Es una manera mas sencilla de escribir condicional "**if else**". `(modificacion,si cumple,no cumple)`
* **`np.choice()`** : Elige aleatoriamente valores de una lista o set de datos, también podemos obtener muestras aleatorias de una matriz unidimensional y devolver las muestras aleatorias de una matriz numpy.
    ```python
    indice = np.random.choice(np.arange(len(X_train)),24 , replace=False)
    ```
* **`np.array_split()`** : Particiona las filas de una matriz o DataFrame en partes iguales creando más dfs con las divisiones.
    ```python
        #(df,nºpartes a divir filas)
        partes_df = np.array_split(df,10) #creara 10 df con igual nº de filas

    ```
* **`np.zeros(3) `** : Crea un array de "0"s: array([0., 0., 0.]).
---
### Metodos de Numpy:
* **`.reshape()`** : Esto ordena a una matriz de 2 dimensiones que deseamos sin modificar la matriz original. *ejm: (4,5) 4 filas 5 columnas*. (Van al final de otro método ligado)
* **`.resize()`** : Al igual que `.reshape` ordena una matriz pero reemplazando a la matriz original
* **`.astype()`** : Cambia el tipo de data. *(int o float)* 

#### Metodos Estadisticos:
* **`.mean()`**
* **`.sum()`**
* **`.cumsum()`**
* **`.min()`**
* **`.max()`**
* **`.std()`**
* **`.var()`**
* **`.corrcoef()`**
---
### Propiedades:
Estas no usan parentesis ni modifican el valor ni forma del original.

* **`.ndim`** : Indica la dimension bien 1 o 2.
* **`.shape`** : Indica la cantidad de listas y datos dentro de ellas. *(filas [ ] , columnas)*
* **`.dtype`** : Indica el tipo de dato *(int32 o float64)*
* **`.size`** : Indica la cantidad de datos que hay en la matriz.
* **`.itemsize`** : Indica la cantidad de bytes que se va a almacenar.
* **`.T`** : Transpone la forma de una matriz sin modificar el original. Ejm: matriz 2x4 --> 4x2

---
### Parámetro

Van ligados a una función o método por una **"`,`"**

* **`,end=', '`** : Indica que el resultado este separado por comas y un espacio.
* **`,num=`** : indica el numero de segmentaciones para el método `.linspace`
* **`,axis=`** : Dirige el procesamiento a filas o columnas de la matriz. **0** para **filas** y **1** para **columnas**.
* **`,dtype=`** : Indica el tipo de dato *(int32 o float64)*
---
### Arreglos 
Son funciones únicas de `np` para lo cual las llamaremos con **`%`**.<br>
Estos son usadas en **DataScience** debido a que son mas rápidas en ejecución que una lista convencional a la hora de procesar grandes volúmenes de datos u operaciones.

* **`%timeit`** (variable) : asigna un contador a la operación de la variable que estamos creando.

###  Aplicaciones Estadisticas con Pandas:

* **`np.log(df)`** : Este parámetro aplicamos una transformación logarítmica a los datos indicados. Debido a que los datos presentan asimetría positiva y no sigue la línea diagonal. Cabe resaltar que para aplicar la `transformación logarítmica` es necesario que no haya datos con valores "0".
  
    ```python
        # Aplicamos la transformación logarítmica a "SalePrice".
        df['SalePrice'] = np.log(df['SalePrice']) 

        #Verificamos la mejoría de los datos y su alineación. 
        from scipy import stats   #Para el gráfico .probplot(df)
        from scipy.stats import norm  #Para el gráfico de distplot

        sns.distplot(df['SalePrice'], fit=norm) #Fit=norm : Ajusta la distribución a una normal.
        fig = plt.figure()

        res = stats.probplot(df['SalePrice'],plot=plt)
    ```

## [5. re (regex)](#indice)

**Regular Expressions** Es una librería para trabajar con expresiones regulares. Estas son verificaciones por patrones o parametros de busqueda. *(cap 8.4)* [Regex101](https://regex101.com/)

### Funciones:
Las funciones **r-strings** tienen el siguiente formato:<br>
        `.funcion(comando a verificar,variable o string,parametro=x)`

* **`re.fullmatch()`** : Verifica si coincide el **string completo** en la variable alojada o parametros especificados ((variable o parametros digitados,"string")). Imprime el resultado en forma de Expresion.<br><br>

* **`re.search()`** : Similar a `.fullmatch()` este verifica la coincidencia de almenos una cadena del string, hace una busqueda para un dato en especifico. Imprime el resultado en forma de Expresion. Opciones de impresion, es necesario un método `.group()` o `.groups()` para mostrar la cadena que coincidio.<br>  ***,flags=re.IGNORECASE** indica que ignore las mayusculas en la verificacion de los strings*. 

```python
        busca3 = re.search("GUIDO","Guido Van Rossum",flags = re.IGNORECASE)
        busca3.group() if busca3 else "no se encontro"

        # Listar en la carpeta solo archivos .csv
        data =  [x for x in os.listdir('files') if re.search(".csv$",x)]
        data
```
* **`re.findall()`** : Extrae del texto todos los datos con el formato que le indiquemos. A diferencia de `.search()` extrae datos para más de una busqueda de manera simultanea. Imprime el resultado
```python
        usuario = 'Tel_casa:52-1234-1234, Celular:52 4321 4321'
        re.findall(r'\d{2}-\d{4}-\d{4}|\d{2} \d{4} \d{4}',usuario)
        # | : Indica "or"

```
* **`re.finditer()`** : Es exactamente lo mismo que `.findall()` solo que  Imprime el resultado en forma de expresión.

* **`re.sub()`** : Al igual que `.replace()` Remueve el texto que le indiquemos, la diferencia es que este lo hace diferenciando texto y expresiones regulares.<br> ***,count=(nº)** indica cantidad de reemplazos al string*.
```python
        re.sub(r"\n",",","Salto 1\nSalto 2\nSalto 3",count=1)
```
* **`re.split()`** : Al igual que `.split()` segmenta el texto que le indiquemos. Podemos indicarle mediante expresiones regulares los espacios, tabulaciones, etc, que hallemos impreso en el texto.<br> ***,maxsplit=(nº)** indica cantidad de segmentaciones al string*.<br>
```python
        quitar_espacios = re.sub(r"\n+",",","W\nX\n\nY\n\n\nZ") #'W,X,Y,Z'
        re.split(r',\s*',quitar_espacios)  #Añadirá una coma con espacio: ['W', 'X', 'Y', 'Z']
```

---
### Métodos:

* **`.group()`** : Imprime el dato sacado del string en forma de un grupo total.
* **`.groups()`** : Imprime el dato sacado del string de manera segmentada ya sea conjuntos de texto o caracteres.
---
### Expresiones:

Las expresiones regulares, en general contienen símbolos especiales llamados meta caracteres como
\, @, #, $ * y signos de agrupación como [ ], { }, ( )
En consecuencia, resulta necesario utilizar los r-strings.

#### - Metacaracteres:
* "`r''`" : Abre una Expresion regular llamado **r-string**, seguido va '  '. Las "," separan los parametros para cada argumento a digitar.
* "`\d`" : Digitos (0-9). indica que verificara solo números enteros en el string, {(nºdigitos)}; 4 a mas digitos{4,}; entre 8 y 10 digitos {8,10}
* "`\s`" : Espacio en blanco.
* "`[ ]`" : verificara si el parametro escrito dentro se verifica con la variable. Los corchetes no necesitan usar condicional "|". Ejm: [A-Z][a-z]
* "` * `" : Indica verificar caracter por caracter del "string".
* "`+`" : Verifica cualquier tipo de expresion sucesiva que cumpla almenos con un parametro indicado anteriormente. Ejm: con un "\n" que encuentre elimina en conjunto la cadena "\n\n\n"
* "` ^ `" : Por si solo indica mostrar el inicio de una cadena de caracteres (string), pero tambien indica encontrar todos los demás caracteres que no están dentro de corchetes.. Ejm: "[^a-z]"
* "` `" : Separa un conjunto ya sea palabras o numeros de otros tal cual se digita el string. Ejm (parametro para texto) (parametro para numero)  --> calle 3876<br><br>
  
* "``\d``"    : Digitos (0-9)
* "``\D``"    : No digitos (0-9)
* "``\w``"    : Caracter de palabra (a-z, A-Z, 0-9, _)
* "``\W``"    : No caracter de palabra
* "``\s``"    : Espacio en blanco (espacio, tab, nueva linea)
* "``\S``"    : No espacio en blanco (espacio '/s', tab '/t', nueva linea '/n')
* "``.``"     : Cualquier caracter excepto nueva linea (codicioso - greedy)
* "``\``"     : Cancela caracteres especiales
* "``^``"     : Inicio de una cadena de caracteres (string)
* "``$``"     : Fin de una cadena de caracteres, se indica al final del caracter. ejm Hola$  

#### - Cuantificadores:

* "``*``"     : 0 o más (codicioso - greedy)
* "``+``"     : 1 o más (codicioso - greedy)
* "``?``"     : 0 or 1 (perezoso - lazy)
* "``{3}``"   : Numero exacto
* "``{n,}``"  : Numero n+
* "``{3,4}``" : Rango de números (Minimo, Maximo), ignorara todo lo que no lo cumpla.
* "``( )``"   : Grupos. Usan condicional "|".
* "``[]``"    : Encuentra caracteres en corchetes. Los corchetes no necesitan usar condicional "|".
* "``[^ ]``"  : Encuentra todos los demás caracteres que no están dentro de corchetes.
* "``|``"     : Condicional O
* "``\b``"    : Se posiciona a los limite de palabras y caracteres
* "``\B``"    : Se posiciona omitiendo los limites de palabras y caracteres.
* "``\1``"    : Referencias. Simula no tener que repetir lo digitado muchas veces, para cadenas de más de un caracter es necesario agruparlos "( )"
---
### Parámetros:

* "**``,flag=re.M``**"  : Interpreta cada linea del texto como un string independiente a pesar que el texto este en su conjunto como un solo string.
* "**``,flags=re.I ó ,flags=re.IGNORECASE``**"  : Interpreta las mayusculas y las minusculas como parte del mismo texto. "HOLA" = "hola"
* "**``,count=(nº)``**"  : indica cantidad de reemplazos al string, para la funcion **`.sub()`**
* "**``,maxsplit=(nº)``**"  : indica cantidad de segmentaciones al string, para la funcion **`.split()`**

## [6. pandas ("*pd*")](#indice)
**Panel Data**. Es una librería que brinda la función de importar documentos `.csv` y usarlos como Dataframes. Es para manejar tablas.<br>
Pandas es una evolución de **numpy** por lo que muchos de sus métodos funcionan aquí:<br>
*Indexados, Métodos Estadísticos*<br>
También incorpora las librerías de: `re`, `datetime`

```python
  import pandas as pd

  pd.set_option('display.max_columns', None) # Muestra todas las columnas del df.

```

Para la carga y exportación de archivos según su tipo de archivo o extension: Tener en cuenta que al exportar con el mismo nombre el archivo se sobrescribirá

  * Archivos `.csv`
    * Para cargar archivos **`.csv`** (tiene que estar guardado como .cvs utf-8 (delimitado por comas)))<br>
    ```python
        variable = pd.read_csv('./../carpeta/nombre de archivo.csv', sep=";", encoding="utf-8", usecols=[0,2,3], skiprows=[1,4],
                               index_col=0, parse_dates=["Fecha","Creado El"])
    ```    
      *El ["Fecha","Creado El"] punto indica en la carpeta actual*<br>
      *Los 2 puntos seguidos indica saltar una carpeta hacia atrás*
      
      * **``,usecols=``** : Columnas que quiero que se muestren, las indicamos según su indice.
      * **``skiprows=[0,1,2]``** : Omitirá cargar filas que le indiquemos.
      * **``index_col=0``** : Podemos usar una columna[indice] como index para el df.
      * **``parser_dates=True``** : Convertirá a formato fecha las columnas que lo sean. O indicando (parser_dates="Column1","Column2")<br><br>
    * Para exportar a formato **`.csv`**, obviando los índices, con encoding **`utf-8`**:
    ```python
        df_tarjetas.to_csv("terrernos2.csv", index=False, encoding="utf-8")

        #En caso queramos exportar en varios archivos un df dividido:
        path = r'C:\Users\foster\Desktop\files"
        partes_df = np.array_split(df,10) #dividimos las filas el df en 10 df con filas iguales

        for ix, df in enumerate(partes)
          df.to_csv(path + "\files" + str(ix+1).zfill(2) + "csv",index=false)
          #Creara archivos con el nombre empezando con nombre_s01.csv
    ```
  * Archivos `.xlsx`
    * Para cargar archivos Excel **`.xlsx`** Tiene que instalarse con un pip install **``openpyxl``** si queremos cambiarlo por otro usaremos ``(,engine="xlrd")``.<br>
    Podemos leer todo el archivo o solo una pestaña especifica con el atributo **`,sheet_name=`** en caso nuestro archivo contenga varias pestañas, también podemos asignarle cada pestaña a una variable en caso asi lo deseemos.
        ```python
        variable = pd.read_excel('./../carpeta/nombre de archivo.xlsx', sheet_name="Nombre_Pestaña_Excel",usecols=[0,2,3],
                                  skiprows=[0,1,2], startrow=4, index_col=0, parser_dates=["Fecha","Hora"])     
        ```
      * **``,sheet_name=``** : puede tener extension "nombre.csv". Igualando a "None" mostrara en un diccionario todos los "Sheet" del cuaderno Excel.
      * **``,usecols=``** : Columnas que quiero que se muestren, las indicamos según su indice.
      * **``skiprows=[0,1,2]``** : Omitirá cargar filas que le indiquemos.
      * **``startrow=4``** : Indica a partir de que fila arme el DataFrame.
      * **``index_col=0``** : Podemos usar una columna[indice] como index para el df.
      * **``parse_dates=True``** : Convertirá a formato fecha las columnas que lo sean. O indicando (parser_dates="Column1","Column2")<br><br>
      
    * Para Revisar rápidamente solo las pestañas de excel **`.xlsx`**.
    ```python
              df = pd.read_excel("C:/Users/Foster-PC/Documents/G&S Instalaciones/PLANTILLAS/BALDOSAS Y DRYWALL/PLANTILLA BYD 2021.xlsx", None)
              df.keys() #De esta manera arrojará solo el nombre de las pestañas.
    ```
    * Para exportar a formato excel **`.xlsx`**. No importa la extension del archivo lo convertirá a excel (.xlsx).
    ```python
        df_tarjetas.to_excel("terrernos2.csv", sheet_name="Hoja1")
        #Podemos exportar en una nueva hoja de nuestro mismo archivo Excel.
    ```
    * Para exportar en multiples pestañas:
    ```python
        df2 = df1.copy() #En caso hagamos una copia del df. 
        with pd.ExcelWriter('output.xlsx') as writer:  
          df1.to_excel(writer, sheet_name='Sheet_name_1')
          df2.to_excel(writer, sheet_name='Sheet_name_2')
    ```
    * Para leer y  Concatenar todos los `sheet_name` de un cuaderno Excel.
    ```python
        df = pd.read_excel("terrernos2.xlsx", sheet_name= None)
        df = pd.concat(df, ignore_index=True) #para tener 1 solo index de toda la tabla.
        df.head()
    ```

  * Archivos `.json`
    * Para Cargar archivos `.json`. index,name,value
    ```python
        pd.read_json("Nombre.json", orient="columns") #columns es la que viene por defecto.
        #no hay "indent". Pero si podemos indicarle el tipo de orientación que queremos.
    ```
    * Para Exportar a `.json`. index,name,value. (indent =True or 3,4,5) Respetará la indentación (TAB).
    ```python
        pd.to_json("Nombre.json", indent=True)
         #{"index":{0:1,1:2,2:3,3:4,4:5},"name"{0:A,1:B,2:C,3:D,4:E},"value"{etc}}

        pd.to_json("Nombre.json", indent=True, orient="columns") 
        #Es el mismo que viene por defecto.

        pd.to_json("Nombre.json", indent=True, orient="index") 
        #{"0"{"index":1,"name":"A","value":3.03},"1"{"index":2,"name":"B",etc},

        pd.to_json("Nombre.json", indent=True, orient="split") 
        #{"columns"["index","name","value"],"index"[0,1,2,3,4],"value"[etc]

        pd.to_json("Nombre.json", indent=True, orient="records") 
        #[{"index":1,"name":A,"value":3.03},{"index":2,"name":B,"value":5.14}]

        pd.to_json("Nombre.json", indent=True, orient="values") 
        #[[1,"A",3.03],[2,"B",5.14],[etc]]

        pd.to_json("Nombre.json", indent=True, orient="table") 
        #"schema","Datos"...
    ```  
    * Importar Datasets (csv,etc) de Github:
      Es necesario entrar al archivo por medio de github y copiar y pegar el URL del archivo en formato **RAW**

      ```python
        url = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/archived/ecdc/total_cases.csv"
        datacov = pd.read_csv(url)
        datacov.shape
      ```


___
### Clase, funciones:
* **`pd.set_option()`** : Configura opciones de la librería de pandas.
  ```python
          pd.set_option("precision",4) 
          #Configuramos que muestre 1 decimal.

          pd.set_option("max_columns",9) 
          # Muestra 9 columnas, incluye indice.

          pd.set_option("display.width",None) 
          #None lo ajustara automáticamente.

          pd.set_option("precision",4,"max_columns",9,"display.width",None)
          #Puede Resumirse estos 3 cambios en una sola linea de código.
          
  ```
* **`pd.Series()`** : Crea una **columna** con los datos a asignarle, una Serie son columnas en pandas. Pueden contener forma de diccionarios "``{'a':b}``", Listas "``['a,b']``" o tuplas "``('a,b')``".
* **`pd.DataFrame()`** : Crea un DataFrame (tabla). Son Series de 2 dimensiones (Tablas). Se pueden armar DataFrames de diccionarios, listas y tuplas. La mejor manera de crear un `DataFrame` es usando un **diccionario** para poder referenciar sus encabezados y sus valores, ya sean listas, tuplas, etc. <br>
En Caso Tengamos una Listas de Listas `array([[          0],[          0]])` es necesario "Aplanarlas" usando el método `.flatten()`
  ```python
          df_tarjetas = pd.DataFrame({"Ubicación":ubicacion, "Precio":precio, "Area":m2})
          #({"nombre_columna",variable_con_datos}), variables previamente definidas (Listas)

          Model_Accuracy = pd.DataFrame({"Accuracy":history.history["accuracy"],
                                "Val_Accuracy":history.history["val_accuracy"]})
          #Dataframe de una Lista de Listas:
          pd.DataFrame({"Survive":predictions_out.flatten()})

          #Cuando indicamos un diccionario con un solo valor:
          files = {'A.txt':12, 'B.txt':34, 'C.txt':56, 'D.txt':78}
          filesFrame = pd.DataFrame(files.items(), columns=['filename','size'])

          #Otra manera de Crear un DataFrame:
          pd.DataFrame(predictions_out, columns=["Survive"])

          #,columns= : referencia a la columna, si ya la especificamos esta servirá para asignarle un nombre a la columna.

          pd.DataFrame(history.history["accuracy"], columns=["Accuracy"]), #de no indicar la columna no cambiara el nombre.
            
  ```
* **`pd.ExcelWriter()`** : Clase para escribir objetos "DataFrame" en hojas de Excel. (uso en exportar en multipestaña)
* **`pd.concat([datas],parameters=)`** : Concatena tablas (df) o columnas de un df una debajo de otra por defecto `,axis=0` (por fila), pero también podemos concaternarlo una tabla a lado de otra usando `,axis=1` (por columna) retornando la compilación de ellas en una nueva tabla. Va entre "**[ ]**" porque sera una lista de DataFrames ([df_1,df_2,df_3])
 
  ```python
        df = pd.concat([df_1, df_2, df_3], axis=1 ,sort=True) #Juntará todos los dataframe en uno solo. 
        #,axis=1 Alinea según la columna      
        #,sort=True1 Que lo ordene.
        #,join= default {"outer"}
        #,key= ['Total', 'Percent'] #Cambia el orden de presentación de las columnas.
        #,ignore_index= True Creara un index enumerado eliminando los demás index previos.


        df = pd.concat([df_train["SalePrice"],df_train[YearBuilt]], axis=1)
        #Concatenamos la Columna de un "df_train" con otra del mismo "df_train", guardandose esta nueva tabla en "df".

        #Creamos 2 columnas y las unimos con concat manteniendo el mismo index:
          total = df_train.isnull().sum().sort_values(ascending=False)
          percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
          missing_data = pd.concat([total, percent], keys=['Total', 'Percent'], axis=1)
          missing_data.head(20)

  ```
  * Un ejemplo para concatenar varios archivos en un ciclo ``for``:

  ```python
      #Concatenando archivos que solo tengan terminación ".csv"
      import re, os 

      data =  [x for x in os.listdir('files') if re.search(".csv$",x)] #carpeta esta en el mismo directorio de nuestro archivo .py

      df_Prueba = pd.DataFrame() #Creamos un df vacío.
      for i in data:  #variable data contiene en lista los archivos [achr1.csv,achr2.csv,achr3.csv]
          df_temp = pd.read_csv("files/" + i)   #creara un dataframe por diccionario
          df_Prueba = pd.concat([df_Prueba, df_temp])   #Juntará todos los dataframe en uno solo.      
  ```
  * Para leer y  Concatenar todos los `sheet_name` de un cuaderno Excel.
    ```python
        df = pd.read_excel("terrernos2.xlsx", sheet_name= None)
        df = pd.concat(df, ignore_index=True) #para tener 1 solo index de toda la tabla.
        df.head()
    ```
* **`pd.to_datetime()`** : Formatea una columna a formato **date**. Arrojando el formato americano año,mes,dia,hora<br> 
Al tener un dato en formato fecha tenemos la ventaja de convertir esta columna fecha en nuestro index mediante `.set_index("Col", inplace=True)` y hacer la búsqueda por fecha sea poniendo solo el año con `df.loc["2020"]` Ejm:

  ```python
      df['Order Date'] = pd.to_datetime(df['Order Date'])
      #Convierte una columna en formato Fecha.
  ```
  * Especificando el formato de fecha que tenemos en la columna, lo convertirá siempre a formato americano *año mes dia hora*:
  ```python
        df['Order Date'] = pd.to_datetime(df['Order Date'],format= "%Y-%m-%d %H:%M:%S") # "2022-01-04 11:35:50" Solo este formato con años.
        
        data["Fe, de picking"] = pd.to_datetime(data["Fe, de picking"], format= "%d/%m/%Y") #En la columna esta como dia/mes/año     
  ```

  * Para cambiar el formato americano a un formato personalizado usamos `dt.strftime()`, considerar que estamos cambiando el formato a `String`:
  ```python
      df['Order Date'] = df['Order Date'].dt.strftime("%d/%m/%y")  #dia/mes/año

  ```

  * Aumentar fechas a una fecha base:
  ```python
        pd.to_datetime(160, unit='D', origin='2020-02-01')
        #Aumente 160 días a una fecha Base.
        #origin= puedes ser también "unix"

  ```

* **`pd.PeriodIndex()`** : Permite hacer agrupaciones de fechas ya sea por dia, mes ,trimestre,año , etc. Se suele usar en los parámetros de `df.pivot_table()` y  `groupby()`.
  
  ```python
        #pd.PeriodIndex(Columna, freq='M')
        pd.PeriodIndex(df_new["Fe, de picking"], freq='M')

        #En pivot_table
        df_2 = df_new.pivot_table(index=pd.PeriodIndex(df_new["Fe, de picking"], freq='M'), values=["Cantidad ","PT"],
                                  aggfunc="sum").sort_index(ascending=True)

        #En groupby
        grupo = df_new.groupby(pd.PeriodIndex(df_new["Fe, de picking"], freq='M')).sum().sort_index(ascending=True).reset_index()
        #reset_index() creara un nuevo index enumerando el numero de filas {0,1,2,3}
  ```
  
* **`pd.get_dummies()`** : Transforma los datos de una columna categórica (columna que tiene clasificaciones) a una forma en la que python pueda entender de que no se esta aportando valor sino una clasificación. Para ello creara varias columnas según el tipo de valores únicos que encuentre en la columna que vamos a aplicar esta función llenando de valores binarios estas nuevas columnas para que python entienda en determinada fila cual de las categorías encontradas se activo.<br>
Esta clasificación se suele usar en la preparación de datos para posteriormente procesarlo con ML o Redes Neuronales.

  ```python
    df = pd.get_dummies(df["Pclass"], prefix="Pclass"), axis=1 #Tabla con los nuevos campos en binario.

    #Otro ejemplo:
    df = pd.concat([df, pd.get_dummies(df["Pclass"], prefix="Pclass")], axis=1)
    df.drop(["Pclass"], axis=1, inplace=True) #Una vez clasificado borramos la columna origen. Concatenamos por columna.

    #,prefix="Nombra_Columna" = Podemos cambiar o conservar el nombre para el cual usara como base para enumerar según la cantidad de valores únicos o categorías encuentre en esta columna. "Nombra_Columna_1", "Nombra_Columna_2", "Nombra_Columna_3 .. etc"
  ```

Para Pandas la manera común de filtrado es:
 * Filtrar por columnas: df[["Col1", "Col2"]] 
 * Filtrar por un Atributo de la columna: df[df["Sexo"] == "Masculino"] 
 * Filtrar por Varios Atributo de la columna: df[df["Clase"].isin([1,2])]

**df[Filtro = df[] operador][[Lista de columnas a mostrar en la tabla filtrada]].where(filtro_2).(quitar nulos, ordenar, +métodos)**

```python
  df[df["Creado Por"] == "JARCAN"][["Descrición Material","Tipo de Orden"]].where(df["Tipo de Orden"] == "OC").dropna()

```

___

### Métodos para DataFrames:
Estos métodos son para una variable donde este alojada el DataFrame.

Se puede llamar a una columna digitandolo entre **`['Columna']`** o digitando su indice, pero también digitandolo como atributo **`.nombre_columna`** Digitandolo por su nombre pero este ultimo no es para columnas con palabras separas por un espacio en cambio el otro por corchetes se puede llamar a más de una columna con un doble corchete: **`[['Columna_A'], ['Columna_B']]`**

* **`df.describe()`** : Trae todas las estadísticas relevantes: `count`, `mean`, `std`, `min`, `25%`, `50%`, `75%`, `max`, `dtype`.

* **`df.astype()`** : Convierte el resultado bien a ``("int","float","object","string","datetime64[ns]","bool")``
```python
          df_distribution.astype({"Cantidad ": "float"}).dtypes
          #Cambia de int a float en una columna en especifico

          df["col1"] = df["col1"].astype(str)
```
* **`df.sort_index()`** : Ordena el indice. Por defecto viene en ascendente. *(,ascending=True)*, para columnas *(,axis=1)*
* **`df.reindex()`** : Ordena el index según el orden de las columnas que le pasemos.
  ```python
        df.columns = ["a,b,c,d,e,f"]
        df = df.reindex(columns=['a','f','d','b','c','e']) #Cambiara el orden de las columnas al que indicamos. 
  ```
* **`df.set_index()`** : Indicamos que columna queremos como indice base:
```python
          df.set_index("Fechas", inplace=True)
          #Reemplazamos al df por la modificación.
```
* **`df.reset_index()`** : Corrige para que el df tenga un index aparte de las columnas.
* **`df.insert()`** : Añade una columna o lista de valores a lado derecho de la posición de columna que le indiquemos.
```python
          df.insert(loc=2, column="Número de pedido", value=df["Pedido"])
          #loc= nº columna, lo insertara a lado derecho
          #column= Nombramos la columna 
```

* **`df.sort_values()`** : Ordena los valores. Por defecto viene en ascendente. *(ascending=True)*, para columnas *(,axis=1)*
  ```python
          df.sort_values(by=["Price","Score","Link"], ascending=[True,False,False])
          # Otra manera de ordenar de menor a mayor a columna "Price". by= funciona para listas o no.

  ```
  En caso los datos o filas de la Columna estén nombrados con mayúsculas y minúsculas ("grupo A") se tiene primero que homogeneizar antes de ordenar para ello usamos el parámetro `,key= lambda x : x.str.lower()`
  ```python
          df.sort_values("Race/Ethnicity", ascending=True, key= lambda x:x.str.lower())

  ```
* **`df.shape`** : Muestra la cantidad de filas y columnas de la data. Para obtener el total de datos usamos. `df.shape[0]*df.shape[1]` *(filas * columnas)*
* **`df.size()`** : Muestra la cantidad de valores en el DataFrame. *(filas * columnas)*
* **`df.head()`** : Muestra las primeras n filas del df.
* **`df.info()`** : Muestra la información general con un `head()` incluido del df. Este método se suele usar para ver si hay **valores nulos** en las **columnas**.
* **`df.tail()`** : Muestra las ultimas n filas del df.
* **`df.rename()`** : Renombrar. Ejm: 
```python
          restdf = restdf.rename(columns={"Atendió":"Mesero"})
```
* **`df.drop()`** : Borra un dato al indicar su nombre o index para borrar columnas y solo nº de index para borrar filas.`,axis=0` para borrar fila o `,axis=1` para borrar columna.
  ```python
    #Para borrar una fila del df indicando un valor de la columna.
    df.drop(df.index[df["Cabin"]=="T"], axis=0, inplace=True)
  ```
* **`df.drop_duplicates()`** : Elimina elementos duplicados.
* **`df.dropna()`** : Borra filas o columnas enteras donde encuentre 1 datos en blanco o un total de fila o columna en blanco *(NaT,NaN)*. Por defecto borrara la filas en donde encuentra 1 dato null, es necesario usar *(inplace=True)* para aplicar el cambio.

  ```python
    df_population_raw.dropna(inplace=True, axis=0, how="any")
    #modo por defecto ,axis=0, how="any".
  ```
  * axis = 0 : fila, 1 : columna
  * how = "all" : toda la fila o columna debe estar llenas de null, "any" : basta 1 solo null que encuentre.<br><br>

* **`df.isnull() o df.isna()`** : Arroja elementos nulos, vacíos (NaN). En combinacion son un operador como `.sum()` arrojará la cantidad de valores nulos de cada columna.
  ```python
    df.isna().sum()
    df.isnull().sum()
    #Ambas formas dan el mismo resultado.

    df['Código'].isnull().unique()
    #Busca elementos nulos únicos en la columna "Código"    
    
    tabla_despv[tabla_despv["Creado Por"].isnull()]
    #Mostrara la tabla conteniendo todas las filas donde la columna tenga valor nulo.
       
  ```
* **`df.fillna()`** : Reemplaza los valores **NULL** con un valor **específico** a indicar. El método fillna() devuelve un nuevo objeto DataFrame a menos que el parámetro inplace se establezca en True , en ese caso, el método fillna() hace el reemplazo en el DataFrame original. <br>
El parámetro `method=` es utilizado para rellenar huecos en serie reindexada relleno/relleno: propagar la última observación válida hacia adelante hasta el siguiente relleno válido/bfill: utilizar la siguiente observación válida para rellenar el hueco. {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}

```python
          df.fillna(0,inplace=True)
          #Reemplazara los valores nulos con un "0"

          df.fillna(method="ffill",inplace=True)
         #Este método propaga los Nan hacia adelante

          values = {"A": 0, "B": 1, "C": 2, "D": 3}
          df.fillna(value=values,inplace=True)
          #Para cada columna (A-D) reemplazara el valor nulo con el valor indicado en el diccionario.
          
```
* **`df.iterrows()`** : Separa el Index de los datos por columna concatenandolos por cada nuevo valor de la columna.
```python
          for row, datos in restdf.iterrows(): #rows= index , datos=df concatenado con todos sus valores de las columnas.
              Orden  = datos["Orden"]
              Tipo  = datos["Tipo"]
              Producto = datos["Productos"]
```
* **`df.unique()`** : Arroja el nombre de los elementos únicos.
* **`df.nunique()`** : Arroja la cantidad de los elementos únicos.
```python
          df["Tipos"].nunique()
          # 5 (resultado forma numérica)
```
* **`df['Columna'].value_counts()`** : Arroja una lista indicando el recuento de datos (valores) únicos en la columna. Este es muy usado para explorar el contenido de las columnas de un df.
```python
          
          df[['Código','Colm_2']].value_counts()
          #arrojara un recuento de los valores únicos de os pares de datos únicos por las 2 columnas.
          
          df['Código'].isnull().value_counts()
          #Arroja buleano con conteo de elementos nulos en la columna "Código"          
```
  * `(normalize=True)` : Obtiene la "Frecuencia Relativa". Indica su proporción con respecto al total en decimales.
    ```python
          df['Código'].value_counts(normalize=True, dropna=False).round(2)
          #Ejem: Female = 0.55 ; males = 0.45; NaN = 1 (redondeado mostrando 2 decimales)
          #Con dropna establecido en False , nos dará el recuento de elementos NaN.
    ```

* **`df.apply()`** : Crea una columna calculada con la función que le indiquemos. Por lo general va compuesta por `lambda`, pero también podemos indicar una función de python o una propia que hayamos creado.
```python
          df["Dígitos"] = df["Cuenta"].apply(lambda x : len(str(x))) #cuenta nº de caracteres.
          df["Atrittion"] = df["Atrittion"].apply(lambda x : 1 if x == "Yes" else 0 ) #cuenta nº de caracteres.
```
* **`df.isin()`** : Sirve para filtrar por los valores de una columna, Retorna un booleano por lo que es recomendable para una mejor visualización aplicar una función de agregación al final o encerrarlo en un df.
```python
          df[df["Tipo de Orden"].isin(["OC", "OS"])] #Muestra toda la tabla con el filtro.
          df[df["Tipo de Orden"].isin(["OC", "OS"])]["Tipo de Orden"] #Mostrara solo esta columna en la tabla
          
```

* **`df.where() o np.where()`** : Es una condicional de formato **(condicional,Valor_if,Valor_else)** que viene de numpy. Ejm: Para crear una columna con condicionales:
```python
    preciosdf["Margen"] = np.where(preciosdf["Ganancias"]>100,"Alto","Bajo")
    #Si cumple tendrá un valor de "Alto" else "Bajo".

    # Buscamos el precio donde GlivArea es igual a 4676 (Area de vivienda), también arroja su index.
    df["SalePrice"].where(df["GrLivArea"] == 4676).dropna()

    #Crea una tabla en el que se mostrarán las columnas "Descrición Material","Tipo de Orden"
    df[df["Creado Por"] == "JARCAN"][["Descrición Material","Tipo de Orden"]].where(df["Tipo de Orden"] == "OC").dropna()

```
* **`df.groupby()`** : Creara una tabla dinámica con las filas de la columna indicada (puede ser más de una Columna). El método se lo agregamos al final. Se puede usar los `groupby()` como input para armar una tabla pivote ``pd.pivot_table()``. *No usa parámetro ,",aggfunc=" pero si .agg({"Columna":"Opercion"})*
  ```python
            grupo_Tipo = restdf.groupby(["Tipo","Producto"]).mean()
            grupo_Tipo.loc["Bebida"] #Mostrará las filas de "Tipo" -> "Bebida"

            grupo_Tipo = restdf.groupby(["Tipo","Producto"], as_index=False).agg({"InvoiceNo":"unique"})
            grupo_Tipo.loc["Bebida"]

  ```
    * Para agrupar según una columna tipo `datetime64[ns]` asignando una operación a las columnas afectadas
      ```python
            grupo = df_new.groupby(pd.PeriodIndex(df_new["Fe, de picking"], freq='M')).sum().sort_index(ascending=True).reset_index()
            #Le ponemos reset_index para que no me tome las fechas como index, pero las ordenamos antes de resetear.
            
      ```
* **`df.pivot()`** : Devuelve el DataFrame remodelado (reshape) organizado por valores, sin agrupar, de índice/columna dados.<br>
Solo tiene 3 parámetros: `,index=` `,columns=` `,values=`.

  ```python
    data3 = df.pivot(columns="OverallQual", values="SalePrice")
    
  ```
* **`df.pivot_table()`** : También puede usar (`pd.pivot_table()`). Crea una Nueva tabla dinámica a partir de un dataframe agrupando sus valores por la operación que indiquemos por `,aggfunc=`.<br>
El "`,aggfunc=`" que traen por defecto es promedio. Puede hacer una tabla pivote de una `groupby()` <br>
***(DataFrame,Valores,Filas target,(jerarquía)columnas mostrada en valor precio,aggfunc= función a aplicar,margins = subtotales)***

  * `,fill_value = 0` : Reemplaza todos los valores nulos de df encontrado por el numero o string que le indiquemos.
  * `,aggfunc =` : Indicamos el método que operara con los valores. *aggfunc=np.sum o aggfunc="sum"; aggfunc="mean"; aggfunc= pd.Series.nunique #Devuelve un valor distinct count*

  ```python
    vent_tipo_mesero = df.pivot_table(values="Precio",index=["Tipo_x","Categoria"],
                        columns="Mesero",aggfunc=np.sum,margins=True).round(0)
    
    # Tabla agrupada por columnas en Meses del conteo distinto (no repetidos) de los ID de compras.
    df.pivot_table(index=["Creado Por",df["Tipo de Orden"]], columns=[pd.PeriodIndex(df["Creado El"], freq="m")],
                values=["Pedido"], aggfunc=pd.Series.nunique) #Devuelve un valor distinct count
  ```
* **`df.melt()`** : Despivotea una tabla pivot, retornandola a la forma de tabla de datos convencional (quitando agrupaciones). Al momento que despivotea va a retornados una columna para los valores de columna y otra para los valores valores. <br>
En una tabla pivote tenemos los siguientes elementos: filas (id_vars), columnas (values), values (value_name).

  * `,id_vars = ` : (filas) Nombre de la columna por la que fue agrupada la tabla, esta es la que va en index o filas.
  * `,values = ` : (Columnas) Nombres de las columnas de la tabla pivote, para no digitar en una lista nombre por nombre de todas las columnas es recomendable sacar sus nombres con una compresión de listas y guardarla en un variable para luego asignarla a este campo de values.
  * `,value_name = ` : (Encabezado Valores) Dado que separara los valores en una columna aparte es necesario asignarle un nombre como "Ventas".
  * `,var_name = ` : (Encabezado Columnas) Asigna un nombre o encabezado al campo de las columnas de la ex tabla pivote. De no asignarle un nombre pondrá un nombre por defecto.
  ```python

    #Despivoteamos una tabla agrupada por productos con columnas por año y ventas en valores.
    data3 = df.pivot(id_vars=["Producto"], values=["2010","2011","2021"], value_name="Ventas", var_name="Año")

    #Primero para listar todos los nombres de la columna values y no estar escribiendo año por año el nombre.
    años = [col for col in df if col!="Producto"] #devuelve todos los nombres de la columna menos la primera "Producto"
    
    #Tras hacer esto podemos igualar ,values= años
    
  ```
* **`df.merge()`** : Es la union de tablas como se efectúan en ``SQL``.<br> 
Se puede precisar la columna de union con `,on=["Columna"]`. `,how=` indica el tipo de union, por defecto esta en ``INNER JOIN`` (intersección) donde en ambas tablas existan datos.
```python
          #LEFT JOIN (columna izquierda completa)
          df = df_A.merge(df_B,on=["Producto"],how="left")

          #OUTER JOIN Preserva todos los valores Tabla A y B.
          df = df_A.merge(df_B,on=["Producto"],how="outer")

          #Unir Filas Por Index
          df = df_A.merge(df_B, left_index=True, right_index=True)

```
* **`df.sample()`** : Coje una parte de la data del DataFrame de manera aleatoria o de la semilla que nosotros indiquemos.
```python
          muestra_df = california_df.sample(frac=0.1, random_state=17)

          # Creamos una nueva columna en df_2 con los datos de df donde solo tomaremos 100 valores de manera aleatoria 
          # del total.
          df_2["network"] = df["network"].sample(n=100).tolist()
```
* **`df.corr()`** : Calcula la correlación por Columna vs columnas, excluyendo NA/valores nulos y textos. Devuelve una tabla matriz de correlaciones.
  ```python
          Correlacion = restdf.corr(method='pearson', min_periods=1))

          rest.corr()["AMZ"].sort_values(ascending=False)
          #Muestra la correlación de una sola columna con los demás campos de manera descendente
  ```
  * method= pearson : standard correlation coefficient; kendall : Kendall Tau correlation coefficient; spearman : Spearman rank correlation
  * min_periods=1 : (Opcional, solo para "pearson" y "spearman") Número mínimo de observaciones requeridas por vs.

* **`df.corrwith()`** : La correlación por pares se calcula entre filas o columnas con otro df de la misma dimension. `,method=` "pearson", "kendall", "spearman". Ejm: Correlación de 1 campo con respecto al resto de un mismo df:
  ```python
          df.corrwith(df["SalePrice"], method="pearson", axis=0).nlargest(10).sort_values(ascending=False)
          #Va a correlacionar una columna del df con todas las demás. Entregando los 10 campos más correlacionados.
  ```
  * method= pearson : standard correlation coefficient; kendall : Kendall Tau correlation coefficient; spearman : Spearman rank correlation
  * min_periods=1 : (Opcional, solo para "pearson" y "spearman") Número mínimo de observaciones requeridas por vs.

* **`df.plot()`** : Muestra una gráfica lineal. Se puede llamar el tipo de gráfica con ``df.plot.GRÁFICA()`` o ``df.plot(kind=GRÁFICA)`` Tiene la siguiente configuración:
    ```python
          axes = temps_df.plot(x="Fahrenheit", y="Celsius",style=".-",figsize=(10,8))
          y_label = axes.set_ylabel("Celsius") #Mostrar etiqueta "Y"
    ``` 
  * **`df.plot.scatter()`** :Gráfico de Dispersion: "Scatter Plot"
    ```python
          axes = restdf.plot.scatter(x="Producto",y="Propina")
  
    ```
  * **`df.plot.bar()`** : Muestra una gráfica de barras. Tiene la siguiente configuración:
    ```python
          axes=datos.plot.bar(x='palabra',y='frecuencia',legend=-False)  #Invierte la legenda del eje "x" La pone en Vertical.         
    ```

---
### Propiedades:
El indexado es igual que numpy trabaja con corchetes **[ : : ]** para indicar los rangos de datos que queremos que nos arrojen. No trabaja con paréntesis. *Ejm: pd.DataFrame.loc*

* **`.loc[]`** : Se usa para visualizaciones. Imprime una columna por su 'nombre'. Para ello es necesario indicar primero el indice luego el nombre de la columna. Imprime también intersecciones con columnas. **['fila','columna']**. También puede cambiar datos del DF. Para seleccionar más de un index a la vez usamos doble **[[]]**

  ```python
            df.loc[:,"SalePrice"] #Siempre indicamos primero el indice y luego el nombre de la columna.
            df.loc[nºIndice,'columna'] or [1:2,"Producto"] #Rango de filas de una columna o sin ella.
            df.loc['fila']
            df.loc['fila','Fila dentro de fila'] #Encaso este en un groupby o alguna tabla dinámica.
            df.loc['fila':'fila']

            #para poder seleccionar más de un index a la vez usamos doble [[]]
            df_population_sample = df_population.loc[["1980","1990","2000","2010","2020"]]

            #También puede usarse el siguiente código para indicar columnas.
            df = df[df.index.isin(["2020","2021"])] #usa corchetes porque usa lista.

            #Podemos usar el indice de otra columna para filtrar otra:
            df.loc[df['TotalBsmtSF']>0,'HasBsmt'] = 1 #Escribe 1 en todos los indices donde "TotalBsmtSF" sea mayor a 0 en "HasBsmt".

            #Otra manera de hacer lo mismo sin "iloc"
            df[df['TotalBsmtSF']>0]['HasBsmt'] = 1

            #Un filtrado en 2 dimensiones fila,columnas[Columna a mostrar].
            df.loc[df["Tipo de Orden"].isin(["OC", "OS"]),["Tipo de Orden"]]["Tipo de Orden"]] #Añadimos esta columna a mostrar al final

            #Del ejercicio anterior otra manera de hacer el filtrado sin loc:
            df[df["Tipo de Orden"].isin(["OC", "OS"])]["Tipo de Orden"]]

            #Añade una columna "Total" en el df
            tabla_total["Total"] = tabla_total.sum(axis=1)

            # Añade una fila "Total" en el df
            tabla_total.loc["Total"] = tabla_total.sum(axis=0)
  ```
  También se puede usar para filtrar:
  ```python
            restdf.loc[restdf["Propina"]>0.05]
  ``` 
* **`.iloc[]`** : Se usa para visualizaciones por indice. Imprime una fila por su indice con todas las columnas correspondientes.

```python
          df.iloc['nºfila']
          df.iloc['nº.fila':'nº.fila2'] #hasta fila 2
```

* **`.index[]`** : A diferencia de `loc` y `iloc` Devolverá el nº de index del valor indicado.
  ```python
    #Indicando el valor de una columna.
    train_features.drop(train_features.index[train_features["Cabin"]=="T"],axis=0)
      
    #Se puede usar en combinación con loc y iloc.
    train_features.drop(train_features.loc[train_features["Cabin"]=="T",:].index,axis=0)
  ```

* **`.at[]`** : imprimirá solo el valor específico en la intersección de fila y columna señaladas por su 'nombre'. *['fila','columna'].También podemos asignarle **"="** un nuevo valor a la intersección.

* **`.str[]`** : Referencia a la librería ``String``, convierte todos los resultados de sus métodos a formato String. Habilita métodos regex. *(Expresiones Regulares)*
  ```python
    #Mostrar la primera letra de la columna Cabin.  
    train_features["Cabin"] =  train_features["Cabin"].astype(str).str[0] #Pasamos a formato string antes.

  ```
  * **`.str.contains()`** : Este método Habilita el poder hacer un búsqueda por Expresiones Regulares en los valores del df o columna.
    ```python
      df[df["Pedido"].str.contains("^451", regex=True)]   

      #Para hacer uso de flags=re.IGNORECASE necesitamos importar la libreria ``re``
      import re 
      df[df["Pedido"].str.contains("OC|OS", regex=True, flags=re.IGNORECASE)]    
    ```
  * **`.str.match(r'')`** : Busca que el elemento contenga unicamente lo especificado en la búsqueda. Si hay un dato mas a lado botara como falso.
  
  * **`.str.contains(r'')`** : Buscara en cada string el patron de búsqueda indicado (r''). No se ve afectado si hay mas datos después del patron que queremos hallar.
  
  * **`.str.replace("$","")`** : Reemplazara el valor existente por uno indicado después de la coma, en este caso por nada (indica borrar símbolo "$".) 

* **`.columns[]`** : Muestra el nombre de todas las columnas del ``df`` .Podemos también cambiar el nombre de las columnas en el orden que estas están con esta propiedad. EJM: 
  ```python
          kchouse.columns=["AreaFt","P.V"] #Cambia el nombre en el orden que están las columnas de df.

  ```

* **`.dtype[]`** : Mostrará el tipo de dato de cada columna. ***(int, float, object,datetime64[ns])***.
  
* **`.values[]`** : Arrojara los valores de la columna seleccionada en forma de array ([1,2,3]).

  ```python
              kc_house["Areaft"].values
  ```
* **`.dt[]`** : Referencia a la librería ``datetime``. Se aplicará a todos las columnas de tipo ``datetime64`` y ``timedelta64`` (Diferencia entre 2 datetime64). Este tipo de dato son el resultado de operar columnas de tipo fecha como restar 2 columnas de tipo `datetime64`.

  ```python
    df['Order Date'] = df['Order Date'].dt.strftime("%d/%m/%y")
    #Arrojara valores de formato string.

    df["Lead Time Entrega"] = (df["Creado El"] - df["Fecha entrega SAP"]).dt.days
    # Devolverá la diferencia en valor de formato entero (int32), sin el dt devolvera un timedelta64.
  ```
---
### Parámetro:

Van ligados a un método por una **"`,`"**

* **`,index=`** : Añade o reemplaza indices en forma de key a los datos de una serie, son encabezados para una fila. Este parámetro hace lo mismo que un diccionario pero de una forma más rápida a la hora de digitar.
* **`,columns=`** : Asigna nombre a las columnas del DF.
* **`,axis=`** : Dirige el procesamiento a filas o columnas de la matriz. **0** para **filas** y **1** para **columnas**.
  ```python
          #Creamos una nueva columna que sea el promedio con todas las columnas:
          df["Valor_prom"] = df.mean(axis=1)    
            
  ```
* **`,ascending=`** : Ordena a ascendente, por defecto viene en True.
* **`,inplace=`** : Viene por defecto en `False`, en modo `True` toma un método que no reemplaza al archivo o valor raíz, como un filtro, para reemplazarlo por la modificación hecha o método aplicado.
* **`,key=`** : Podemos añadir una función `lambda` que procesará previamente nuestro método.
* **`,na=`** : En modo ``false`` ignorará los datos vacíos.
* **`,case=`** : En modo ``false`` ignorará mayúsculas y minúsculas.
* **`,regex=`** : En modo ``true`` entenderá los operadores de `re` como el "or" ("``|``").
* **`,legend=`** : En modo ``-false`` Pondrá la leyenda del eje "x" en Vertical en el `.plot.bar()`.
  
## [7. openpyxl](#indice)
Es una librería que nos permite abrir los archivos excel y operar como si estuviéramos dentro del software. Cabe resaltar que pandas corre archivos excel bajo esta librería en su interior.<br>
*pip install openpyxl*

Se importa de la siguiente manera:
```python
import openpyxl
```
Otras librerías dentro de openpyxl
```python
from openpyxl.chart import BarChart, Reference #Gráfico de cuadros
from openpyxl.styles import Font #Fuentes de excel
``` 
Para el manejo de las celdas de excel es necesario la librería `string` el cual nos traerá el abecedario `.ascii_uppercase`.
```python
import string

  abecedario = list(string.ascii_uppercase)
```

### clase, Función:

* **`load_workbook()`** : Carga un archivo de excel `.xlsx`
  
  ```python
          wb = load_workbook('sales_2021.xlsx')#abrimos resultado de pandas
          pestaña = wb['Report'] #asignamos a la variable la pestaña existente
  ```
### Librerías, Métodos:

* **`Reference()`** : Librería para referenciar los datos que se graficará. Primero se referencia los datos luego se gráfica. Previamente debemos indicar con las propiedades de `.active` donde están los datos con los que se referenciará la gráfica.
  ```python
          data = Reference(pestaña, min_col=min_col+1, max_col=max_col, min_row=min_fila, max_row=max_fila)
          #data hace referencia a Columnas a tomar.
          categorias = Reference(pestaña, min_col=min_col, max_col=min_col, min_row=min_fila+1, max_row=max_fila)
          #categorias hace referencia a Filas a tomar.

  ```
* **`BarChart()`** : Librería de Gráfico de barras de Excel. Es necesario asignar esta librería a una **variable** para usar sus siguientes **Métodos**:
  ```python
        barchart = BarChart()
  ```
  * **`.add_data()`** : Añade el `Reference` con respecto a columnas (`data`) al gráfico.
    ```python
          barchart.add_data(data, titles_from_data=True)

    ```
  * **`.set_categories()`** : Añade el `Reference` con respecto a Filas (`categorias`) al gráfico.
    ```python
          barchart.set_categories(categorias)
          
    ```
  * **`.title=`** : Asigna un titulo al gráfico. Ejm: "Nombre"
  * **`.style=`** : Asigna un estilo al gráfico. Ejm: 5
  <br><br>
* **`.add_chart()`** : Este método añadira el grafico previamente creado a la pestaña de nuestro workbook cargado (`pestaña = wb["Nombre_Pestaña"]`)
```python
          pestaña.add_chart(barchart, 'B12')
```

### Propiedad:

* **`.active`** : Entra a la pestaña activa de la variable donde previamente hemos cargado el archivo excel, tiene las siguientes propiedades: Cabe resaltar que el conteo de las columnas y filas activas las arroja en forma de numero empezando con el valor "1" y no "0" como lo es en python.
  ```python
          min_col = wb.active.min_column # Cuenta empezando por "1"
          max_col = wb.active.max_column
          min_fila = wb.active.min_row
          max_fila = wb.active.max_row
  ```
  * **`.min_column`** : Columna minima de nuestro cuadro o tabla. 
  * **`.max_column`** : Columna maxima de nuestro cuadro o tabla.
  * **`.min_row`** : Fila minima de nuestro cuadro o tabla.
  * **`.max_row`** : fila maxima de nuestro cuadro o tabla.
  <br><br>
* **`.style`** : De la librería ``openpyxl.styles`` da formato a los gráficos.
  ```python
            pestaña[f'{i}{max_fila+1}'].style = 'Currency'
            #Convierte una celda a tipo moneda en excel.   
  ```
* **`.font`** : De la librería ``openpyxl.styles`` da formato a la celda.
  ```python
            pestaña['A1'].font = Font('Arial', bold=True, size=20)
  ```

## [8. matplotlib ("*plt*")](#indice)
Es una librería que da una representación gráfica de nuestros datos en cuadros estadísticos en 2D.<br>
*pip install matplotlib*

Se importa de la siguiente manera:
```python
    import matplotlib.pyplot as plt 
```
### clase, Función:

* **`plt.figure()`** : Crea un lienzo para insertar un gráfico dentro. Podemos indicar nºfilas (nrows=), nªcolumnas (ncols=), tamaño del lienzo (figsize=): La variable `axes` o `ax` es la que dibujará en el lienzo para ello indicamos el tipo de gráfico que queremos e indicaremos con que datos graficará:

  ```python
      plt.figure(figsize=(16,9))
      
  ```
* **`plt.plot()`** : Es usado para graficar valores dentro del lienzo.

  ```python
      re_x = np.arange(100,14000,1) #rango eje "x"
      re_y = re_x*reglin.slope + reglin.intercept #regresion para cada valor "y"
      ax.plot(x,y,"r--")
      #Previamente definimos "x" , "y" con tipo de linea y color

  ```

* **`plt.subplot()`** : Es usado para graficar listas ya que estas no poseen el atributo `plot`. Añade `axes` a la figura existente (lienzo) para insertar un gráfico dentro. Podemos indicar nºfilas (nrows=), nªcolumnas (ncols=), tamaño del lienzo (figsize=): La variable `axes` o `ax` es la que dibujará en el lienzo para ello indicamos el tipo de gráfico que queremos e indicaremos con que datos graficará:
 
    ```python
    figure, axes = plt.subplot(nrows=4, ncols=6, figsize=(6,4)) #para multicuadros.
    ```
  * Para regresión lineal:

    ```python
      fig, ax = plt.subplots(figsize=(6,5))
      ax.scatter(x=kchouse.AreaFt, y=kchouse["P.V"]) #scatter de datos
      re_x = np.arange(100,14000,1) #rango eje "x" (necesario numpy)
      re_y = re_x*reglin.slope + reglin.intercept #regresión para cada valor "y"
      ax.plot(re_x,re_y,"r--") #variable ax es la que dibuja el gráfico.
      plt.show()
      
    ```
* **`plt.title()`** : Inserta titulo al gráfico.
* **`plt.xlabel()`** : Inserta titulo al eje x.
* **`plt.ylabel()`** : Inserta titulo al eje y.
* **`plt.legend()`** : Insertara la leyenda a nuestra gráfica.

    * **``,loc=``** : Indica la posición donde se ubicara la leyenda ejemplo ("upper left","upper right", etc)

## [9. seaborn ("*sns*")](#indice)

Es una librería para visualización de datos, esta esta desarrollada sobre **matplotlib**. Además está integrada con la librería de **pandas** por lo cual puede leer **DataFrame** y campos directamente como argumentos de las funciones de visualización:
___
### Clase, Función:

* **`sns.set()`** : Cambia el tamaño de las fuentes en las gráficas.
```python
        sns.set(font_scale=2)
```
* **`sns.set_style()`** : Cambia los ajustes a la configuración de la librería Seaborn. 
```python
        sns.set_style("whitegrid")
```
* **`barplot`** : Diagrama de barras. 
```python  
        axes =sns.barplot(x=valores,y=frecuencias, palette="bright")
```
* **`regplot()`**: Muestra un diagrama de regresión lineal de 2 variables.
```python
        axes = sns.regplot(x=kchouse.AreaFt, y=kchouse["P.V"], scatter=True,order=1,color="blue", label="order 1", line_kws={"label":"y={0:.1f}x+ {1:.1f}".format(reglin.slope,reglin.intercept)})
        #order= nº de polinomio de la regresión.
```
* **`.heatmap`** : Diagrama o mapa de calor.
```python
        axes = sns.heatmap(confusion_df, annot=True, cmap='nipy_spectral_r')
```
* **`sns.pairplot`** : (Gráfico_Matriz) "Parcela" Un diagrama de pares traza una relación por pares en un conjunto de datos. La función ``pairplot`` crea una cuadrícula de ejes de modo que cada variable en los datos se compartirá en el eje y en una sola fila y en el eje x en una sola columna. Eso crea parcelas
```python
        cuadricula = sns.pairplot(data= iris_df, vars=iris_df.columns[:4],hue="Especie")
#vars = (lista de nombres de variables)Son en total 4 columnas del df. (data + target)
#huw = Agrupa la gráfica por la columna que le indiquemos.
```
___
### Métodos:
___
### Propiedad:
___
### Parámetro:
* **`,hue=`** : "matiz" (opcional), Este parámetro toma el nombre de la columna para la codificación de colores.
* **`,palette=`** : Cambia los colores del gráfico. Ejm: "cool"

## [10 plotly ("*px*")](#indice)

### CUFFLINKS
[Fuente y guia de uso Git](https://github.com/santosjorge/cufflinks)

Es una librería que da una representación gráfica de nuestros datos de manera interactiva y dinámica <br>
*pip install plotly*

También necesitaremos un librería llamada `cufflinks` que va a hacer de intermediario entre `pandas` y `plotly` el cual va a hacer mas fácil de escribir y leer el código cuando grafiquemos nuestras visualizaciones.
*pip install cufflinks*

Se importa de la siguiente manera para hacer gráficas con pandas:
```python
    import pandas as pd
    import cufflinks as cf
    from IPython.display import display,HTML

    cf.set_config_file(sharing="public", theme="space", offline="True")
    
    #para ver todos los temas disponibles para las gráficas usamos: cf.getThemes()
    #space da color oscuro.

    setattr(plotly.offline, "__PLOTLY_OFFLINE_INITIALIZED", True)
    #Esto soluciona el error de attributeerror: module 'plotly.offline' has no attribute '__plotly_offline_initialized'

```

### Funciones:

* **`cf.go_offline()`** : Gráficos offline. Para que no redireccione a la pagina de ``plotly``.
* **`cf.datagen. ... ()`** : Genera DataFrames preparados para el tipo de gráfica con valores aleatorios que querramos, dentro del ``( )`` indicamos cuantas DataFrames queremos generar. Para graficarlar usamos ``.iplot()``. Examinando como están armados los DF podemos guiarnos para las gráficas.
  ```python
    cf.datagen.lines(1)
    cf.datagen.histogram(4)
    cf.datagen.bubble() #Para esta grafica se toma minimo 4 variables. (x=,y=,size=,categoria=)
  ```

* **`cf.subplot()`** : Grafica un conjunto de graficas previamente enlistadas y concatenadas entre **``[ ]``** (Suma de iplots). Para imprimir la grafica usamos: cf.iplot()<br>
Una comprension de listas no es mas que una concatenacion de listas: [""]+[""]
  
  
    ```python
      m = [df[df['X']=="A"][['Col1','Col2']].iplot(kind='box',asFigure=True)] +
          [df[df['X']=="B"][['Col1','Col2']].iplot(kind='box',asFigure=True)]
      cf.iplot(cf.subplots(m))

      # Con figure (Multigraficas independientes)

      #Ponemos cada una de las gráficas -> df.figure(kind=,) dentro de un cf.subplots().
      figs = cf.subplots([df1.figure(kind='scatter'),df2.figure(kind='hist'),df3.figure(kind='box'), 
                          df4.figure(kind='barh')], subplot_titles=['Bar 1','Bar 2','Bar 3','Bar 4'],
                          vertical_spacing=.20,horizontal_spacing=.05)

      #Formato y Titulo
      figs['layout'].update(showlegend=True, title='Subplots Graphics Comparasion', font=dict(color='#ffffff'))

      #Imprimimos la gráfica.
      cf.iplot(figs)
    ```


* **`cufflinks.to_df(Figure)`** : Extraerá de un gráfico ``plotly`` alojado en una variable, el dataframe que contiene el gráfico.

### Métodos:

* **`df.figure()`** : Sirve para hacer multi-gráficas de grafías independientes en una sola gráfica.

* **`df.iplot()`** : Armara la gráfica según los parámetros.
  * **`Gráfico de Lineas`** : Se suele utilizar cuando hay tiempos involucrados, con el fin de ver la evolución de los tiempo (Fechas).

  ```python
      df_population.iplot(kind="line", title="Year vs Population" ,xTitle="Country", yTitle="Population",mode='lines+markers'
      ,fill=True,,error_y=5, error_type='percent')    
    
      
      # ,mode='lines+markers' : Genera puntos a lo largo de la linea
  ```
  * **`Gráfico de Lineas Diferencial`** : Este es un Gráfico de Lineas normal con la adición que en el pie del gráfico se añade un gráfica que muestra la diferencia entre las gráficas bien sea en una diferencia numérica (`spread`) o proporcional (`ratio`). Esta mas orientado a la comparación entre 2 campos.
  
  ```python
      #Diferencia en Valor Numérico.
      df_population[["United States","Brazil"]].iplot(kind='spread',mode="lines+markers",size=10, title="Diferencia Numérica")      

      #Diferencia en Proporción.
      df_population[["United States","Brazil"]].iplot(kind='ratio',mode="lines+markers",size=10, title="Diferencia Numérica")      

  ```
  
  * **`Gráfico de Dispersión (ScatterPlot)`** : Se suele utilizar para hacer comparación entre 2 variables. 

  ```python
      df_population.iplot(kind="scatter", mode="markers", xTitle="Year", yTitle="Population", title="Year vs Population",
                                size=12,symbol=['x-open',"circle-open","square-open","diamond-open","triangle-up-open"]))

      # scatter o line da gráfica de lineas por ello usamos mode="markers para forzar los puntos de dispersion. 
      # size=12 : Tamaño de los puntos.

      #Para exploración de datos "Scatter" se dimensiona la gráfica en un cuadrado. 

      data2.iplot(kind='scatter', x='TotalBsmtSF', y='SalePrice', mode='markers', size=9, xTitle='TotalBsmtSF', yTitle='SalePrice',
                  title='TotalBsmtSF vs SalePrice', bestfit=True, bestfit_colors=["red"], dimensions=[700,700], legend=False)

  ```
  * **`Gráfico de Barras (BarPlot)`** : Se puede usar para casi todo tipo de contexto.<br>
  
    * **Barras verticales** se suele usar cuando las etiquetas caben.
    * **Barras Horizontales** se usan cuando el titulo no cabe (es grande). También cuando queremos presentar elementos de un conjunto. Ejm: ventas por Categoria

  ```python
      #Gráfica de Barras Verticales
      df_population_2020.iplot(kind="bar", title="Year vs Population" ,xTitle="Countries", yTitle="Population", colors="red")
      
      # ,kind="barh" = Barras Horizontales.
      # ,bargap = .2 : Afecta la anchura de las barras.
      # ,barmode="stack" : Crea barras segmentadas por categorías y las va sumando unas con otras poniendo a las mas pequeñas en el tope, es un acumulado. "overlay" : Similar a "stacks" pero no las suma sino da su valor tal cual (comparativo con otros) las presenta ordenada según estén las categorías en la leyenda.
  ```
  * **`Gráfico de Cajas (BoxPlot)`** : Es usado para encontrar los valores estadísticos como la media, mediana los qurtines (Q1,Q3) Q1 se refieren menor igual al 75% de datos y Q3 al 25%.

  ```python
      df_population.iplot(kind="box",boxpoints="all", title="Year vs Population",dimensions=[1200,500])
      
      #boxpoints="all" : Muestra la concentración de datos en forma scatter a lado del boxplot.

      #Sacando 2 columnas del df, para ordenar por columna - valor y saque un boxplot por calidad. 
      data3.pivot(columns="OverallQual",values="SalePrice").iplot(kind="box",
             title="OverallQual vs SalePrice", dimensions=[1100,600]) #no muestra etiqueta X ni Y.

  ```
  * **`Gráfico Histograma (Histogram)`** : Sirve para ver la Distribución de data Numérica. Para ejecutar este gráfico es necesario tener los campos (incluido el index) ordenado por columnas y solo tener valores en las filas.

  ```python
      df_population[["United States","Indonesia"]].iplot(kind="hist",title="Year vs Population",xTitle="Countries", yTitle="Population",orientation='v',bins=0, subplots=False,sortbars=False, barmode="overlay")

      # ,barmode="stack" : Da el Acumulado; "overlay" (por defecto) da su valor tal cual para comparación. "group" Agrupa por pares de barras.
      # ,bins=50  : Aumentara el número de barras (sample) para el gráfico.
      # ,orientation='h' : Hará un histograma con barras Horizontales.
      # ,histnorm= "percent", "probability", "density", "probability density". Establece el tipo de normalización para una traza de histograma. Por defecto la altura de cada barra muestra la frecuencia de ocurrencia, es decir, el número de veces que se encontró este valor en el recipiente correspondiente
      #,histfunc= "count", "sum", "avg", "min", "max". Establece la función de agrupación utilizada para un seguimiento de histograma.
                                  
  ```
  
  * **`Gráfico de Pie (PieChart)`** : Se puede usar esta gráfica hasta con 4 variables. No es muy usada, es mas visual. Es necesario usar un index numérico de no tenerlo reseteamos el index con `reset_index(inplace=True)`

  ```python
      df_population.reset_index(inplace=True)

      df_population_2020.iplot(kind="pie",labels="country", values="2020", title="Population 2020")
  ```

  * **`Gráfico Mapa de Calor (Heatmap)`** : Mapa de calor.
  
  ```python
      df.corr().iplot(kind="heatmap",title="Correlation Matrix",colorscale="reds",dimensions=[1200,1000])
      #columns vs columns
  
  ```

* **`Multi-Gráficas`** : Subplot's varias gráficas.

  ```python
    # Creación de 4 DataFrames.
    df1=cf.datagen.heatmap()
    df2=cf.datagen.heatmap()
    df3=cf.datagen.heatmap()
    df4=cf.datagen.heatmap()

    #Ponemos cada una de las gráficas -> df.figure(kind=,) dentro de un cf.subplots().
    figs = cf.subplots([df1.figure(kind='bar',),df2.figure(kind='bar'),df3.figure(kind='bar'),df4.figure(kind='bar')], 
                  subplot_titles=['Bar 1','Bar 2','Bar 3','Bar 4'],
                  vertical_spacing=.20,horizontal_spacing=.05)

    #Formato y Titulo
    figs['layout'].update(showlegend=True, title='Subplots Graphics Comparasion', font=dict(color='#ffffff'))

    #Imprimimos la gráfica.
    cf.iplot(figs)

  ```

### Parámetros:

* **`,kind=`** : "line", "scatter", "bar", "box", "hist", "pie".
* **`,legend=False`** : Quita las Leyendas.
* **`,categories="Columna"`** :(groupby) Agrupa demanera distinct_count las columnas categoricas y las coloca en la leyenda del grafico.
* **`,colors=`** : para más de un dato en el gráfico usamos.
* 
    ```python
      ,color=["blue","red","yellow"]
    ```
* **`,colorscale=`** : Escalas de color: "accent", "-accent", "blues", "-blues", "reds", "-reds", "purples","-purples", "paired", "-paired", "spectral", "-spectral", "brbg", "-brbg"

* **`,rangeslider=True`** : Añade es un desplazador de rango al pie del gráfico para desplazarse en la visualización de datos.
* **`,annotations`** : Añade una marca con respecto al index(date) , con su respecto formato de texto. Ejemplo Series de tiempo en el index.
  
    ```python
      ,annotations={'2015-02-02':'Market Crash', '2015-03-01':'Recovery'}, textangle=-70,fontsize=13,fontcolor='grey'
    ```
  
* **`,error_y=5, error_type='percent'`** : Añade un rango de error de la data graficada de +- 5%. Error_type puede ser también `error_type='continuous_percent'`.

* **`,sortbars=True`** : Ordena las gráficas de Barras manera descendente.
  
* **`,subplots=True`** : Crea una gráfica por cada Categoria del DataFrame, desglosa la grafica en varias mini-graficas. En el caso de tener varios gráficos de un tipo alojadas en una variable podemos hacer muchas minigráficas.
  ```python
    cf.datagen.histogram(4).iplot(kind='histogram',subplots=True,bins=30)

    cf.datagen.lines(4).iplot(subplots=True,shape=(4,1),shared_xaxes=True,vertical_spacing=.02,fill=True)
    #son 2 mosaicos de gráficas independientes
  ```
* **`,asFigure=True`** : Compilas las gráficas creadas por `subplot()` en una gran gráfica sin mezclarlas.
  
* **`,shape=(5,1)`** : Según el nº de gráficas o categorías que tengamos ejecutara una gráfica aparte. Hace lo mismo que ``,asFigure=True``

* **`,shared_xaxes=True, shared_yaxes=True`** : Fuerza el mostrar etiquetas en eje X & Y. Amplia la vision sea que activemos en X o Y (mejora las multigraficas subplot=True).
  
* **`,hspan or vspan`** : Crea un rango sombreado bien en horizontal `hspan` o vertical `vspan`.

    ```python
      cf.datagen.lines(3).iplot(hspan=dict(y0=-1,y1=2,color='orange',fill=True,opacity=.4))
    ```
* **`,fill=True`** : Activa sombreado, ejemplo en el gráfico de linea se vera sombreado todo el area bajo la linea.
  
* **`,bestfit=True`** : Genera una regresión lineal de los datos implicados en la gráfica.
  
  ```python
      ,bestfit=True,bestfit_colors=['pink','blue']
      #podemos asignarle un color a la regresión. El color también es tomado por el gráfico.
  ```
* **`,dimensions=[Ancho,Alto]`** : Modificara las dimensiones de la gráfica, para temas de exploración de datos es recomendable tener una medida de figura cuadrado y quitar las leyendas. Ejm: ,dimensions=[700,700]
  
  ```python
      data2.iplot(kind='scatter', x='TotalBsmtSF', y='SalePrice', mode='markers', size=9, xTitle='TotalBsmtSF', yTitle='SalePrice',
                  title='TotalBsmtSF vs SalePrice', bestfit=True, bestfit_colors=["red"], dimensions=[700,700], legend=False)
  ```

* **`,world_readable=True`** : Hace que el gráfico pueda compartirse de forma publica ¿?.

### plotly.express (px)

Es una librería parecida a `cufflinks` es de fácil uso y gráficos de alto nivel de visualización. En complemento con cufflinks para gráficos más avanzados.

[Fuente y guía Gráficos](https://plotly.com/python-api-reference/)

Se importa de la siguiente manera para hacer gráficas con pandas:
```python
    import plotly.express as px

    #Para mantener el tema oscuro:
    import plotly.io as pio
    pio.templates.default = "plotly_dark"
```

### Gráficas

* **`px.histogram`** : Gráfica de histograma muy personalizable. Cufflinks resumen el personalizar todo este código.

```python
  df = px.data.tips()
  fig = px.histogram(df, x="day", y="total_bill", color="sex",
            title="Receipts by Payer Gender and Day of Week vs Target",
            width=800, height=600,
            labels={"sex": "Payer Gender",  "day": "Day of Week", "total_bill": "Receipts"},
            category_orders={"day": ["Thur", "Fri", "Sat", "Sun"], "sex": ["Male", "Female"]},
            color_discrete_map={"Male": "RebeccaPurple", "Female": "MediumPurple"},
            template="plotly_dark"
            )

  # Añadir el símbolo de $ a los números del Eje "Y"
  fig.update_yaxes(tickprefix="$", showgrid=True)

  # Fuente de texto, y legenda centrado
  fig.update_layout(font_family="Rockwell",legend=dict(title=None, orientation="h", y=1, yanchor="bottom", x=0.5, xanchor="center"))

  # Añade la linea horizontal con respecto al valor de y0 - y1
  fig.add_shape(type="line", line_color="salmon", line_width=3, opacity=1, line_dash="dot",
                x0=0, x1=1, xref="paper", y0=950, y1=950, yref="y")

  # Añade un texto en la posición en el elemento "x" y posición "y" indicada
  fig.add_annotation(text="below target!", x="Fri", y=400, arrowhead=1, showarrow=True)

  fig.show()

```

* **`px.scatter_matrix`** : Gráfica la correlación entre las propias columnas del DataFrame, no requiere preparación de la data.
```python
  columns = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']

  fig = px.scatter_matrix(df[columns],title="Scatter Matrix",height=1200,width=1200)

  fig.show()
```

### Parámetros:

* **`,height=1200, width=1200`** : Modificara las dimensiones de la gráfica como lo haría ``,dimensions=[Ancho,Alto]`` en cufflinks, para temas de exploración de datos es recomendable tener una medida de figura cuadrado y quitar las leyendas. Ejm: ,dimensions=[700,700].
  
* **`,template="plotly_dark"`** : Modificara el color del tema de fondo de los gráficos.

## [11. collections ("*clt*")](#indice)
Este módulo implementa tipos de datos para contenedores especializados que proporcionan alternativas a los contenedores integrados de uso general de Python, dict, list, set, and tuple.

* **`namedtuple()`**: función factory para crear subclases de tuplas con campos con nombre

* **`deque`**: contenedor similar a una lista con appends y pops rápidos en ambos extremos

* **`ChainMap`**: clase similar a dict para crear una vista única de múltiples mapeados

* **`Counter`**: subclase de dict para contar objetos hashables. Convierte una lista de datos en diccionario con sus repeticiones como valor.

* **`OrderedDict`**: subclase de dict que recuerda las entradas de la orden que se agregaron

* **`defaultdict`**: subclase de dict que llama a una función de factory para suministrar valores faltantes

* **`UserDict`**: envoltura alrededor de los objetos de diccionario para facilitar subclasificaciones dict

* **`UserList`**: envoltura alrededor de los objetos de lista para facilitar la subclasificación de un list

* **`UserString`**: envoltura alrededor de objetos de cadena para facilitar la subclasificación de string

## [12. selenium](#indice)
Esta Libreria permite poder hacer "**Web Scraping**" para jalar datos de paginas en internet. Para ello es necesario descargar el **ChromeDriver** (es un ejecutable ) para que python pueda conectarse por chrome. [ChomeDriver](https://chromedriver.chromium.org/downloads).<br>

Esta libreria tambien puede operar en paginas web desarrolladas con **JavaScript**.<br>
Considerar que esta libreria no es tan rapida a la hora de operar como si lo es la libreria **`Scrapy`**.

*pip install selenium*

```python
from selenium import webdriver
```
___
### Módulo:
Importamos el archivo "**chomedriver.exe**"

```python
    driver = webdriver.Chrome("C:/Users/Foster-PC/Downloads/Instaladores/Data Science/ChromeDriver (Web Scraping)/chromedriver.exe")
```
___
### Métodos:

* **``get.("URL")``** : Cargará la página en el **"chromedriver.exe"**.
* **``driver.quit()``** : Cerrará el **"chromedriver.exe"**.
* **``driver.find_element_by_xpath()``** : Buscara elementos por su "**XPath**":
```python
          ('//tag[@atributo="Valor"]')
```
* **``driver.find_element_by_class_name()``** : Buscara elementos por su tag "**Class**".
* **``driver.find_element_by_id()``** : Buscara elementos por su tag "**id**".
* **``driver.find_element_by_tag_name()``** : Buscara elementos por su tag. Ejm: "**tr**" (Table Rows).
* **``...click()``** : Ejecuta un click en el elemento previamente encontrado.
___
### Propiedades:
Las propiedades en python trabajan con **[ ]**.
* ``.page_source`` = Muestra todo el codigo `html` de la página en formato de texto sin respetar las indentaciones. Podemos asignar esto a una variable para posteriormente trabajarlo en BeautifulSoup.
___
Para Seleccionar Listas desplegables es necesario la clase **`Select`**:

```python
    from selenium.webdriver.support.ui import Select
```
```python
    box = driver.find_element_by_class_name("panel-body")
    #En este caso el atributo es un "id" usamos "element_by_id"
    dropdown = Select(box.find_element_by_id("country")) #Encontrar 1 solo Element.
    #selecciona segun el texto que aparece en la lista despegable.
    #tambien puede ser por indice con "select_by_index("0")" Contando desde el "0"
    dropdown.select_by_visible_text("Spain") # 1 solo Element.
```
___
Para no tener errores con respecto a los tiempos de carga de la pagina tras las acciones que realizamos usamos:

```python
    import time
    
    time.sleep(5)
    #Con esto nos aseguramos que despues del click duerma 5 segundos hasta
    #recibir el proximo comando.
```

## [13. BeautifulSoup ("*bs*")](#indice)
Esta Libreria permite interpretar los datos y leer el código fuente de los datos extraido de la pagina web. Considerar que esta libreria no opera con paginas web desarrolladas con **JavaScript**<br><br>
*pip install beautifulsoup4*

* A la hora de extraer elementos de los HTML es importante respetar la jerarquía de estos:

  1. ID 
  2. Class name
  3. Tag name, CSS Selector
  4. Xpath

```python
from bs4 import BeautifulSoup as bs
```
___
### Clase:
* **`bs()`** : Pondremos dentro del paréntesis la **variable** donde previamente tenemos asignado el código fuente de la página web.
Es necesario usarlo con `Request` para que extraiga el URL en formato de texto, junto con el uso de un **Parse** ("lxml").
```python
        soup = bs(requests.get("URL").text, 'lxml')
        
```
___
### Métodos:

* **`.find_all()`** : Buscará y Traerá más de un elemento de la web, todo lo que le indiquemos.Para traer todos los links de la página usamos ("a"). En html asi se llama a los links con "a".<br>
*("tipo de elemento, atributo={"":""}<-"class=postingCardInfo")*
```python
        soup.find_all("div", attrs={"class":"postingCardInfo"})
```
* **`.find()`** : Buscará y Traerá de la web un elemento que le especifiquemos. (Una linea de código).
* **`.prettify()`** : Muestra el código HTML de manera mas legible respetando sus Indentaciones.
* **`.get_text()`** : Convertirá el resultado a formato de texto (string). Modifica a la variable contenedora. Hace lo mismo que la Propiedad "`.text`".

 ```python
        .get_text(strip=True, separator=' ')
        #Elimina los saltos de linea y separa el texto en espacios en blanco.
 ```
* **`.find_next_sibling(Tag)`** : Se usa cuando no hay una referencia (nodo) o ancla que anteceda a nuestro nodo. Buscará un nodo  teniendo como antecesor a un nodo hermano al mismo nivel dentro del código HTML. Para ello necesitamos un ancla a definir de la siguiente manera:

 ```python
        ancla = soup.find('div', id='product_description') 
        parrafo = ancla.find_next_sibling('p').get_text(strip=True)

 ```
___
### Propiedad:
* **`.text`**: Convertirá el resultado a formato de texto (string). Modifica a la variable contenedora.

___
### Tags:
* **`.title`**: Mostrar Titulo de la pagina web.


## [14. scrapy ("*scp*")](#indice)
Esta no es una libreria, es un **FrameWork** el cual es la herramienta más completa para **WebScraping**. Es más rapida debido que puede ejecutar solicitudes a paginas web en paralelo (spiders).

Esta libreria puede operar con paginas web desarrolladas con **JavaScript**.
Considerar que requiere un curva de aprendizaje. <br><br>
*pip install scrapy* <br>

```python
import scrapy
```
___
### HTML:
Es un lenguaje de marcado, define la estructura y el significado de una pagina web.
Para indagar elementos en la Página Web podemos usar **Ctrl + f**.
Ciertos elementos claves para indagar en el código HTML.<br><br>

Tiene la siguiente forma:<br> 
    ``<(tag) (atributo)=(valor atributo) > (contenido) <(tag)>``

```html
          <h1 class="title"> Titanic (1997) </h1>
```
* Tags:
  * **``<head>``** : Cabeza del html.
  * **``<body>``** : cuerpo del html.
  * **``<header>``** : Contiene una introduccion al contenido y suele ir en la parte superior del cuerpo.
  * **``<article>``** : Contiene articulos.
  * **``<button>``** : Refiere a un boton en la pagina para hacer "**click**"
  * **``<p>``** : Contiene parrafos de articulos, noticias, etc.
  * **``<h1>``** : Contiene al titulo del html.
  * **``<div>``** : Se refiere a divisor. Es un contenedor genérico.
  * **``<nav>``** : Se refiere a navegación. Contiene a la barra de paginación Ejm: pag 1,2,3,...99.
  * **``<li>``** : Contiene elementos de una lista.
  * **``<a>``** : Se refiere a ancla (anchor: enlace). Este contiene al atributo `href` que son enlaces.
  ```html
          <a href="http://example.com"> Texto </a>
  ```
  * **``<table>``** : Representa a una tabla que contiene filas y columnas con la data para ser extraida. Esta contiene 2 elementos:
    * **``<tr>``** : Table row. Son las filas de la tabla.
      * **``<td>``** : Table Data. Son los datos de la tabla.
 
  * **``<ul>``** : Representa a una lista desordenada.
  * **``<iframe>``** : Permite colocar una página dentro de otra página. Hace complicado hacer Web Scraping debido a que tenemos que cambiar de un `<iframe>` a otro, no es muy común encontrar este tag.
<br><br>
* Formato de Búsqueda:
Contiene la siguiente leyenda para filtrar la informacion:

  * **``/``** = Se refiere a un nodo Ejm: <"div">; <"ul">; <"label">; va de uno en uno cuando ruteamos ('/head/h1').
  * **``//``** = Indica saltar nodos hasta llegar a la coincidencia. 
  * **``[]``** = busca un elemento dentro del nodo indicado.
  * **``@``** = indica atributo como "Class" que esta igualado a algo.

En consola de html:
  ```html
          $x('//article[@class="Etiquetas"]//h3/a["ejemplo"]/text()').map(x=>x.wholeText)
  ```
Para obtener las direcciones href filtramos del siguiente modo:
Usamos el atributo al final /@href y en map buscamos por valor (value).
___
  ```html
          $x('//ul[@class="nav nav-list"]/li/ul/li/a/@href').map(x=>x.value)
  ```
* Estado de Respuestas:
Los códigos de estado de respuesta del HTTP indican si se ha completado satisfactoriamente una solicitud HTTP. Se agrupan en 5 clases.
  
  1. Respuestas Informativas: (100-199)
  2. Respuestas Satisfactoria: (200-299)
  3. Redirecciones: (300-399) 
  4. Error del Cliente: (400-499) puede el servidor negar la solicitud.
  5. Error del Servidor: (500-599)



### Terminal:
Escribimos `scrapy` en el terminal mostrara sus comandos. Para ejecutar cada uno de ellos es necesario escribir: **``scrapy commando [option] [args]``**

* Comandos:
  * **``bench:``** Hace un Bench para probar el rendimiento de scrapy.
  * **``fetch:``** Obtiene el marcado HTML de una página Web.
  * **``genspider:``** Crea un nuevo spider dentro de una plantilla predefinida.
  * **``runspider:``** 
  * **``settings:``** Muestra la configuracion por defecto de scrapy.
  * **``shell:``** Es un entorno de pruebas para testear nuestro código sin ejecutar a nuestro spider o crear un nuevo Spider.
  * **``startproject:``** Crea un nuevo proyecto en scrapy con sus carpetas necesarias. Ejm: ´scrapy startproject nombre_del_archivo´.   
  * **``version:``** Indica la version de scrapy.
  * **``view:``**
<br><br>

* Objetos:
  * **`response`** : Sirve para encontrar elementos (es como un Soup). Este solo puede utilizar los métodos: **``.xpath()``** y **``.css()``** para encontrar elementos den el HTML.
  * **`yield`** : Sirve para devolver valores como lo hace `return`. 
<br><br>

* Métodos:
  * **``.xpath()``** : Busca elementos dentro del HTML. Entiende el siguiente parametro.
  ```
          response.xpath('//tag[@atributo="Valor"]')
  ```
  * **``.css()``**   :
  * **``.get()``**   : Obtendra un elemento. Ya sea de la ruta del ``.xpath()`` o ``.css()``
  * **``.getall()``**   : Obtendra una lista con los elementos. Para ellos necesitaremos tener de antemano una **lista vacia** para llenarla con un ciclo `for`.
<br><br>

* Propiedades:
  * **``.body``** : Muestra el codigo HTML previamente extraido. 
<br><br>

* Parámetro:
  * **``/text()``** : Este parametro seleccion solo el texto de un nodo de codigo HTML. Para extraerlo usaremos `.get()` o `.getall()` según sea conveniente.
  ```
          response.xpath('//h1/text()').get()
  ```
___

### * startproject:
Para crear un nuevo proyecto escribiremos lo siguiente en el terminal:
```
          scrapy startproject nombre_del_archivo
```

Creara una carpeta que contendrá los siguientes archivos:

- **``scrapy.cfg``** : Este archivo correrá comandos de scrapy. Ejecutara lo que hallamos escrito dentro de los demas archivos "**py**" incluyendo el **spider**.
- **``items.py``** : Ayuda a estructurar mejor la data que extraemos. Puede ser reemplazada con la palabra clave "``yield``" para devolver elementos de la página según su estructura por defecto.
- **``middleware.py``** : Podemos añadir funcionalidades personalizadas para procesar las solicitudes y respuestas. Contiene un "spider-middleware" y un "downloader-middleware".
- **``pipelines.py``** : Almacena la data que extraemos en una base de datos (MongoDB, SQlite).
- **``settings.py``** : Podemos añadir configuraciones extras a nuestro proyecto.

Aparecera en consola 2 lineas una para situarse en el directorio que se creo la carpeta que nombramos, y la otra linea es para crear el spider

```python
    cd scrapy_tutorial #situa en la carpeta
    scrapy genspider example example.com #aqui pegamos nuestro URL de esta manera:
```
`scrapy genspider nombre_spider www.abc.com/halo` (Sin "http" y sin "/" al final).

Esto creará nuestro spider que nombramos dentro de una carpeta **spiders**.
Los **spiders** se dividen en 2 tipos:

  - scrapy-Spider : Se personaliza para extraer data de las webs.
  - CrawlSpider   : Este es para hacer **Clouding** sirve para seguir links.

___
### * shell:
Probamos una solicitud a la página web. Para salir del modo shell escribimos "`exit`".
```
          r = scrapy.Request(url='https://www.pegamoslink.com/')
```
Luego usamos el comando "`fetch`" para ver la respues de nuestra solicitud.
```
          fetch(r)
          response.body (verificamos que tenemos el codigo HTML)
```
___
### * Ejecutar "scrapy.cfg":

Para ejecutar nuestro codigo guardado en los archivos **``py``** escribimos en el terminal:

```
          scrapy crawl nombre_del_spider
```

Para Ejecutar y guardar en un archivo ``.csv`` o ``json`` escribimos el siguiente código:

```
          scrapy crawl nombre_del_spider -o nombre.csv.json
```
___
### Tags:



## [15. request](#indice)
Esta Librería permite hacer solicitudes a las paginas web. Es usado para la técnica de **Web Scraping**.<br><br>

```python
import request 
```

___
### Clase, Funcion:
* **`()`** :
___
### Métodos:
* **`.get("URL")`** : Extrae el código HTML (crudo). 
* **`.content.decode("utf-8")`** : Codifica el contenido (código HTML) en "utf-8". Codifica mejor los valores "ñ" , tildes y caracteres especiales para evitar futuros problemas.

___
### Propiedad:
* **`.status_code`** : Hace una solicitud de prueba a la página regresando el **estado de respuesta** del HTTP.

## [16. scipy (stats)](#indice)
Esta Libreria permite trabajar con regresion lineal. De esta usaremos el módulo **``stats``**<br><br>

```python
from scipy import stats
```

___
### Clase, Funcion:
* **`linregress()`**: Crea una regresion lineal de 2 variables.
```python
        reglin = stats.linregress(x=kchouse.AreaFt, y=kchouse["P.V"])
```
___
### Métodos:
___
### Propiedad:
* **`.slope`**: Arroja el valor de la pendiente."m"
* **`.intercept`**: Arroja el valor de la interseccion. Cuando "x es 0" Valor "b"
* **`.rvalue`**: Arroja el valor de la correlacion de los datos. Este evalua la precision de un modelo.

## [17. TextBlob ("*tb*")](#indice)
Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural** como análisis morfológico, extracción de entidades, análisis de opinión, traducción automática, análisis de sentimientos, etc.

 Esta librería ha sido entrenada con textos en ingles debido a ello está más optimizada para ese idioma más que otros.<br><br>
*pip install textblob*

Descargaremos el cuerpo de textos con el que `textblob` funciona. Esto incluye el poder separar las palabra por adjetivos , sustantivos; tambien separar frases y oraciones; separa oraciones del parrafo y tambien trae definicion de sinonimos y antonimos. Para ello necesitamos escribir en la consola o terminal.<br><br>
*ipython -m textblob.download_corpora*

Para importar TextBlob:

```python
    from textblob import TextBlob
```

Para cambiar el algoritmo por defecto **(pattern)** de analisis de sentimientos a otro algoritmo **(NaiveBayesAnalyzer)**.

```python
    from textblob.sentiments import NaiveBayesAnalyzer
    #Ejm:
    blob3 = TextBlob(texto, analyzer = NaiveBayesAnalyzer())
```

Para la **singularidad y pluralidad de palabras**; **Correccion de palabras mal escritas** importamos la libreria "**`Word`**" de "**``TextBlob``**".
```python
    from textblob import Word
```
___
### Clase, Funcion:
* **`TextBlob()`**: Crea o convierte un string a tipo TextBlob.

___
### Métodos:

* **`detect_language()`** : Indica el tipo de idioma del lenguaje analizado.
* **`translate(to="")`** : Traducira el texto al idioma que le especifiquemos. "es", "en", etc

#### Métodos de la Sub-Libreria Word:

* **`pluralize()`** : Arrojara el plural de la "palabra" indicada.
* **`singularize()`** : Arrojara el singular de la palabra indicada.
* **`spellcheck()`** : Sugiere mediante una lita de palabras con sus probabilidades la posible corrección de la palabra mal escrita.
* **`correct()`** : Arroja la corrección de oraciones de palabras mal escritas. 
* **`word_counts()`** : Arroja la cantidad de veces que la palabra indicada es encontrada.
___
### Propiedad:

* **`.sentences`** : Arroja cada una de las oraciones hasta donde terminan con un punto inicia otra. Segmenta por oraciones.
* **`.words`** : Arroja cada una de las palabras únicas del texto. Segmenta por palabras únicas
* **`.word_counts`** : Arroja un diccionario con cada una de las palabras únicas como **Key** y la cantidad de veces que aparecen estas palabras como **Values**.
* **`.tags`** : Arroja una lista con cada palabra segmentada indicando su tipo (Sustantivos, Pronombres, Verbos, Adjetivos, Conjunciones, Adverbios, Interjecciones, Preposiciones)
* **`.noun_phrases`** : Arroja las frases-sustantivos encontrados en el texto.

#### Propiedades de la Sub-Libreria Word:

* **`.definitions`** : Arroja el significado de la palabra en cuestion.
* **`.synsets`** : Arroja una lista de posibles sinonimos para la palabra.
___
### Parámetro:

* **`,analyzer=`** : Podemos cambiar el analizador de sentimientos a la funcion `TextBlob` . Ejm: NaiveBayesAnalyzer 
```python
    blob3 = TextBlob(texto, analyzer = NaiveBayesAnalyzer())
```

## [18. nltk](#indice)
**Natural Language Toolkit** Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural**. No requiere "pip install" python ya lo trae.<br>

Para importar nltk:

```python
    import nltk
```

Para usar "**Stop Words**" descargamos el siguiente paquete para "nltk".

```python
    nltk.download("stopwords")
```
Ahora importamos el "**stopwords**"
```python
    from nltk.corpus import stopwords
```
Para segmentar por palabras y  asignar las "**stopwords**" a una variable:
```python
    stops = stopwords.words("english")
    print(stops)
    #Podemos usar idioma "spanish" o cualquier otro.
```

## [19. spacy](#indice)
Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural**.<br>
*pip install spacy*<br><br>

Para trabajar con el idioma ingles es necesario descargar por consola:<br>
*py -m spacy download en*

Para importar spacy:

```python
    import spacy
```
Cargamos un paquete de spacy en lenguaje ingles entrenado con paginas web; tambien podemos usar cualquier otro paquete por Ejm: Textos en español entrenados con noticias ("es_core_news_sm"). A nuestra variable "nlp" (Natural Language Processing):
```python
    nlp = spacy.load("en_core_web_sm") 
    #cargamos el lenguaje ingles ("en_core_web_sm") a nuestra variable.
```
Asignamos un nlp con texto a una variable:
```python
    documento = nlp("TEXTO")
```
Para sacar de un texto sus entidades y tipos:
```python
    for entity in documento.ents:
        print(f"{entity.text}:{entity.label_}")
```
### Propiedades:

* **`.ents`** : Arroja las entidades halladas en el texto. 
* **`.text`** : Arroja el texto  en formato de string con sus comillas ('').

## [20. sklearn](#indice)
**Scikit Learn** Es una librería para **Machine Learning**.<br>
*pip install sklearn*<br>

Librerías de sklearn:

### > sklearn.datasets:

Son bases de datos que ya incorpora la librería de `sklearn` y las podemos cargar llamándolas de la propia librería de sklearn. Estos Datasets vienen la data repartida en 2 tomas `.data` y `.target`.

#### load_digits:
Esta librería trabaja con un dataset de reconocimiento óptico de digitos (números) escritos a mano.
Con un pixel de 8x8 y un rango de (0,16) valores diferentes por pixel. Este dataset funciona con arrays de 2 dimensiones.

Para importar esta librería:<br>
```python
    from sklearn.datasets import load_digits
```
Para ver la información de lo que hace el dataset importado:
```python
    digist = load_digits()
    print(digist.DESCR)
```
#### load_iris:

Son un dataset de 3 tipos de flores con 4 tipos de medidas para determinar que tipo de flor pertenecen. Vienen con sus respectivas Etiquetas.

```python
    from sklearn.datasets import load_iris
```

##### Propiedades:

* **`.target`** : Son las **etiquetas** con el resultado de lo que debe arrojar el modelo. Posteriormente lo usará para corroboración con lo arrojado por el modelo.
* **`.data`** : Se refiere a toda la lista de **datos en cuestion a aprender**. En este caso son imagenes en formato array (1x64) con las que se ha entrenado el modelo para corroborar con su respectivo target (Etiqueta).
* **`.images`** : Contiene la **data** en arrays de 8x8 **listas para ser representadas** en imagenes.

##### Tags:

* **`.shape`** : Mostrara ca cantidad de (filas,columnas), bien sea de un target, data, etc.

#### fetch_california_housing:

Dataset de precios de las casas del estado de California en 1990. Los métodos, y comandos usados en el dataset anterior funcionan para todos los datasets. 

```python
    from sklearn.datasets import fetch_california_housing
```
##### Propiedades:

  * **``dataset.feature_names``** = Muestra el nombre de las columnas del dataset para train.
  * **``dataset.target_names``** = Muestra el nombre de las columnas del target para test.
___
### > Estimadores (Modelos):
Son los que ejecutan las pruebas a los datasets(train, test) arrojando resultados de predicción.

#### Métodos Generales:

* **`model.fit()`** : Carga la data para entrena el modelo en base a la data previamente separa para el entrenamiento (train). Devuelve como resultado una expresión que indica que ha cargado el modelo para entrenar pero aún no lo ha ejecutado.

```python
    model.fit(X=X_train, y=Y_train) #Aprende del Train (1347 datos)
    #Asignamos el dataset train para el entrenamiento.
    #Epochs=1 Indica el numero de veces que debe procesar todo el set de datos.
```
* **`model.fit_transform()`** : Ajusta el X en un espacio incrustado y devuelva esa salida transformada. Usa en los datos de **entrenamiento** para que podamos escalar los datos de entrenamiento y también aprender los parámetros de escala de esos datos. Se usa para aprendizaje no Supervisado.
* **`model.transform()`** : Este método ajusta y transforma los datos de entrada al mismo tiempo y convierte los puntos de datos. Si usamos ajuste y transformación separados cuando necesitamos ambos, disminuirá la eficiencia del modelo, por lo que usamos fit_transform() que hará el trabajo de ambos.
```python
    datos_reducidos = tsne.fit_transform(digits.data)
    #Usa la Data para entrenarse sin el Target.
```
* **`model.predict()`** : Ejecutara la predicción de nuestra data que previamente separamos para esta función

```python
    prediccion = model.predict(X=X_test)
    #predice el Test (450 datos)
```
* **`model.score()`** : Mostrará que tan preciso fue nuestra predicción (X_test) con respecto al resultado que debería haber arrojado.

```python
    print(f'{model.score(X_test,Y_test):.2%}')
    #97.78%, se evaluá los arrays de imágenes (X_test) con sus targets.
```
#### 1.- sklearn.neighbors:

```python
    from sklearn.neighbors import KNeighborsClassifier
```
Esta clase usará el algoritmo de los K números mas cercanos para aprender.
Es necesario guardar la clase en una variable para usar sus métodos.

```python
    knc = KNeighborsClassifier()
```
#### 2.- sklearn.svm:

```python
    from sklearn.svm import SVC
```
El objetivo de un SVC lineal (clasificador de vectores de soporte) es ajustarse a los datos que proporciona, devolviendo un hiperplano de "mejor ajuste" que divide o categoriza sus datos. ... Además de los paquetes de visualización que estamos usando, solo necesitará importar svm de sklearn y numpy para la conversión de matriz.

```python
    svc = SVC(gamma="scale")
```
#### 3.- sklearn.naive_bayes:

```python
    from sklearn.naive_bayes import GaussianNB
```
Un algoritmo Gaussian Naive Bayes es un tipo especial de algoritmo NB. Se usa específicamente cuando las características tienen valores continuos. También se supone que todas las características siguen una distribución gaussiana, es decir, una distribución normal

```python
    GNB = GaussianNB()
```
#### 4.- sklearn.linear_model:

  ```python
      from sklearn.linear_model import LinearRegression
  ```
  Regresión lineal con sklearn:

  * **``LinearRegression()``**

    ```python
        model_rl = LinearRegression()
        model_rl.fit(X=X_train, y=y_train) #Previo "train_test_split"

        predice = (lambda x:model_rl.coef_*x+model_rl.intercept_)
    ```
    * Propiedades:
        * **``.coef_``**       : "m"
        * **``.intercept_``**  : "b"
    <br><br>
  * **``ElasticNet()``**
  * **``Lasso()``**
  * **``Ridge()``**

#### 5.- sklearn.manifold :

  ```python
      from sklearn.manifold import TSNE
  ```
T-Distributed Stochastic Neighbor Embedding
```python
    tsne = TSNE(n_components=2, random_state=11)
    datos_reducidos = tsne.fit_transform(digits.data)
```
#### 6.- sklearn.cluster:

```python
        from sklearn.cluster import KMeans
```
Para Aprendizaje de maquina no supervisado:
  * DBSCAN():
  
    ```python
            from sklearn.cluster import DBSCAN
    ```
  * MeanShift():
  
    ```python
            from sklearn.cluster import MeanShift
    ```
  * SpectralClustering():
  
    ```python
            from sklearn.cluster import SpectralClustering
    ```
  * AgglomerativeClustering():
  
    ```python
            from sklearn.cluster import AgglomerativeClustering
    ```
    
#### 7.- sklearn.decomposition:
```python
        from sklearn.decomposition import PCA
```
___
### > Comandos sklearn:

#### from sklearn.model_selection:

* **train_test_split**

    ```python
            from sklearn.model_selection import train_test_split
    ```

  * **`train_test_split()`** : Esta función nos permite dividir un dataset en 2 bloques; uno destinado al aprendizaje (**entrenamiento**) y otro para hacer las **pruebas**. Por defecto esta division le asigna 75% de los datos a  **entrenamiento** y 25% a **pruebas**.

      **Variables:** <br>

      * X_train = Hace referencia a la data en cuestión para el **entrenamiento**
      * X_test  = Hace referencia a la data en cuestión para las  **pruebas** 
      * Y_train = Hace referencia a el target de cada data en cuestión de: **entrenamiento**
      * Y_test  = Hace referencia a el target de cada data en cuestión de: **pruebas**

      ```python
          X_train,X_test,Y_train,Y_test = train_test_split(digist.data,digist.target,random_state=11)
          #(X,y,random_state=11)
          #(dataset,Etiquetas,semilla:11)
      ```
* **KFold**

    ```python
            from sklearn.model_selection import KFold
    ```

    Esta clase Segmenta la data en k partes el cual usará una de las partes para ``test`` y las otras para ``train`` hasta haber terminado de conjugar con cada una de las segmentaciones previamente dada logrando asi aumentar el entrenamiento del modelo para mejorar su precision.

    Cuando haya mucha cantidad de datos o no haya patrones difíciles por descubrir `k` será más pequeño. Este es un método de validación, el modelo final será entrenado con toda la tabla.

    ```python
        kf = KFold(n_splits= 10 , random_state= 11, shuffle= True)
        #n_splits = nº de dobleces (De acuerdo a la repartición de los datos)
        #random_state = nº de semilla
        #shuffle = barajeara los datos antes de ponerlos en los dobleces.
    ```
* **cross_val_score**

    ```python
            from sklearn.model_selection import cross_val_score
    ```
    Validación Cruzada.

    ```python
        puntuacion = cross_val_score(estimator= knc, X=digits.data, y=digits.target,cv=kf)
        #estimator = Modelo de estimador.
        # X = data (train y test)
        # y = targets (train y test)
        # cv = generador de validación cruzada (como se repartirá la data para train y test)
    ```

#### from sklearn.metrics:

* **metrics: Funciones**

  * **``metrics.r2_score()``** : Saca el r2. Esta se mide de 1- a 1 mientras más cercana a 1 indica absoluta precision del modelo en cambio con -1 es inversamente proporcional. Valores cercanos a 0 indica que no hay relación.
  ```python
                      metrics.r2_score(esperado,prediccion)
  ```
  * **``metrics.mean_squared_error()``** : Saca el Mean Squared Error (Promedio de Errores Cuadrados). Mientras más cerca de "0" indica mayor precision del modelo.
  ```python
                    metrics.mean_squared_error(esperado,prediccion)  
  ```
* **confusion_matrix**

    Para  mostrar en un array los datos acertados y erróneos del la prueba.

    ```python
        from sklearn.metrics import confusion_matrix

        confusion = confusion_matrix(y_true=esperado,y_pred=prediccion)
    ```
* **classification_report:**

    Parecido a la matriz de confusion muestra métricas de la comparación de prueba y target.

    - Precision: Verdaderos positivos/ vp + falsos positivos
    - recall (Sensitividad): Verdaderos positivos/ total positivos
    - f1-score : promedio entre Precision y recall.
    - support : Cantidad de valores reales que tenia para cada dígito para el ejemplo.

    ```python
        from sklearn.metrics import classification_report

        nombres = [str(digit) for digit in digits.target_names]
        #Pasamos los targets de formato array a string.
        print(classification_report(esperado,prediccion,target_names= nombres)) #En ese orden (espera,prediccion,targetnames=)
    ```

#### from sklearn.feature_extraction.text:

Esta Librería esta orientada a la extracción y tokenización de palabras de un texto.

* **CountVectorizer:**
  
    ```python
        from sklearn.feature_extraction.text import CountVectorizer
    ```

  Es el constructor que recibe los comandos que indiquemos para la tokenización.

  * **.fit_transform():** Este método extrae y tokeniza las palabras del texto a indicar.

    ```python
        sample_data = ["Hello Word","Hello Hello Word", "Word Word Word"] 

        vectorizer = CountVectorizer()
        x = vectorizer.fit_transform(sample_data)

    ```

  * **.get_feature_names():** Este método mostrara las palabras tokenizadas.

    ```python
        print(vectorizer.get_feature_names())
    ```

## [21. TensorFlow](#indice)
Es una librería para **Depp Learning**.<br>
*pip install tensorflow*<br>

```python
    import tensorflow as tf
```

### Keras:

Es un FrameWork encima de TensorFlow que ayuda a la creación del modelo de manera mas simplificada.

```python
    from tensorflow import keras
```

#### Datasets:

Keras trae sus propios datasets para trabajar y entrenar un modelo:

```python
    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
```

##### Models:

Eligiremos el modelo según el tipo de modelo que queramos construir teniendo en cuenta nuestros datos y target. Según el tipo de modelo que elijamos se suele guardar dentro de una variable llamada **model**.

* **`model = keras.models.Sequential([])`** : Para armar nuestras capas siempre partimos con `.Sequential`

Métodos para nuestra variable model:

* **`model.summary() `** : Nos arrojara un resumen de todas las capas y tipo de capas que hemos ingresado a nuestro `.Sequential` o model.

    ```python
        model.summary()

        # Visualizar Gráficamente en archivo png.

        from tensorflow.keras.utils import plot_model
        from IPython.display import Image

        plot_model(cnn, to_file="covnet.png", show_shapes=True,show_layer_names=True) #Guarda la imagen .png
        Image(filename="covnet.png") #muestra la imagen .png en la notebook.

        #Es necesario descargar e instalar el "graphviz".
    
    ```
* **`model.compile() `** : Compilaremos el modelo indicando el optimizador, función de error y métrica de presición.
  
    ```python
        model.compile(optimizer="adam", #Es el más balanceado de los Optimizadores
                      loss=,
                      metrics=["accuracy"])
    ```
* **`model.fit() `** : Tras asignar nuestro `.compile` entrenará el modelo.

    ```python
        history = model.fit(train_features, train_labels, validation_split=0.2, ephocs=100)
        #Entrenaremos un modelo con 100 ciclos.
        #batch_size : Indica el numero de muestras a procesar durante un "epochs"
        #validation_split : Indica que deje el 20% de los datos para validación
    ```

* **`model.evaluate() `** : Nos arrojara la presición ("accuracy") del modelo entrenado.

    ```python
        loss, accuracy = model.evaluate(train_features, train_labels, verborse=2)
        # verbose = es la elección de cómo desea ver la salida de su Red Neuronal mientras se está entrenando. valores [0,1,2]
    ```

* **`model.save() `** : Guardara nuestro modelo entrenado en un archivo `.h5`

    ```python
        model.save("modelo_1_binario.h5") #El formato es ".h5"
    ```

* **`model.load_model() `** : Cargara nuestro modelo de un archivo `.h5`

    ```python
        from tensorflow.keras.models import load_model

        model = load_model("modelo_1_binario.h5")

    ```

##### Layers:

Son las capas que van dentro de los modelos

* **`keras.layers.Input(())** : **Capa de Entrada**. En el caso de trabajar con los DataFrames nuestra entrada seria el **nº de columnas** (features) que tendrá en cuenta el modelo para predecir.<br>
**keras.layers.Input((None,nºColumnas_df valor "X",nºfilas_df valor "Y"))**
  
      ```python
        keras.layers.Input((None,18,)) #Dejamos la coma en "Y" indicando que puede tomar todos los valores de la fila.
        
    ```
  
* **`keras.layers.Dense()** : **Capas Ocultas**. Aquí podremos elegir el **nº de capas** con el **nº de neuronas** indicando su respectiva **función de activación** por capa (RELU suele ser la más usada).

    ```python
        keras.layers.Dense(64, activation="relu")
        
    ```
 * Con este mismo método indicaremos nuestra capa de salida: Referenciamos nuestro target, 1 neurona por columna a predecir. Por ejemplo si nuestro objetivo era predecir el valor de una columna tendrá 1 sola neurona como resultado y si ese resultado es binario (1=si, 0=no) eligiremos nuestra **función de activación SIGMOIDE **
    
    ```python
        keras.layers.Dense(1, activation="sigmoid") #En caso de una salida binaria.
        
    ```

* **`keras.layers.Dropout()** : **Elimina Neuronas**. Indica el % de neuronas que se eliminaran al saltar de una capa oculta a otra.

    ```python
        keras.layers.Dense(64, activation="relu")
        keras.layers.Dropout(0.2) #20% de eliminación de neuronas.
        keras.layers.Dense(128, activation="relu") #Por ello aumentamos el nº de neuronas
        
    ```
  

#### Losses:

Hace referencia a la Función de Error a elegir. El cual indicará el comportamiento del modelo, como aprendió el modelo. Sirve para ver si hubo overfeeding o algún otro problema a la hora del aprendizaje.<br>

Esta es la etapa de compilación del modelo por eso usamos el método `.compile`:

```python
model.compile(optimizer="adam", #Es el más balanceado de los Optimizadores
              loss=,
              metrics=["accuracy"])
```

* **``keras.losses.binary_crossentropy``** : Se suele usar para modelos con un valor de salida binaria.

```python
    model.compile(optimizer="adam",
                  loss=keras.losses.binary_crossentropy,
                  metrics=["accuracy"])
```
#### Entrenamiento del Modelo:

Guaramos en una variable el entrenamiento del modelo, para ejecutarlo usaremos el método `.fit`. El objetivo es que aprenda el modelo para que con una tabla de features nueva pueda predecir los valores de la columna target.<br>
Guardamos este entrenamiento del modelo en una variable **history** para luego usar sus métodos `.history.keys()`.
**model.fit(Tabla de las columnas del df a entrenar, Tabla con la columna target, validation_split=0.2, ephocs=100)**

```python
    history = model.fit(train_features, train_labels, validation_split=0.2, ephocs=100)
    #Entrenaremos un modelo con 100 ciclos.
    #batch_size : Indica el numero de muestras a procesar durante un "epochs"
    #validation_split : Indica que deje el 20% de los datos para validación
```
___
### Evaluación del Modelo:

Nos arrojara la presición ("accuracy") del modelo entrenado.

```python
    loss, accuracy = model.evaluate(train_features, train_labels, verborse=2)
    # verbose = es la elección de cómo desea ver la salida de su Red Neuronal mientras se está entrenando. valores [0,1,2]
```

### Gráfica del aprendizaje:

Evaluaremos gráficamente como aprendió nuestro modelo. Verificaremos que el "model accuracy" este lo mas cercano a 1 y en contra parte el "model loss" tiene que estar lo mas cercano a 0. Teniendo como referencia el accuracy que nos arrojo el método ``model.evaluate()``.

El método "fit" nos devuelve un diccionario que guardamos en nuestra variable **"history"** con los resultados de la entrenamiento. El **``"val_loss"``** y el **``"val_accuracy"``** son para los datos de **validación** que hemos indicado con el parámetro de **``validation_split=0.2``**, el cual usa el 20% de los mismos datos para evaluar y arrojarnos el **``loss``** y **``accuracy``**.

Con esto se busca evitar el **overfitting** (Sobre-Entrenamiento) para que el modelo no memorize los resultado, sino aprenda. Esto suele suceder cuando cargamos con demasiados **Ephocs** al entrenamiento. En caso contrario de poner muy poco **Ephocs** tendríamos lo que se conoce como **underfitting**

```python
    history.history.keys() #arroja el nombre de las columnas de la evaluación: (["loss","accuracy","val_loss","val_accuracy"])
    import matplotlib.pyplot as plt

    #Model Accuracy
    plt.plot(history.history["accuracy"])
    plt.plot(history.history["val_accuracy"])
    plt.title("Model Accuracy")
    plt.ylabel("Accuracy")
    plt.xlabel("Epoch")
    plt.legend(["Train","test"], loc="upper left")
    plt.show()

     #Model Loss
    plt.plot(history.history["loss"])
    plt.plot(history.history["val_loss"])
    plt.title("Model Loss")
    plt.ylabel("Loss")
    plt.xlabel("Epoch")
    plt.legend(["Train","test"], loc="upper left")
    plt.show()

```
### Predicción

Tras tener nuestro modelo entrenado podemos cargarle nuevos datos al modelo según los campos de entrenamiento para que nos prediga nuestra columna target.

```python
    predictions = model.predict(test_features) #variable con nuevos valores de los campos de entrenamiento.

```

### Exportar Modelo

Exportara el modelo entrenado en un archivo `.h5` con el fin de no volver a gastar recursos para volver a entrenar.

```python
    model.save("modelo_1_binario.h5") #El formato es ".h5"
```
Para cargar un modelo:

```python
    from tensorflow.keras.models import load_model

    model = load_model("modelo_1_binario.h5")

```