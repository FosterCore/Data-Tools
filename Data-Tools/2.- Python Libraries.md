# *Librerias*

## Indice:

* [pip](#pip)
* [os](#os)
* [pathlib](#pathlib--path)
* [1. math](#1-math)
* [2. statistics](#2-statistics-st)
* [3. random](#3-random-rd)
* [4. numpy](#4-numpy-np)
* [5. re](#5-re-regex)
* [6. pandas](#6-pandas-pd)
* [7. openpyxl](#7-openpyxl)
* [8. matplotlib](#8-matplotlib-plt)
* [9. seaborn](#9-seaborn-sns)
* [10. plotly](#10-plotly-px)
* [11. collections](#11-collections-clt)
* [12. selenium](#12-selenium)
* [13. BeautifulSoup](#13-beautifulsoup-bs)
* [14. scrapy](#14-scrapy-scp)
* [15. request](#15-request)
* [16. scipy](#16-scipy-stats)
* [17. TextBlob](#17-textblob-tb)
* [18. nltk](#18-nltk)
* [19. spacy](#19-spacy)
* [20. sklearn](#20-sklearn)
* [21. TensorFlow](#21-tensorflow)
* [22. Camelot](#22-camelot)
* [23. Tabula](#23-tabula)
* [24. geopandas](#24-geopandas)
* [25. polars](#25-polars)
* [26. hvplot & panel](#26-hvplot--panel)


## [pip](#indice)

"Pip Installs Packages", es utilizado para instalar y gestionar paquetes o módulos de python.

El "pip" va ligada con la version de python que estemos trabajando, por lo que si tenemos varias versiones de python habrá varias versiones de pip. Para priorizar el pip que queremos usar, debemos subir en prioridad la ruta donde este la carpeta python de la version que queremos y ubicarlo en lo mas alto (primeras lineas). De no funcionar este método podemos cambiar el nombre de pip o eliminar ya que se puede acceder a el según los nombres de los archivos que se encuentren en la carpeta:<br><br>

"C:\Users\Foster-PC\AppData\Local\Programs\Python\Python311\Scripts\" (aquí esta instalado "pip")

Y puedes estar con los siguientes nombres: "pip", "pip3" o "pip3.11".

Para no crear conflicto solo nos quedaremos con pip de la version que queremos como principal, los demás "pip" los eliminamos y nos referiremos a ellos cuando querramos instalar un paquete indicando su version ejemplo:<br><br>

```
pip3.9 install pandas

#Podemos instalar o actualizar en una linea varias o una librería en especifico:
pip install --upgrade pip #Podemos actualizar pip con el mismo código.
pip install --upgrade pandas
pip install --upgrade requests urllib3 chardet charset_normalizer
pip install -U pandas #Otra manera de escribir

#Instalar solo en el usuario del sistema
pip install --user pandas
python -m pip install --user pandas #otra manera

#Para ver la lista de librerías que tienen actualización pendiente.
pip list --outdated

#Actualizar todas las librerías que tienen actualización pendiente.
pip list --outdated | ForEach-Object { pip install --upgrade $_.split()[0] }


#Para ver la version de la Libreria que tenemos instalado.
pip show pandas

#Muesta todas las librerias que tenemos instaladas (solo nombres, su version es referencial)
pip list

#Muesta todas las librerias y su version que tenemos instaladas
pip freeze

#Guardamos en un archivo .txt todos los nombres de las librerias que tenemos instaladas.
pip freeze --no-version > requirements.txt
pip freeze | awk -F'==' '{print $1}' > requirements.txt


```

### Entorno Virtual
Es un espacio para poder instalar las librerías necesarias para que nuestro scrip funcione correctamente de manera eficiente.

Para ello abrimos el terminar de **VSCode** e instalamos la librería de `virtualenv`. Todos los siguientes comandos se deben ejecutar en el terminal.

* **Crear un Entorno Virtual** :
    ```
        virtualenv -p py Nombre(suele ser 'env')
    ```
* **Activar el Entorno Virtual** : Cuando aparece activo sale el nombre del entorno virtual en paréntesis en el terminal posteriormente a nuestra ruta. Seguidamente después de activarlo podemos empezar a instalarle librerías que solo quedaran guardadas en el entorno virtual.
    ```
        .\env\Scripts\activate
        .\env\Scripts\deactivate
    ```
* **Desactivar el Entorno Virtual** : Si hemos activado el EV significa que estamos dentro de él por lo tanto solo digitando `deactivate` nos saldremos del EV.

* **Exportar Librerías del Entorno Virtual** : Creara un archivo llamado `requirements.txt` con los requerimientos de las librerías previamente instaladas en nuestro EV. Cabe Resaltar que también podemos exportar las librerías instaladas en nuestro equipo en `pip` en caso no estemos en un entorno virtual.
    ```
        pip freeze > requirements.txt

        pip freeze | awk -F'==' '{print $1}' > requirements.txt  #Exporta solo los nombres sin especificar los números de versiones.

    ```
* **Importar Librerías del Entorno Virtual** : Al migrar a otra maquina o EV copiaremos todas las demás carpetas (incluyendo requirements.txt) excepto la carpeta **env**. Importaremos todas las librerías indicadas en el archivo `requirements.txt` de la siguiente manera.
    ```
        pip install -r .\requirements.txt
        ò
        pip install -r req.txt
    ```


## [os](#indice)
El módulo os de Python le permite a usted realizar operaciones dependiente del Sistema Operativo como crear una carpeta, listar contenidos de una carpeta, conocer acerca de un proceso, finalizar un proceso, etc. Esta librería trabaja retornando valores de **Strings** con el **shell** activado o en modo True (el cual puede ser un riesgo latente).

```python
import os
```
***``Windows limita en número de caracteres a 260, tanto OS y Pathlib no podrán leer archivos con nombres tan largos mayores a 260 caracteres``***

Es importante aclarar que python tiene como carácter reservado el " **\\** " por ello debemos reemplazar el símbolo " **/** " o digitar un doble " **\\** **\\** " para anularlo u lea como un string. Otra manera es también pegar la dirección tan cual antecediéndolo con un r-String. Aquí las siguientes soluciones:

 ``"C:/Users/Foster-PC/"``<br>
 ``"C:\\Users\\Foster-PC\\"``<br>
 ``r"C:\Users\Foster-PC\"`` #Esta es la solucionas más sencilla pegamos la dirección y le agregamos r'

Otra manera de convertir el carácter a " **/** ".

```python
(Variable_path).replace("\\","/")
```

Para mover carpetas y archivos a una ruta en especifica usamos:
```python
import shutil

#CarpetaExistente.
shutil.move('C:/Users/Foster-PC/Desktop/Archivo.py', 'C:/Users/Foster-PC/Desktop/Nueva Carpeta')

# Para eliminar árboles de directorios completos con todos sus archivos dentro.
shutil.rmtree('C:/Users/Foster-PC/Desktop/Nueva_Carpeta').
#Si tenemos el directorio grabado en una variable no dejara borrarla.
```
* CMD:
  * Ejecutar un script de python:
  ```cmd
          C:\Users\Foster-PC\Documents\Visual Studio Code\Learning>py nombre.py
  ```
  * Comandos del sistema:
    * **`cls`** : Limpia la consola. También puede usarse `clear`
    * **`cd`** : Entrar y movernos a un directorio. Para entrar a carpetas cuyo nombre tiene espacios en blancos se debe escribir el nombre entre comillas. ***>cd "Tutoriales Internet"***
      ```
                >cd ("Tutoriales Internet/Tutorial 0") #para ingresar por más directorios a la vez.

      ```
    * **`cd../`** : Salir de un directorio. Ejm: ***cd../../../*** (Regresara 3 directorios hacia atrás.); ***cd/*** (Saldrá hasta la raíz "C:")
      ```python
                ('./../practica') # De la ubicación actual ("./") Retrocede un directorio ("../")

      ```
    * **`pwd`** : Muestra el directorio donde nos encontramos actualmente.
    * **`dir`** : Muestra el contenido de un directorio archivos y carpetas (DIR).
    * **`tree`** : Muestra el contenido y sub carpetas en forma de árbol raíz.
    * **`mkdir`** : Crea una carpeta en el directorio actual donde nos ubicamos. También puede usarse `md`
    * **`rmdir`** : remueve una carpeta en el directorio actual donde nos ubicamos. También puede usarse `rd`
    * **`del`** : Elimina un archivo.abc o una carpeta en el directorio actual donde nos ubicamos.

___
### Clase, Función:

* **`os.system()`** : Ejecuta comandos al sistema (cmd). Ejm: ('dir'). A diferencia de la librería ``subprocess`` aquí si se puede ejecutar los comandos tal cual se digitan con sus espacios. Es necesario ejecutarlo en archivos `.py` para ver el resultado en la consola. Usa el path actual por defecto:
```
               os.system('py --version')
```
* **`os.listdir()`** : Crea una lista con los archivos encontrados en un directorio. Podemos indicar dentro del paréntesis que carpeta queremos que haga la lista de sus archivos contenidos. Arranca buscando del directoria actual donde nos encontramos. Esta función es muy usada para enlistar archivos.
```python
          os.listdir('C:/Users/Foster-PC/Documents/Visual Studio Code/Learning/Beat Data - Tutorials/sales')
          #Creara una lista con todos los archivos .csv dentro de la ruta indicada.
```
Otra manera partiendo de la Ruta de Trabajo
```python
          files = os.listdir('../files') #Lista con el nombre de cada archivo.csv
          #los 2 puntos indican una carpeta antes
          df=pd.DataFrame() #vacío
          for x in files:
              file=pd.read_csv('files/'+x) #Directorio de cada archivo.
              df=pd.concat([file,df]) #añade cada Dataframe creado.
```

* **`os.getcwd()`** : Muestra la ruta del archivo python en el que estamos trabajando (Ruta de trabajo). Lo muestra en Formato **" \ "** por ello debemos utilizar un `.replace("\\","/")` para usarlo en una cadena lógica de comandos.
* **`os.chdir()`** : Este método cambia el directorio de trabajo actual a la ruta dada.
  ```python
          path = "C:/GoogleDrive/06/archivos_unificar

          os.chdir(path)
          # %pwd o os.getcwd() para verificar que nuestra ruta se ha movido.
  ```
* **`os.path.join()`** : La función `.join()` es usada para que diferentes sistemas operativos puedan correr el código tanto como en windows ( \ ) como en Mac o Linux ( / ). En conclusion Esta función **arroja una ruta** como resultado.
```python
          os.path.join(os.getcwd(), 'Carpeta') #Del directorio añadirá '/Carpeta' a la ruta.
          #Esto no crea una carpeta con el nombre 'Carpeta'.

          #Tambien podemos hacer lo mismo usando ".curdir" el cual arroja el directorio actual:
          root_logdir = os.path.join(os.curdir,"my_logs")
          #output: '.\\my_logs'
```
* **`os.mkdir()`** : Creará una nueva carpeta, utiliza el path o directorio actual por defecto. Necesitaremos indicarle la ruta adentro incluyendo la carpeta que queremos crear considerando usar " / "
```python
          os.mkdir(os.path.join('./../../','Nueva_Carpeta')) #leerá una ruta adentro donde creara la carpeta.
          os.mkdir('C:/Users/Foster-PC/Desktop/Nueva Carpeta') #Podemos poner la ruta entera también.

          #Comprobando que un ruta exista, caso contrario crea la carpeta
          ruta: "/ruta/a/directorio"

          try:
            os.chdir(ruta)  # Intenta cambiar al directorio para comprobar si existe
            print(f"El directorio '{ruta}' ya existe.")
          except FileNotFoundError:
            os.makedirs(ruta)  # Si no existe, crearlo
            print(f"Directorio '{ruta}' creado con éxito.")

          #Otra manera con if not, else:

          if not os.path.exists(ruta):
              os.makedirs(ruta)
              print(f"Directorio '{ruta}' creado con éxito.")
          else:
              print(f"El directorio '{ruta}' ya existe.")

```
* **`os.makedirs()`** : Creará varias carpetas según la ruta que le especifiquemos. Utiliza el path o directorio actual por defecto. Necesitaremos indicarle la ruta adentro incluyendo la carpeta que queremos crear considerando usar " / ".
```python
          os.makedirs(os.path.join('C:/Users/Foster-PC/Desktop','Nueva Carpeta','Carpeta Hijo'))
          #creara "Carpeta Hijo" dentro de "Nueva Carpeta". Creara a ambas.
```
* **`os.removedirs() o rmdirs()`** : Elimina carpetas vacías, de uno en uno. No elimina conjunto de carpetas. Si tenemos el directorio grabado en una variable o nuestro proyecto se sitúa en esa carpeta no dejara borrarla.
```python
          os.rmdirs('C:/Users/Foster-PC/Desktop/Nueva_Carpeta')

```
* **`os.rename()`** : Renombrará a una carpeta. (nombre del archivo, nuevo nombre). Tanto **OS** como **Pathlib** no funcionaran correctamente con rutas o nombres mayores a 260 caracteres.
```python
          for file in os.listdir():
            if file.endswith('.csv'):
              os.rename(file, f'2021_{file}')

          #Otro Ejemplo Limpiando el nombre con R.E
          import re
          reg = re.sub("#","",i)
          os.rename(f"{path}/{i}", f"{path}/{reg}") #funciona con path y sin el.
```
* **`os.remove()`** : Solo eliminar archivos tras una ruta dada.
  ```python
    os.remove("C:/Users/Foster-PC/Downloads/Descargas Jdowloader 2/Automatizar SAP - PYTHON/0.mp4")

  ```
* **`os.path.exists()`** : Muestra con un booleano (True or False) si un archivo.py o una carpeta existe.
* **`os.path.abspath()`** : Muestra la ruta absoluta de la carpeta o archivo.py en la ruta de trabajo que le indiquemos.
* **`os.path.realpath()`** : Muestra la ruta real donde se encuentra la carpeta o archivo.py que le indiquemos. (resuelve los enlaces simbólicos) Por ello suelen usar:
```python
          os.path.abspath(os.path.realpath(__file__))
          #Muestra la ruta incluyendo las carpetas y el archivo.py

```
* **`os.path.basename()`** : Toma la ruta y nos arroja solo el nombre con su extension:
  ```python
            a = os.path.basename("./ruta/archivo.csv")
            #output: a = "archivo.csv"
  ```
* **`os.path.splitext()`** : Dividida a un archivo por su nombre y extension.
  ```python
            archivo, ext = os.path.splitext(archivo)

            #Podemos elegir quedarnos solo con el nombre discriminando la extension.

            Numero_facturas = [os.path.splitext(i)[0] for i in factura_pdf]
  ```

___
### Métodos:
___
### Propiedad:

* **`os.extsep`** : Referencia al "." (lo ubica) que divide la extensión de un archivo.
  ```python
            print("hola.abc".split(os.extsep))
            #Cortara por la extension del archivo "."
  ```

## [pathlib > Path](#indice)
Esta Librería, al igual que con `os` permite trabajar con nuestro directorio y comandos del sistema, ya sea creando carpetas en nuestro sistema, etc. La diferencia con `os` es que `pathlib` los resultado que arroja no son de formato `str` sino que son de formato `path`.

Es importante aclarar que esta librería leerá las rutas de los directorios con el siguiente carácter " **/** " :

 ``C:/Users/Foster-PC/``

Python 3 incluye el módulo ``pathlib`` para manipular rutas de sistemas de archivos de forma agnóstica en cualquier sistema operativo. El módulo ``pathlib`` es similar al os.path, pero ``pathlib`` ofrece una interfaz de nivel más alto, y, a menudo, más conveniente, que os.path. [Tutorial pathlib](https://www.digitalocean.com/community/tutorials/how-to-use-the-pathlib-module-to-manipulate-filesystem-paths-in-python-3-es) <br><br>

```python
from pathlib import Path
```
La contraparte de la función ´os.path.join()´ es PurePath para tener compatibilidad de código con diferentes sistemas operativos.
La diferencia es que como `PurePath()` es una clase esta la podemos usar para encapsular todo los comandos bajo la clase `Path()`, haciendo de esta manera todo nuestro código con `Path()` compatible con cualquier sistema operativo.
```python
from pathlib import PurePath
```
___
### Clase, Función:

Normalmente suelen guardarse en una variable la clase `Path('ruta')` con la ruta de la carpeta o archivo.py a trabajar.

* **`PurePath(Path())`** : Convertirá el código `Path` compatible con cualquier sistema operativo.

* **`Path('C:Ruta/')`** : Esta clase representa **una ruta** del sistema de archivos. Indicaremos los directorios en donde queremos que se situé el `Path`. El **Path** se encontrara por defecto en nuestra Ruta de trabajo. Con respecto a las funciones y métodos no importa donde este ubicado el `Path()` siempre se ejecutaran los comando de la ruta que indiquemos.
* **`Path.cwd()`** : Muestra la ruta del archivo python en el que estamos trabajando. Ruta de trabajo

___
### Métodos:
Muchos de los métodos arrojaran como resultado un generador de objeto. Para poder ver el resultado necesitamos encerrar el código en una **clase** **`list()`**.

* **`.iterdir()`** : Crea una lista con los archivos encontrados en un directorio (Path() toma por defecto el path actual de nuestro trabajo). Podemos indicar dentro del paréntesis de la clase **`Path('Carpeta_Prueba')`** que carpeta queremos que haga la lista de sus archivos contenidos. Arranca buscando del directoria actual donde nos encontramos. Debido a que el resultado es formato ``Path`` necesitamos pasarlo a formato ``list``. Esta método devuelve en formato path una lista con los elementos a diferencia de `os.listdir` que devuelve ya los elementos dentro de una lista.
```python
              list(Path('Carpeta_prueba').iterdir()) #list es solo para verlo por pantalla.
              list(Path.iterdir(Path('C:/Users/Foster-PC/Documents/Visual Studio Code/Learning/Tutoriales Internet/Tutorial 0')))
              #Esta es otra manera de indicar el Path. Ejecutara entorno a la ruta.
```
* **`.joinpath()`** : Esta función **arroja una ruta** como resultado. Puede usarse con la función ``Path`` o ``PurePath`` con este ultimo funcionará el código en varios sistemas operativos. En conclusion Esta función **arroja una ruta** como resultado uniendo (Path(), "nombre") *(Por mas que creamos una ruta con nombres con directorios que no existan no creara las carpetas hasta que usemos el método mkdir() o mkdir(parents=True) para mas de 1 carpeta.)*
```python
          print(Path.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # Path.joinpath(Path(),'Nueva Carpeta')
          print(PurePath.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # PurePath.joinpath(Path(),'Nueva Carpeta')
          print(PurePath.joinpath(Path.cwd(),'Carpeta'))
```

* **`.mkdir()`** : Crea una carpeta. Propiedades:
  *  ``(exist_ok= True or False)`` : Indicara si ya existe una carpeta con ese nombre, el código no se rompa y no arroje error sino lo omita.
    ```python
              print(Path('Nueva_carpeta').mkdir(exist_ok=True)) #usando el path actual.

              print(Path('C:/Users/Foster-PC/Desktop/Nueva Carpeta').mkdir(exist_ok=True))
               #Especificando el path con el nombre de la carpeta a crear.
              print(PurePath.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # PurePath.joinpath(Path(),'Nueva Carpeta')
    ```

  *  ``(parents= True or False)`` : Creara mas de una carpeta con el parámetro en `True`. Para lo cual nuestra ruta debe contener una carpeta nueva con otra que sea hijo (dentro de carpeta nueva).
    ```python
              print(Path.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta','Carpeta Hijo').mkdir(exist_ok=True,parents=True))
    ```
* **`Path.rename()`** : Cambia el nombre de path1 a path2.
```python
          Path.rename(Path('C:/Users/Foster-PC/Desktop/Nueva Carpeta'),Path('C:/Users/Foster-PC/Desktop/New Directory'))
```
* **`Path.rmdir()`** : elimina la carpeta situado en el path indicado.
```python
          Path.rmdir(Path('C:/Users/Foster-PC/Desktop/New Directory'))
```
* **`Path.exists(Path())`** : Indica con un booleano si el path indicado existe.
* **`Path.resolve(Path())`** : Arroja la ruta absoluta de la carpeta o archivo.py indicado previamente en la ruta.
* **`Path.stem(Path())`** : Arroja solo el nombre, sin la extension, de la carpeta o archivo.py indicado previamente en la ruta.
* **`Path.suffix(Path())`** : Arroja solo la extension de los archivo.py indicado previamente en la ruta.
* **`Path.stat(Path()).st_size`** : Arroja el tamaño de los archivo.py indicado previamente en la ruta.

___
### Propiedad:

## [1. math](#indice)
Es una librería que brinda funciones para operaciones matemáticas clásicas como: **`.log`**

## [2. statistics ("*st*")](#indice)
Es una librería que brinda funciones para cálculos estadísticos como: **`.mean`**, **`.median`**, **`.mode`**, **`.stdev`**, **`.pstdev`**, **`.variance`**, **`.pvariance`**, etc.

## [3. random ("*rd*")](#indice)
Es una librería que ofrece generadores de números pseudo-aleatorios para varias distribuciones:

* **`.randrange()`** : Selecciona un item de manera aleatoria indicado (1,7)
* **`.randint()`** : Crea números aleatorios en el rango especificado con una distribución "Uniforme Discreta". Ejemplo para un Array de numpy:
```python
    Array_1 = np.random.randint(1, 100, size=1000)
    #Creara un array con valores del 1-100 con 1000 datos dentro del array.
```
* **`.uniform()`** : Crea números aleatorios de números decimales en el rango especificado. Ej: (1, 100, size=1000)

## [4. numpy ("*np*")](#indice)
**Numeric Python**. Es una librería que brinda la función de trabajar con matrices, algebra lineal, estadística para análisis de datos.<br>
Esta librería solo permite trabajar con datos homogéneos bien sea con números o con letras. Y trabaja solo con indices "0" ahi es donde entra `pandas`.
Incluye la librería **random**.

* Cabe resaltar que para numpy cada cierre de **"`[]`"** es una fila.
* El punto de origen de las matrices es la coordenada **(0,0)** Empezando por fila y columna.

### Tipos de Data:
* **`np.int64`**
* **`np.float32`**
* **`np.complex`**
* **`np.bool`**
* **`np.object`**
* **`np.string`**
* **`np.unicode_`**
---
### Indexado:
*  Con **[x,y]** traera de un dato entre filas y columnas (coordenada)
*  Con **[[x,y]]** traera las filas específicas señaladas.
*  Con **[x:y]** traera de un rango continuo de solo filas.
*  Con **[x:y,0]** traera de un rango de filas la primer columna.
*  Con **[x:y,[0,2]]** Delimitara por columnas (traera 2 primeras columnas contando con la columna "0")

Ejemplo:

```python
  X_train[:, :5] selecciona las primeras 5 características de cada fila. Toma solo 5 columnas.
  X_train[:, 2:] selecciona todas las características a partir de la tercera columna en adelante en cada fila.

  # Resumen: [Empieza:Finaliza]
```
---
### Clase, Funciones:
Ciertas funciones pueden pasar a ser métodos depende a quien esten modificando.

* **`np.array()`** : Crea una matriz de una sola dimension o una fila **([ ])**. Es necesario que para Crear un Array todos los datos sean del mismo tipo ya sea **float** o **int**. La diferencia entre un Array y una lista, es que los arrays son vectores y por ello son muy eficientes y mas rápidos a la hora de hacer cálculos. También podemos operar vectores contra otros vectores cosa que con las listas tendríamos que abrir un ciclo **``for``** para iterar.
* **`np.asarray()`** : Crea una **copia activa** de un Array, haciendo que si modificamos el Array_A, el Array_B también tendrá la modificación. Viene por defecto el parámetro **``,copy=False``** que conecta las modificaciones a en b.
```python
    Lista = [1,2,3,4,5,6]
    a = np.array(Lista) # Convertimos la Lista A a un Array

    b = np.asarray(a)
```
* **`np.mat()`** : Crea una matriz de 2 dimensiones *('1 2;3 4')*
* **`np.arange()`** : Funciona exactamente igual que la clase `range`. Genera una lista de valores dentro de un intervalo dado. Por defecto el intervalo mínimo es 0.
  ```python
      np.arange(3)
      # array([0, 1, 2])
      np.arange(3.0)
      # array([ 0.,  1.,  2.])
      np.arange(3,7)
      # array([3, 4, 5, 6])
      np.arange(3,7,2) #Intervalo de 2 en 2.
      # array([3, 5])

  ```
* **`np.linspace()`** : A diferencia de `arange` indicamos la cantidad de divisiones de nuestro array. Es un intervalo Especificado.
* **`np.unique()`** : Retorna valores unicos y de manera ordenada.
* **`np.where()`** : Es una manera mas sencilla de escribir condicional "**if else**". `(modificacion,si cumple,no cumple)`
* **`np.choice()`** : Elige aleatoriamente valores de una lista o set de datos, también podemos obtener muestras aleatorias de una matriz unidimensional y devolver las muestras aleatorias de una matriz numpy.
    ```python
    indice = np.random.choice(np.arange(len(X_train)),24 , replace=False)
    ```
* **`np.array_split()`** : Particiona las filas de una matriz o DataFrame en partes iguales creando más dfs con las divisiones.
    ```python
        #(df,nºpartes a divir filas)
        partes_df = np.array_split(df,10) #creara 10 df con igual nº de filas

    ```
* **`np.zeros(3) `** : Crea un array de "0"s: array([0., 0., 0.]).
* **`np.random.<método>(3) `** : Llama a la librería Random que esta incorporada en numpy para hacer uso de sus métodos.<br>
Crea un array con números aleatorios siguiendo una distribución normal por defecto pero podemos indicarle otro tipo de distribución (.normal, .unirforme, .exponential)
  ```python
        datos = np.random.normal(size=10000) #Creara 10k números aleatorios con distribución normal.

         datos = np.random.randint(1, 100, size=1000) #Crea números decimales entre 2 rangos especificados con una distribución "Uniforme Discreta".
         #,size = cantidad de nº, da por defecto un array. También puede ser (nºfilas,nºcolumnas) arroja matriz.

        datos = np.random.choice(Lista, 24, replace=False, p=[0.1, 0, 0.3, 0.6, 0])
        #Elige 24 números al azar de la lista.
  ```
* **`np.vstack((np1, np2)) `** : Apila de manera vertical un array debajo de otro array. Puede usarse para actualizar tablas con nuevos datos (nuevas filas de datos).
* **`np.argmin() `** : Devuelve el indice del valor mínimo de un Array.
* **`np.argmax() `** : Devuelve el indice del valor máximo de un Array.

---
### Metodos de Numpy:
* **`.reshape()`** : Esto ordena a una matriz de 2 dimensiones que deseamos sin modificar la matriz original. *ejm: (4,5) 4 filas 5 columnas*. (Van al final de otro método ligado)
  ```python
    x = np.array([1,2,3,4]) #np.array de una lista.
    x.reshape(-1,1) #Transformamos a 2 Dimensiones
  ```
* **`.resize()`** : Al igual que `.reshape` ordena una matriz pero reemplazando a la matriz original
* **`.astype()`** : Cambia el tipo de "kind" de data. *(int o float)*
* **`.argsort()`** : Ordena una columna en base al ordenar de menor a mayor la columna de referencia.<br>
**low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()[::-1]][:10,:]**
**Col_Ordendar[Col_Referencia[1D de ser necesario].argsort()[Mostrar de mayor a menor]][nº Valores a mostrar]**

  ```python
        sat_scores = np.array([1440,1256,1543,1043,989,1412,1343]) #Ordenara de menor a mayor esta columna
        students = np.array(["Jhon", "Bob", "Alice", "Joe", "Jane", "Frank", "Carl"]) #En base a eso acomodara los nombres.

        # Asume el valor para cada nombre en el orden dado ejemplo: "Alice" (columna 2) = 1546 (nº mayor de la lista)
        # [::-1] muestra los resultados de menor a mayor.

        z = students[sat_scores.argsort()[::-1]] #Ordenara students en base de ordenar de menor a mayor sat_scores

        #array(['Alice', 'Jhon', 'Frank', 'Carl', 'Bob', 'Joe', 'Jane'], dtype='<U5')

        z = students[sat_scores.argsort()[::-1]][:3,:] #Mostrara solo 3 primeros nombres.
        #array(['Alice', 'Jhon', 'Frank'], dtype='<U5')

        from sklearn.preprocessing import StandardScaler, RobustScaler
        saleprice_scaled = StandardScaler().fit_transform(df[['SalePrice']]) #Transformamos el valor de "SalePrice" a 2D.
        saleprice_scaled #Nos arroja la distribución de los datos ya ordenado de menor (-) a mayor (+).

        #En este caso convertimos de 2D a 1D seleccionando solo una columna de array "saleprice_scaled[:,0]"
        low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10,:]
  ```
* **`.ravel()`** : Quita una dimensión a una Lista. En otras se utiliza para aplanar un arreglo multidimensional en un arreglo unidimensional.
* **`.stack()`** : Apila 2 array uno encima del otro encerrandolos entre corchetes.
  ```python
    import numpy as np

    # Crear dos arrays NumPy
    array1 = np.array([1, 2, 3])
    array2 = np.array([4, 5, 6])

    # Apilar los arrays a lo largo del eje 0 (nuevo eje)
    stacked_array = np.stack((array1, array2), axis=0) #Apila por filas; 1:columnas

    output: [[1 2 3]
             [4 5 6]]

  ```


#### Métodos Estadísticos:
* **`.mean()`**
* **`.sum()`**
* **`.cumsum()`**
* **`.min()`**
* **`.max()`**
* **`.std()`**
* **`.var()`**
* **`.corrcoef()`**
---
### Propiedades:
Estas no usan parentesis ni modifican el valor ni forma del original.

* **``np.newaxis``**:  es un alias de None, que se utiliza para indexar matrices en Python. El uso más sencillo de numpy.newaxis es agregar una nueva dimensión a un array NumPy en Python. Por ejemplo, convertir un array 1D en un array 2D, convertir un array 2D en un array 3D, etc.
  ```python
        vector = [ 0.04965172  0.04979645  0.04994022  0.05008303] # esto es vector[0:4,] no posee columnas.
        vector[:, np.newaxis][0:4,]
        #output: [[ 0.04965172][ 0.04979645][ 0.04994022][ 0.05008303]]

  ```
* **`.ndim`** : Indica la dimension bien 1 o 2.
* **`.shape`** : Indica la cantidad de listas y datos dentro de ellas. *(filas [ ] , columnas)*
* **`.dtype`** : Indica el tipo de "kind" de dato *(int32 o float64)*
* **`.size`** : Indica la cantidad de datos que hay en la matriz.
* **`.itemsize`** : Indica la cantidad de bytes que se va a almacenar.
* **`.T`** : Transpone la forma de una matriz sin modificar el original. Ejm: matriz 2x4 --> 4x2

---
### Parámetro

Van ligados a una función o método por una **"`,`"**

* **`,end=', '`** : Indica que el resultado este separado por comas y un espacio.
* **`,num=`** : indica el numero de segmentaciones para el método `.linspace`
* **`,axis=`** : Dirige el procesamiento a filas o columnas de la matriz. **0** para **filas** y **1** para **columnas**.
* **`,dtype=`** : Indica el tipo de "kind" de dato *(int32 o float64)*
---
### Arreglos
Son funciones únicas de `np` para lo cual las llamaremos con **`%`**.<br>
Estos son usadas en **DataScience** debido a que son mas rápidas en ejecución que una lista convencional a la hora de procesar grandes volúmenes de datos u operaciones.

* **`%timeit`** (variable) : asigna un contador a la operación de la variable que estamos creando.

###  Aplicaciones Estadísticas con Pandas:

* **`Emparejar Arrays con Nulos`** : En caso tengamos 2 listas que no tienen el mismo tamaño las emparejamos de nulos con la siguiente iteración.
```python
    p = [[1,2,3,4,5,6], [0,5]]

    n = len(max(p, key=len)) # Nos arroja el número de la lista màs grande.

    # Empareja a las listas mas pequeñas con la mas grande llenándolas de nulos.
    p2 = [x + [None]*(n-len(x)) for x in p]
    p2
```

* **`np.log(df)`** : Este parámetro aplicamos una transformación logarítmica a los datos indicados. Debido a que los datos presentan asimetría positiva y no sigue la línea diagonal. Cabe resaltar que para aplicar la `transformación logarítmica` es necesario que no haya datos con valores "0".

    ```python
        # Aplicamos la transformación logarítmica a "SalePrice".
        df['SalePrice'] = np.log(df['SalePrice'])

        #Verificamos la mejoría de los datos y su alineación.
        from scipy import stats   #Para el gráfico .probplot(df)
        from scipy.stats import norm  #Para el gráfico de distplot

        sns.distplot(df['SalePrice'], fit=norm) #Fit=norm : Ajusta la distribución a una normal.
        fig = plt.figure()

        #Otra manera usando ,kde=True:
        sns.displot(df["Age"], kde=True) #kde: predice cual es el patron que siguen los datos, no arroja una distribución definida.

        res = stats.probplot(df['SalePrice'],plot=plt)
    ```

## [5. re (regex)](#indice)

**Regular Expressions** Es una librería para trabajar con expresiones regulares. Estas son verificaciones por patrones o parámetros de busqueda. *(cap 8.4)* [Regex101](https://regex101.com/)

### Funciones:
Las funciones **r-strings** tienen el siguiente formato:<br>
        `re.funcion(Expresion re ,variable o string,parametro=x)`

* **`re.fullmatch()`** : Verifica si coincide el **string completo** en la variable alojada o parámetros especificados ((variable o parámetros digitados,"string")). Imprime el resultado en forma de Expresión.<br><br>

* **`re.search()`** : Similar a `.fullmatch()` este verifica la coincidencia de almenos una cadena del string, hace una búsqueda para un dato en especifico. Imprime el resultado en forma de Expresión. Opciones de impresión, es necesario un método `.group()` o `.groups()` para mostrar la cadena que coincidió.<br>  ***,flags=re.IGNORECASE** indica que ignore las mayúsculas en la verificación de los strings*.

```python
        busca3 = re.search("GUIDO","Guido Van Rossum",flags = re.IGNORECASE)
        busca3.group() if busca3 else "no se encontro"

        # Listar en la carpeta solo archivos .csv
        data =  [x for x in os.listdir('files') if re.search(".csv$",x)]
        data
```
* **`re.findall()`** : Extrae todas las coincidencias del texto todos los datos con el formato que le indiquemos. A diferencia de `.search()` extrae datos para más de una búsqueda de manera simultanea. Imprime el resultado
```python
        usuario = 'Tel_casa:52-1234-1234, Celular:52 4321 4321'
        re.findall(r'\d{2}-\d{4}-\d{4}|\d{2} \d{4} \d{4}',usuario)
        # | : Indica "or"

        # Verificar que se haya digitado correctamente un email: "pepito_123@hotmail.com"
        re.findall(r'([\da-z_\.-]+)@([\da-z\.-]+)\.([a-z\.]{2,6})',variable)
        #\d : digitos ; a-z: Letras minusculas ; "_" ; \. :Tome el punto literal ;
        #  + : Toma + ocurrencia similares del codigo anterior hasta el "@"
```
* **`re.finditer()`** : Es exactamente lo mismo que `.findall()` solo que  Imprime el resultado en forma de expresión.

* **`re.sub()`** : Al igual que `.replace()` Remueve el texto que le indiquemos, la diferencia es que este lo hace diferenciando texto y expresiones regulares.<br> ***,count=(nº)** indica cantidad de reemplazos al string*.
```python
        re.sub(r"\n",",","Salto 1\nSalto 2\nSalto 3",count=1)
        # (r"buscar","reemplazar","texto o variable", count= primera que encuentre)

        #Reemplaza por una celda vacia cualquiera de los caracteres del corechete y la cadena "18%".
        lista_df = [float(re.sub(r'[,PENUSDIGVTOTALSUBCargo]', '', i).replace('18%', '')) for i in Ultima_col]
        #otra manera:
        lista_df = [float(re.sub(r'[,$PENUSDIGVTOTALSUBCargos]|18%', '', i)) for i in Ultima_col]

```
* **`re.split()`** : Al igual que `.split()` segmenta el texto que le indiquemos. Podemos indicarle mediante expresiones regulares los espacios, tabulaciones, etc, que hallemos impreso en el texto.<br> ***,maxsplit=(nº)** indica cantidad de segmentaciones al string*.<br>
```python
        quitar_espacios = re.sub(r"\n+",",","W\nX\n\nY\n\n\nZ") #'W,X,Y,Z'
        re.split(r',\s*',quitar_espacios)  #Añadirá una coma con espacio: ['W', 'X', 'Y', 'Z']
```

---
### Métodos:

* **`.group()`** : Imprime el dato sacado del string en forma de un grupo total.
* **`.groups()`** : Imprime el dato sacado del string de manera segmentada ya sea conjuntos de texto o caracteres.
---
### Expresiones:

Las expresiones regulares, en general contienen símbolos especiales llamados meta caracteres como
\, @, #, $ * y signos de agrupación como [ ], { }, ( )
En consecuencia, resulta necesario utilizar los r-strings.

#### - Metacaracteres:

Para generar un patron de búsqueda usamos los meta-caracteres, suelen usarse de la siguiente forma:

r"\w{1,3}\s\d{1,2}"

* "`r''`" : Abre una Expresión regular llamado **r-string**, seguido va '  '. Las "," separan los parámetros para cada argumento a digitar.
* "`\d`" : Dígitos (0-9). indica que verificara solo números enteros en el string, {(nºdigitos)}; 4 a mas digitos{4,}; entre 8 y 10 dígitos {8,10}
* "`\s`" : Espacio en blanco.
* "`[ ]`" : Todos los valores dentro del corchete valen como 1 solo caracter del cual iterara buscando cada valor individualmente hasta el ultimo caracter indicado en el corchete.verificara si el parámetro escrito dentro se verifica con la variable. Los corchetes no necesitan usar condicional "|". Ejm: [A-Z][a-z]
* "` * `" : Indica verificar caracter por caracter del "string".
* "`+`" : Verifica cualquier tipo de "kind" de expresion sucesiva que cumpla almenos con un parametro indicado anteriormente. Ejm: con un "\n" que encuentre elimina en conjunto la cadena "\n\n\n"
* "` ^ `" : Por si solo indica mostrar el inicio de una cadena de caracteres (string), pero tambien indica encontrar todos los demás caracteres que no están dentro de corchetes.. Ejm: "[^a-z]"
* "` `" : Separa un conjunto ya sea palabras o numeros de otros tal cual se digita el string. Ejm (parametro para texto) (parametro para numero)  --> calle 3876<br><br>

* "``\d``"    : Digitos (0-9)
* "``\D``"    : No digitos (0-9)
* "``\w``"    : Caracter de palabra (a-z, A-Z, 0-9, _). Son llamados metacaracteres
* "``\W``"    : No caracter de palabra
* "``\s``"    : Espacio en blanco (espacio, tab, nueva linea)
* "``\S``"    : No espacio en blanco (espacio '/s', tab '/t', nueva linea '/n')
* "``.``"     : Cualquier caracter excepto un salto de linea (codicioso - greedy) : a.b match "a1b", "a#b"
* "``\``"     : Cancela caracteres especiales. ejm: \. : toma el punto literal, le desactiva su funcion especial.
* "``^``"     : Inicio de una cadena de caracteres (string)
* "``$``"     : Fin de una cadena de caracteres, se indica al final del caracter. ejm Hola$

#### - Cuantificadores:

* "``*``"     : 0 o más (codicioso - greedy)
* "``+``"     : 1 o más (codicioso - greedy): Toma + ocurrencia similares del elemento o grupo precedente.
* "``?``"     : 0 or 1 (perezoso - lazy) hace que los caracteres sean opcionales: a? ; velve los caracteres codiosos en perezosos:<br>
  ".*?" ; ".+?" : coincidirá con la menor cantidad de caracteres posibles. (tomara donde aparecen menos)
* "``{3}``"   : Numero exacto
* "``{n,}``"  : Coindide con a partir de n a más repeticiones de un elemento o grupo precedente.
* "``{3,4}``" : Rango de números (Minimo, Maximo), ignorara todo lo que no lo cumpla.
* "``( )``"   : Grupos. Usan condicional "|". Se consideran como un solo elemento por lo que pueden usar los modificadores "*+?"
* "``[]``"    : Hace referencia a un elemento de los que podria encontrar con una sola coincidencia, para ampliar el numero de coincidencias se usa los modificadores codiciosos "*+" : (Los corchetes no necesitan usar condicional "|".)
  * "[abc]" : coincidirá con cualquiera de los caracteres "a", "b" o "c".
  * "[a-z]" : coincidirá con cualquier letra minúscula.
  * "[a-zA-Z0-9]" : coincidirá con cualquier letra mayúscula o minúscula, así como con cualquier dígito.
* "``[^ ]``"  : Encuentra todos los demás caracteres que no están dentro de corchetes.
* "``|``"     : Condicional O
* "``\b``"    : Se posiciona a los limite de palabras y caracteres
* "``\B``"    : Se posiciona omitiendo los limites de palabras y caracteres.
* "``\1``"    : Referencias. Simula no tener que repetir lo digitado muchas veces, para cadenas de más de un caracter es necesario agruparlos "( )"
---
### Parámetros:

* "**``,flag=re.M``**"  : Interpreta cada linea del texto como un string independiente a pesar que el texto este en su conjunto como un solo string.
* "**``,flags=re.I ó ,flags=re.IGNORECASE``**"  : Interpreta las mayusculas y las minusculas como parte del mismo texto. "HOLA" = "hola"
* "**``,count=(nº)``**"  : indica cantidad de reemplazos al string, para la funcion **`.sub()`**
* "**``,maxsplit=(nº)``**"  : indica cantidad de segmentaciones al string, para la funcion **`.split()`**

## [6. pandas ("*pd*")](#indice)

**Panel Data**. Es una librería que brinda la función de importar documentos `.csv`,`xslx`,etc y usarlos como Dataframes. Es para manejar tablas.<br>
Pandas es una evolución de **numpy** por lo que muchos de sus métodos funcionan aquí:<br>
*Indexados, Métodos Estadísticos*<br>
También incorpora las librerías de: `re`, `datetime`

```python
  # Para Instalar la Version Beta de pandas 2.0
  pip install --upgrade --pre pandas==2.0.0rc0 # Ya se encuentra la version oficial de 2.0 para el cual se usa el siguiente parametro para usar "pyarrow" a cualquier formato que importemos a df.

  df = pd.read_csv(path, sep=";", encoding="utf-8", dtype_backend='pyarrow')

  import pandas as pd

  pd.set_option('display.max_columns', None) # Muestra todas las columnas del df.
  pd.options.display.float_format = "{:,.2f}".format #redondeamos todos los números a 2 decimales.

```

Para la carga y exportación de archivos según su tipo de "kind" de archivo o extension: Tener en cuenta que al exportar con el mismo nombre el archivo se sobrescribirá

  * Archivos `.txt`

    Pandas no tiene un metodo especifico para convertir a tabla un archivo `.txt` por lo que podremos hacerlo migrar a `.csv` para poder convertirlo a DataFrame:

      ```python
        ruta = r"C:\Users\Foster-PC\Documents\Visual Studio Code\Learning\Python\Tutoriales Internet\Fundamentos Python\req.txt"
        df = pd.read_csv(ruta, header=None, names=['columna_texto']) #indicamos que no tiene cabecera y añadirmos una.

        # Aplicamos un filtro u operacion en este caso añadimos una columna para separar texto:
        df['texto_anterior'] = df['columna_texto'].str.split('==').str[0]

        #Volvermos a Guardar en un archivo .txt
        df['texto_anterior'].to_csv('req3.txt', index=False, header=None)
      ```

  * Archivos `.csv`

    Es comumente usado, pero a comparacion de otras maneras de guardar nuestra informacion lo hace muy pesado y no es tan rapido al leer o guardar informacion en este formato, se podria usar como un formato final. Añadir tambien que al importar este formato a un DF se pierde la configuracion de los tipos de campos que se ha designado al guardar por lo que hay otras maneras de guardar y leer como `.pickle` o `parquet` (requiere instalar paquetes)

    * Para cargar archivos **`.csv`** (tiene que estar guardado como .cvs utf-8 (delimitado por comas)))<br>
    ```python
        variable = pd.read_csv('./../carpeta/nombre de archivo.csv', sep=";", encoding="utf-8", usecols=[0,2,3], skiprows=[1,4], index_col=0, parse_dates=["Fecha","Creado El"])
    ```
      *El ["Fecha","Creado El"] punto indica en la carpeta actual*<br>
      *Los 2 puntos seguidos indica saltar una carpeta hacia atrás*

      * **``,usecols=``** : Columnas que quiero que se muestren, las indicamos según su indice.
      * **``skiprows=[0,1,2]``** : Omitirá cargar filas que le indiquemos.
      * **``index_col=str,index``** : Podemos usar una columna[indice] como index para el df, sea ya nombrandolos o indicando el numero de index de la columan. Podemos poner mas de un index para generar **multindex**.
      * **``header=[0,1]``** : Indicamos que numero de fila usara como encabezado, pueden ser mas de 1.
      * **``parser_dates=True``** : Convertirá a formato fecha las columnas que lo sean. O indicando (parser_dates="Column1","Column2")<br><br>
    * Para exportar a formato **`.csv`**, obviando los índices, con encoding **`utf-8`**:
    ```python
        df_tarjetas.to_csv("terrernos2.csv", index=False, encoding="utf-8")

        #En caso queramos exportar en varios archivos un df dividido:
        path = r'C:\Users\foster\Desktop\files"
        partes_df = np.array_split(df,10) #dividimos las filas el df en 10 df con filas iguales

        for ix, df in enumerate(partes)
          df.to_csv(path + "\files" + str(ix+1).zfill(2) + "csv",index=false)
          #Creara archivos con el nombre empezando con nombre_s01.csv
    ```
  * Archivos `.xlsx`
    * Para cargar archivos Excel **`.xlsx`** Tiene que instalarse con un pip install **``openpyxl``** si queremos cambiarlo por otro usaremos ``(,engine="xlrd")``.<br>
    Podemos leer todo el archivo o solo una pestaña especifica con el atributo **`,sheet_name=`** en caso nuestro archivo contenga varias pestañas, también podemos asignarle cada pestaña a una variable en caso asi lo deseemos.
        ```python
        variable = pd.read_excel('./../carpeta/nombre de archivo.xlsx', sheet_name="Nombre o numero",usecols=[0,2,3],
                                  skiprows=[0,1,2], startrow=4, index_col=0, parser_dates=["Fecha","Hora"],
                                  dtype={"OC":str})

        df_pd_P = pd.read_excel(ruta, sheet_name=3, skiprows=list(range(231, 291)) + [0, 1], usecols=[2, 3, 4], dtype="str", parse_dates=False)
        ```
      * **``,sheet_name=``** : (str|int) indicamos el nombre o numero segun indice. Si colocamos entre corchetes nos dara un diccionario en vez de una tabla df.
      * **``,usecols=[0,2,3]``** : Columnas que quiero que se muestren, las indicamos según su indice.
      * **``skiprows=[0,1,2]``** : Omitirá cargar filas que le indiquemos.
      * **``startrow=4``** : Indica a partir de que fila arme el DataFrame.
      * **``index_col=str,index``** : Podemos usar una columna[indice] como index para el df, sea ya nombrandolos o indicando el numero de index de la columan. Podemos poner mas de un index para generar **multindex**.
      * **``parse_dates=True``** : Convertirá a formato fecha las columnas que lo sean. O indicando (parser_dates="Column1","Column2")<br><br>

    * Para Revisar rápidamente solo las pestañas (Sheets) de excel **`.xlsx`**.
    ```python
              df = pd.read_excel("C:/Users/Foster-PC/Documents/G&S Instalaciones/PLANTILLAS/BALDOSAS Y DRYWALL/PLANTILLA BYD 2021.xlsx", None)
              df.keys() #De esta manera arrojará solo el nombre de las pestañas.
    ```
    * Para exportar a formato excel **`.xlsx`**. No importa la extension del archivo lo convertirá a excel (.xlsx).
    ```python
        df_tarjetas.to_excel("terrernos2.xlsx", sheet_name="Hoja1", index_label= "Nombres")
        # index_label= Toma el nombre de una columna como indice dentro del Excel.
        #Podemos exportar en una nueva hoja de nuestro mismo archivo Excel.
    ```
    * Para exportar en multiples pestañas:
    ```python
        df2 = df1.copy() #En caso hagamos una copia del df.
        with pd.ExcelWriter('output.xlsx') as writer:
          df1.to_excel(writer, sheet_name='Sheet_name_1')
          df2.to_excel(writer, sheet_name='Sheet_name_2')
    ```
    * Para leer y  Concatenar todos los `sheet_name` de un cuaderno Excel.
    ```python
        df = pd.read_excel("terrernos2.xlsx", sheet_name= None)
        df = pd.concat(df_2, ignore_index=True) #para tener 1 solo index de toda la tabla.
        df.head()
    ```

  * Archivos `.json`
    * Para Cargar archivos `.json`. index,name,value
    ```python
        pd.read_json("Nombre.json", orient="columns") #columns es la que viene por defecto.
        #no hay "indent". Pero si podemos indicarle el tipo de "kind" de orientación que queremos.
    ```
    * Para Exportar a `.json`. index,name,value. (indent =True or 3,4,5) Respetará la indentación (TAB).
    ```python
        pd.to_json("Nombre.json", indent=True)
         #{"index":{0:1,1:2,2:3,3:4,4:5},"name"{0:A,1:B,2:C,3:D,4:E},"value"{etc}}

        pd.to_json("Nombre.json", indent=True, orient="columns")
        #Es el mismo que viene por defecto.

        pd.to_json("Nombre.json", indent=True, orient="index")
        #{"0"{"index":1,"name":"A","value":3.03},"1"{"index":2,"name":"B",etc},

        pd.to_json("Nombre.json", indent=True, orient="split")
        #{"columns"["index","name","value"],"index"[0,1,2,3,4],"value"[etc]

        pd.to_json("Nombre.json", indent=True, orient="records")
        #[{"index":1,"name":A,"value":3.03},{"index":2,"name":B,"value":5.14}]

        pd.to_json("Nombre.json", indent=True, orient="values")
        #[[1,"A",3.03],[2,"B",5.14],[etc]]

        pd.to_json("Nombre.json", indent=True, orient="table")
        #"schema","Datos"...
    ```
    * Importar Datasets (csv,etc) de Github:
      Es necesario entrar al archivo por medio de github y copiar y pegar el URL del archivo en formato **RAW**.<br>
      Para poder leer el Raw del archivo, entramos en el archivo en GitHub y en Raw damos click derecho y copiamos direccion de enlace.

      ```python
        url = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/archived/ecdc/total_cases.csv"
        datacov = pd.read_csv(url)
        datacov.shape
      ```
  * Archivos `.parquet`

    Esta extension es la más recomendable para guardar nuestros DF, debido a que conserva la configuracion de las columnas (object, number, int32, float64, datetime, etc) y tambien por lo que al guardar o leer lo hace de manera rapida y la compresion hace al archivo muy ligero en comparacion con otros formatos, tiene la mejor compresion.
    Es necesario instalar 2 paqutes para poder usar `.parquet`:

    ```python
      pip install pyarrow
      pip install fastparquet

    ```

    * Para Cargar archivos `.parquet`:
    ```python
        pd.read_parquet("Nombre.parquet", columns=["date", "Name"]) #Cargamos solo columnas puntuales del DF.

        #Podemos escribir antes %timeit para verificar cuanto tiempo toma ya sea cargar y guardar el archivo en "ms".
        %timeit pd.read_parquet("Nombre.parquet", columns=["date", "Name"])

    ```
    * Para guardar archivos `.parquet`.
    ```python
        pd.to_parquet("Nombre.parquet")

        !ls -GFlash Nombre.parquet #Arroja el peso del arricho guardado.
    ```
  * Portapapeles

    Podemos copiar y guardar en portapapeles e importarlo en un DataFrame:

    ```python
      pd.read_clipboard(index_col="Col_A") #Carga del portapapeles.
      #index_col= : Podemos indicarle el nombre de la Columna o indice [0]

      pd.to_clipboard(index=True, sep=",", header=True) #Guarda en el Portapapeles.
      #,index=True : Por defecto imprime el index del copiado.
      #,sep="," : Indicamos el separador para que lo imprima. No es necesario para Excel
      #,header=True : Considera el encabezado por default.
    ```

___
### Clase, funciones:
* **`pd.set_option()`** : Configura opciones de la librería de pandas.
  ```python
          #Previo podemos indicar que se use la configuración regional predeterminada del sistema operativo para el formato de fecha y hora.
          import locale
          locale.setlocale(locale.LC_TIME,"")

          pd.set_option("display.max_columns", None) # Muestra todas las columnas del df.
          pd.set_option("display.float_format", "{:.2f}".format) # Muestra los numeros con 2 digistos de decimales.

          #Para Usar Arrow de Apache como backend para la version estable de Pandas 2.0 se añade el siguiente parametro al importar a df.

          df = pd.read_csv(path, sep=";", encoding="utf-8", dtype_backend='pyarrow')

          pd.set_option("precision",4)
          #Configuramos que muestre 1 decimal.

          pd.set_option("max_columns",9)
          # Muestra 9 columnas, incluye indice.

          pd.set_option("display.width",None)
          #None lo ajustara automáticamente.

          pd.set_option("precision",4,"max_columns",9,"display.width",None)
          #Puede Resumirse estos 3 cambios en una sola linea de código.

  ```
* **`pd.options()`** : Configura al igual que `set.option()` opciones de la librería de pandas.
  ```python
          # Usar "plotly" como backend para grafias con el metodo de pandas.
          import plotly.express as px  #Es necesario importar plotly previamente.

          pd.options.plotting.backend = "plotly"

          #df.group(by=["day"]).sum()[["tip"]].sort_values("tip").plot(kind="bar")

          #Configurar pandas a 2 decimales:

          pd.options.display.float_format = "{:,.2f}".format #redondeamos todos los números a 2 decimales.

  ```

* **`pd.Series()`** : Crea una **columna** con los datos a asignarle, una Serie son columnas en pandas. Pueden contener forma de diccionarios "``{'a':b}``", Listas "``['a,b']``" o tuplas "``('a,b')``". Para convertir una Serie (1D) a dataframe (2D) usamos el parametro ``np.newaxis`` o tambien podemos encerrar en doble **[[ ]]**, en ambos casos convertira un array de 1D como podria ser el llamar a una columna df["SalePrice"].shape a 2D df[["SalePrice"]].shape<br<br>>

Cabe resaltar que al llamar o seleccionar una columna del dataframe estamos frente a un array de 1D.

  ```python
    In [24]: df['a'].shape
    Out[24]: (5,)      # <--- 1D array

    In [26]: df[['a']].shape
    Out[26]: (5, 1)    # <--- 2D array

    #Esto es igual a:
    In [25]: df['a'][:, np.newaxis].shape
    Out[25]: (5, 1)    # <--- 2D array

    #Convertir de 2D a 1D:
    In [25]: df[['a']][:,0].shape
    Out[25]: (5,)    # <--- 1D array

  ```
* **`pd.DataFrame()`** : Crea un DataFrame (tabla). Son Series de 2 dimensiones (Tablas). Se pueden armar DataFrames de diccionarios, listas y tuplas. La mejor manera de crear un `DataFrame` es usando un **diccionario** para poder referenciar sus encabezados y sus valores, ya sean listas, tuplas, etc. <br>
En Caso Tengamos una Listas de Listas `array([[          0],[          0]])` es necesario "Aplanarlas" usando el método `.flatten()` o simplemente indicando la indexacion de todas las filas, una columna = [:,0]
  ```python
          data = {"Ubicación":ubicacion, "Precio":precio, "Area":m2} # Diccionario
          df_tarjetas = pd.DataFrame(data, index=datos["Letras"], columns=["Numeros"])
            # data = Diccionario donde esta la info.
            # index = Indica la columna que usaremos como indice en caso de no quere el default 1,2,3.. etc.
            # columns = Indicamos solo las columnas que queremos visualizar. Obiamos la Columna que usamos como index.

          #({"nombre_columna",variable_con_datos}), variables previamente definidas (Listas)

          Model_Accuracy = pd.DataFrame({"Accuracy":history.history["accuracy"],
                                "Val_Accuracy":history.history["val_accuracy"]})
          #Para devolver un Array de 1D de un Dataframe de una Lista de Listas:
          pd.DataFrame({"Survive":predictions_out.flatten()})
          pd.DataFrame({"Survive":predictions_out[:,0]}) #Ambos son validos nos dan 1 columna o 1D

          #Cuando indicamos un diccionario con un solo valor:
          files = {'A.txt':12, 'B.txt':34, 'C.txt':56, 'D.txt':78}
          filesFrame = pd.DataFrame(files.items(), columns=['filename','size'])

          #Otra manera de Crear un DataFrame:
          pd.DataFrame(predictions_out, columns=["Survive"])

          #,columns= : referencia a la columna, si ya la especificamos esta servirá para asignarle un nombre a la columna.

          pd.DataFrame(history.history["accuracy"], columns=["Accuracy"]), #de no indicar la columna no cambiara el nombre.


  ```
* **`pd.ExcelWriter()`** : Clase para escribir objetos "DataFrame" en hojas de Excel. (uso en exportar en multipestaña)
* **`pd.concat([datas])`** : Concatena tablas (df) o columnas de un df una debajo de otra por defecto `,axis=0` (por fila), pero también podemos concaternarlo una tabla a lado de otra usando `,axis=1` (por columna) retornando la compilación de ellas en una nueva tabla. Va entre "**[ ]**" porque sera una lista de DataFrames ([df_1,df_2,df_3])

  ```python
        df = pd.concat([df_1, df_2, df_3], axis=1 ,sort=True, ignore_index=True) #Juntará todos los dataframe en uno solo.
        #,axis=1 Concatena por columna
        #,sort=True1 Que lo ordene.
        #,ignore_index= True Junta los indices de la concatenacion en uno solo, Creara un index enumerado eliminando los demás index previos.
        #,join= default {"outer"}
        #,key= ['Total', 'Percent'] #Cambia el orden de presentación de las columnas.


        df = pd.concat([df_train["SalePrice"],df_train[YearBuilt]], axis=1)
        #Concatenamos la Columna de un "df_train" con otra del mismo "df_train", guardandose esta nueva tabla en "df".

        #Creamos 2 columnas y las unimos con concat manteniendo el mismo index:
          total = df_train.isnull().sum().sort_values(ascending=False)
          percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
          missing_data = pd.concat([total, percent], keys=['Total', 'Percent'], axis=1)
          missing_data.head(20)

  ```
  * Para leer y  Concatenar todos los `sheet_name` de un cuaderno Excel.
    ```python
        df = pd.read_excel("terrernos2.xlsx", sheet_name= None)
        df = pd.concat(df_2, ignore_index=True) #resetea los index contando de 0 para las tablas concatenadas.
        df.head()
    ```
  * Un ejemplo para concatenar varios archivos en un ciclo ``for``:

  ```python
      #Concatenando archivos que solo tengan terminación ".csv"
      import re, os

      data =  [x for x in os.listdir('files') if re.search(".csv$",x)] #carpeta esta en el mismo directorio de nuestro archivo .py

      df_Prueba = pd.DataFrame() #Creamos un df vacío.
      for i in data:  #variable data contiene en lista los archivos [achr1.csv,achr2.csv,achr3.csv]
          df_temp = pd.read_csv("files/" + i)   #creara un dataframe por diccionario
          df_Prueba = pd.concat([df_Prueba, df_temp])   #Juntará todos los dataframe en uno solo.
  ```
  * Para hacer lo mismo que el paso anterior pero de manera mas simplificada con la liberaria glob:

    ```python
        from glob import glob #para no escribir glob 2 veces ya que usamos su metodo "glob".

         # Importara todos los archivos csv que inicien con "analisis" de manera ordenada buscando en todas las carpetas anteriores que lo contengan.
        csv_files = sorted(glob("data/**/**/*/analisis*.csv", recursive=True)) #Buscara en 3 niveles de carpetas.

        # ('**') : Indica que es un directorio padre (pueden ser varios niveles), con ´,recursive=True´ permite que se busque en todos los subdirectorios hijos.
        # ('*') : Indica que es un directorio hijo, y buscara en todas la carpetas de ese nivel. Tambien añadira a la lista todo lo que encuentre.
        #recursive=True : Es necesario para activar la busqueda recursiva en todos los niveles de carpetas.

        Lista = sorted(glob(path+"/4500*.pdf"))
        x = [(i.split(sep="\\")[-1]).replace(" ","") for i in Lista]  #Selecciona el ultimo elemento del split.

        #Tambien usa ??? como numero de caracteres:

        csv_files = sorted(glob("data/*/???-*.csv")) # Ejemplo una Factura: FAC-00021

        pd.concat((pd.read_csv(file) for file in csv_files), ignore_index=True)

        # Buscar en todos los subdirectorios que encuentre: os.path.join arroja una cadena de texto : "./**/F*"
        Lista = sorted(glob(os.path.join(Ruta_Actual, "**", "F??-*.pdf"), recursive=True))
    ```

* **`pd.to_numeric()`** : Formatea una columna a formato **numerico**.
  ```python
    df["Price"] = pd.to_numeric(df["Price"], errors="coerce").fillna(0)
    #,errors="coerce" : Indica "forzado", Indica que cualquier valor que no interprete como numerico lo convertira en valor NaN, con el fin de que no arroje error al convertir toda la columna a numerico. Es recomendable cambiar el valor NaN a 0 como ejemplo.

  ```
* **`pd.to_datetime()`** : Formatea una columna a formato **date**. Arrojando el formato americano año,mes,dia,hora<br>
Al tener un dato en formato fecha tenemos la ventaja de convertir esta columna fecha en nuestro index mediante `.set_index("Col", inplace=True)` y hacer la búsqueda por fecha sea poniendo solo el año con `df.loc["2020"]`.<br>
Una vez usada la columna fecha como index, podremos acceder a el con ``.loc`` para filtrar o ``.index``

  ```python
      df['Order Date'] = pd.to_datetime(df['Order Date'])
      #Convierte una columna en formato Fecha.
  ```
  * Especificando el formato de fecha que tenemos en la columna, lo convertirá siempre a formato americano *año mes dia hora*:
  ```python
        df['Order Date'] = pd.to_datetime(df['Order Date'],format= "%Y-%m-%d %H:%M:%S") # "2022-01-04 11:35:50" Solo este formato con años.

        data["Fe, de picking"] = pd.to_datetime(data["Fe, de picking"], format= "%d/%m/%Y") #En la columna esta como dia/mes/año
  ```

  * Para cambiar el formato americano a un formato personalizado usamos `dt.strftime()`, considerar que estamos cambiando el formato a `String`: Previamente esta columna tiene que estar en formato fecha.
  ```python
      df['Order Date'] = df['Order Date'].dt.strftime("%d/%m/%y")  #dia/mes/año
      #%d : dias en 2 digitos
      #%m : mes en 2 digitos
      #%y : año en 2 digitos
      #%Y : año en 4 digitos

      df.index.strftime("%d/%m/%y")  #Para cambiar el formato cuando nuestra fecha sea nuestro index.

      #Para indicar los meses por sus nombres en Español
      df['Order Date'] = df['Order Date'].dt.month_name(locale='es_ES.utf8') # en_US (ingles)

  ```

  * Aumentar fechas a una fecha base:
  ```python
        pd.to_datetime(160, unit='D', origin='2020-02-01')
        #Aumente 160 días a una fecha Base.
        #origin= puedes ser también "unix"

  ```

* **`pd.PeriodIndex()`** : Permite hacer agrupaciones de fechas ya sea por dia, mes ,trimestre,año , etc. Se suele usar en los parámetros de `df.pivot_table()` y  `groupby()`.

  ```python
        #pd.PeriodIndex(Columna, freq='M')
        pd.PeriodIndex(df_new["Fe, de picking"], freq='M')

        #En pivot_table
        df_2 = df_new.pivot_table(index=pd.PeriodIndex(df_new["Fe, de picking"], freq='M'), values=["Cantidad ","PT"],
                                  aggfunc="sum").sort_index(ascending=True)

        pandas_df.groupby([pd.PeriodIndex(pandas_df["Creado El"], freq='M'),"Creado Por"]).agg({"Pedido":"sum"})

        #En groupby
        grupo = df_new.groupby(pd.PeriodIndex(df_new["Fe, de picking"], freq='M')).sum().sort_index(ascending=True).reset_index()
        #reset_index() creara un nuevo index enumerando el numero de filas {0,1,2,3}
  ```

* **`pd.get_dummies()`** : **One-Hot-Encoding** ("Variables Nominales"). Transforma los datos de una columna categórica (columna que tiene clasificaciones) a una forma en la que python pueda entender de que no se esta aportando valor sino una clasificación. Para ello creara varias columnas según el tipo de "kind" de valores únicos que encuentre en la columna que vamos a aplicar esta función llenando de valores binarios estas nuevas columnas para que python entienda en determinada fila cual de las categorías encontradas se activo.<br>
Esta clasificación se suele usar en la preparación de datos para posteriormente procesarlo con ML o Redes Neuronales.<br>
Un punto negativo de esta Categorizacion es que crea una nueva columna por cada categoria unica que encuetra, haciendo que en ciertos casos nos genere demasiadas columnas nuevas el cual acarrea el Problema de la Dimensionalidad. Este problema nos fuerza a que tengamos mucha cantidad de datos a mayor dimensiones de nuestra tabla para reducir el sesgo por falta de datos.<br>.
Un punto Positivo es que debido que categoriza nuestros datos en 0 y 1 hace que no sea necesario el escalamiento de estos datos a diferencia del **Ordinal-Encoder** (se usa para categorizar variables que guardar una relacion de proximidad "Variables Ordinales").

  ```python
    df = pd.get_dummies(df["Pclass"], prefix="Pclass"), axis=1 #Tabla con los nuevos campos en binario.

    #Otro ejemplo:
    df = pd.concat([df, pd.get_dummies(df["Pclass"], prefix="Pclass")], axis=1)
    df.drop(["Pclass"], axis=1, inplace=True) #Una vez clasificado borramos la columna origen. Concatenamos por columna.
    df.drop(columns="Pclass", inplace=True) #Otra manera de borrar por nombre_columna

    #,prefix="Nombra_Columna" = Podemos cambiar o conservar el nombre para el cual usara como base para enumerar según la cantidad de valores únicos o categorías encuentre en esta columna. "Nombra_Columna_1", "Nombra_Columna_2", "Nombra_Columna_3 .. etc"
  ```

* **`pd.cut()`** : A diferencia de las categorizaciones de valores categoricos, aqui categorizamos valores numericos, indicando el numero de categorias a segmentar o especificando un rango para cada segmento de valores numericos.

  ```python
    edades = np.array([1,7,8,15,28,35,50,55,70,75,100])
    # Para que segmente de forma Automatica:
    segmentacion = pd.cut(edades, bins=3, labels=["baja", "media", "alta"], include_lowest=True, retbins=True)

    #bins= : indica el numero de segmentos automaticos que queremos que calcule y separe. Tambien Podemos especifiar un rango.
    #labels= : Etiquetas de Categorizacion.
    #include_lowest=True : Indicamos que incluya el valor màs bajo de los datos numericos.
    #retbins=True : Indica que muestre o retorne los rangos de los contenedores que uso de manera automatica o que especificamos.

    # Para que segmente de forma Especifica segun los intervalos que indiquemos (Hay parametros adicionales al indicar los rangos a segmentar):

    segmentacion = pd.cut(edades, bins=[0, 11, 17, 59, np.inf]
                      , labels=["infante", "joven", "adulto", "mayor"], include_lowest=True, retbins=True
                      ,right=True)

    #right=True : Tomara los saltos entre intervalos como <= ej: 0-11 ; 11-17 ; 17-59 ; 59-np.inf

    # Para verificar la categorizacion que hemos realizado:

      print(segmentacion[1], "\n") #Nos Arroja los valores de los intervalos que uso para la segmentacion.
      print(segmentacion[0].categories, "\n") #Nos Muestra nuestras etiquetas que definimos.
      print(segmentacion[0].codes, "\n") #Nos arroja la categorizacion de forma numerica. [0,0,1,1,2]
      print(np.array(segmentacion[0])) #Nos arroja la categorizacion con las Etiquetas que indicamos.

  ```

Para Pandas la manera común de filtrado es:
 * Filtrar por columnas: df[["Col1", "Col2"]]
 * Filtrar por filas o nº de index: df[df.index == 38]
 * Filtrar por un Atributo de la columna: df[df["Sexo"] == "Masculino"]
 * Filtrar por Varios Atributo de la columna: df[df["Clase"].isin([1,2])]
 * Borrar o Excluir valores de un Filtro: df = df[~df["Col"].isin(["Valor_1","valor_2"])]  #Solo funciona con .isin([])

**df[<Filtro> df[] operador][[Lista de columnas a mostrar en la tabla filtrada]].where(<filtro_2>).(quitar nulos, ordenar, +métodos)**

```python
  df[df["Creado Por"] == "JARCAN"][["Descrición Material","Tipo de Orden"]].where(df["Tipo de Orden"] == "OC").dropna()

```
* Filtrar por multiples Criterios usando "&":
  ```
  df = pd.DataFrame(
    {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]})

  Crit1 = df[AAA] <= 5.5
  Crit2 = df[BBB] == 10.0
  Crit3 = df[CCC] > -40.0
  AllCrit = Crit1 & Crit2 & Crit3

  df[AllCrit]
  ```
___

### Métodos para DataFrames:
Estos métodos son para una variable donde este alojada el DataFrame.

Se puede llamar a una columna digitandolo entre **`['Columna']`** o digitando su indice, pero también digitandolo como atributo **`.nombre_columna`** Digitandolo por su nombre pero este ultimo no es para columnas con palabras separas por un espacio en cambio el otro por corchetes se puede llamar a más de una columna con un doble corchete: **`[['Columna_A'], ['Columna_B']]`**

* **`df.describe()`** : Trae todas las estadísticas relevantes: `count`,`null_count`, `mean`, `std`, `min`, `25%`, `50%`, `75%`, `max`, `dtype`.
  ```python
    #Para filtrar por tipo de columna filtramos de la siguiente manera:
    df.select(pl.col(pl.Int64,pl.Utf8)).describe()
    #Otra manera es usar "pl.all().exlude()"
    df.select(pl.all().exclude(pl.Utf8,pl.Date)).describe()

    #Filtrar nombre de columnas numericas:
    df.select(pl.col(pl.Int64)).describe().columns

    #Filtrar nombre de columnas categoricas:
    df[df.describe(exclude=['category']).columns]
    df[df.describe(exclude=['number']).columns]

  ```

* **`df.astype()`** : Convierte el resultado bien a ``("int","float","object","str","datetime64[ns]","bool","category")``
```python
          df = df.astype({"Precio Neto":float, "Proveedor":str, "Posición":str}).dtypes
          #Cambia de int a float en una columna en especifico

          # Otra forma es:
          df["col1"] = df["col1"].astype(str)

          #Una forma rapida de convertir todos los numeros de un df a decimales es :
          df.[df.str.isdecimal()].apply(int) #Fuerza a convertir los numeros del df a decimales.

          #Tambien podemos cambiar el formato de los index, para ello hay que referenciarlos:
          df.index.astype(int)

```
* **`df.convert_dtypes()`** : Podemos elegir el formato del df, ya sea "numpy_nullable" (default) o "pyarrow". Esto en caso queramos especificar en todo el df o una columna.

  ```python
          #Por default:
          DataFrame.convert_dtypes(infer_objects=True, convert_string=True, convert_integer=True, convert_boolean=True, convert_floating=True, dtype_backend='numpy_nullable')

          df = df.convert_dtypes(dtype_backend='pyarrow') #Convertira todas las columnas a "pyarrow".

          #Podemos Especificar las columnas que vamos a convertir a pyarrow sin afectar las demas.
          df[["A","B"]] = df[["A","B"]].convert_dtypes(dtype_backend='pyarrow')
  ```

* **`df.select_dtypes()`** : Selecciona las columnas a mostrar del DataFrame segun el tipo de varialbe que indiquemos ``("number","int","float","object","str","datetime64[ns]","bool","category")``
```python
          df = df.select_dtypes(include=["number", "catergory"]).dtypes

          # Otra forma es Excluir
          df = df.select_dtypes(exlude="object",).dtypes

```
* **`df.tolist()`** : Engloba los datos del df o una columna, en una lista de datos. "[a,b,c,d]"
* **`df.sort_index()`** : Ordena el indice. Por defecto viene en ascendente. *(,ascending=True)*, para columnas *(,axis=1)*.
Siempre en importante despues de haber improtado data, ordenar el index.
  ```python
    df.sort_index(inplace=True)
  ```
* **`df.reindex()`** : Ordena el index según el orden de las columnas que le pasemos. Por defecto afecta a las columas (axis=1).
  ```python
        df.columns = ["a,b,c,d,e,f"]
        df = df.reindex(columns=['a','f','d','b','c','e'], axis=1) #Cambiara el orden de las columnas al que indicamos.
  ```
* **`df.set_index()`** : Indicamos que columna queremos como indice base. Tambien podemos hacer multi-index para tener varios indices (relacionados como categorias) eligiendo las columnas que queremos como indices.
```python
          df.set_index("Fechas", inplace=True)
          #Ponemos como index una columna de tipo fecha.

          #Multi-Index:
          df.set_index(["Region","Provincia","Distrito"], inplace=True) #Multi-Index de 3 niveles.

          #Para hacer un filtro sensillo por Multi-Index usamos la propiedad loc.
          df.loc["Lima"] #Nombramos la Region que queremos, ya sea Region, Provincia o Distrito.

          #Para hacer un filtro avanzado contemplando los 3 niveles de Multi-Index usamos la propiedad loc + pd.IndexSlice
          #df.loc[pd.IndexSlice[primer nivel, segundo nivel, [tercer nivel_1, tercer nivel_2]], "Col_A", "Col_B ]
          df.loc[pd.IndexSlice[:, :, ["Los Olivos", "Comas"]], :]


```
* **`df.reset_index()`** : Añade una columna index adicional. Para solo resetear sin duplicar el index con el que tenemos actual usamos drop=True.
  ```python
    df.reset_index(drop=True)
    #,drop=True : resetea el index que trae por defecto sin añadir una columna index adicional.
    #,name="nombre_newcol" : Se asignara nombre a las nuevas columnas que se añada a tabla.
    #,level= : De tener MultiIndex indicamos el nombre del index que aplicaremos el reseteo.

    #Resetear index y nombrar la columna index:
    consultas = consultas.reset_index(drop=True).index.name = "id" #De tener multi-index usaria ".names" y corchetes.
    consultas = consultas.index.name = "id"
    consultas

  ```

* **`df.insert()`** : Añade una columna o lista de valores a lado derecho de la posición de columna que le indiquemos.<br>
**.insert(posicion, "Nombre_col", Serie o Lista)**
```python
          df.insert(loc=2, column="Número de pedido", value=df["Pedido"])
          #loc= nº columna, lo insertara a lado derecho
          #column= Nombramos la columna

          for i in Lista:
            main_table.insert(0,"Factura", os.path.splitext(os.path.basename(i))[0]) #index_col, nombre, valor

* **`df.assing()`** : Añade nuevas columnas al DataFrame, estas puedes incluir operaciones. Seria el equivalente a `with_columns` de `polars`.
```python
          df.assign(temp_f=lambda df: df['temp_c'] * 9 / 5 + 32,
                    temp_k=lambda df: (df['temp_f'] + 459.67) * 5 / 9)
          #Añadimos 2 nuevas columnas con las siguientes operaciones de las columnas existentes.
```
* **`df.replace()`** : Reemplaza valores de los valores de las columnas que le indiquemos.
  ```python
        #Reemplaza los valores True por 1 y False por 0
        df["paid"] = df["paid"].replace({True:1, False:0}, #ignore_index=True)

        #ignore_index=True : Resetea el index de 0 a n-1. Hace lo mismo que un"reset_index(inplace=True)"

        df["col_1].replace(to_replace= "texto regex a buscar", value= "valor de reemplazo", regex=True)

        #Quitar la coma de un numero en formato str y convertir en float.
        big_table["Valor Venta"] = big_table["Valor Venta"].str.replace(",","").astype(float)


  ```

* **`df.sort_values()`** : Ordena los valores. Por defecto viene en ascendente. *(ascending=True)*, para columnas *(,axis=1)*
  ```python
          df.sort_values(by=["Price","Score","Link"], ascending=[True,False,False], ignore_index=True)
          # Otra manera de ordenar de menor a mayor a columna "Price". by= funciona para listas o no.

          #ignore_index=True : Resetea el index de 0 a n-1. Hace lo mismo que un"reset_index(inplace=True)"

  ```
  En caso los datos o filas de la Columna estén nombrados con mayúsculas y minúsculas ("grupo A") se tiene primero que homogeneizar antes de ordenar para ello usamos el parámetro `,key= lambda x : x.str.lower()`
  ```python
          df.sort_values("Race/Ethnicity", ascending=True, key= lambda x:x.str.lower())

  ```
* **`df.shape`** : Muestra la cantidad de filas y columnas de la data. Para obtener el total de datos usamos. `df.shape[0]*df.shape[1]` *(filas * columnas)*
* **`df.size()`** : Muestra la cantidad de valores en el DataFrame. *(filas * columnas)*
* **`df.head()`** : Muestra las primeras n filas del df.
* **`df.info()`** : Muestra la información general con un `head()` incluido del df. Este método se suele usar para ver si hay **valores nulos** en las **columnas**.
* **`df.tail()`** : Muestra las ultimas n filas del df.
* **`df.rename()`** : Renombrar. Ejm:
```python
          #Renombrar una columna
          restdf = restdf.rename(columns={"Atendió":"Mesero"})

          #Renombrar el Index
          restdf = restdf.rename(index = {0:"Not Survived", 1: "Survived"})

          #Renombrar todas las primeras columnas de dfs:
          for i in range (0,10,2): #Va de 2 en 2
          df = tablas_html[i]
          df.rename(columns={df.columns[1]: "Team"}, inplace=True) #renombra todas las primeras columnas a "Team"
          df.pop("Qualification")
```
* **`~`** : Indica "No". En vez de arrojarnos el filtro que le indiquemos hace lo contrario, excluye el filtro que le indicamos de la tabla. Ejemplo Borra todos valores arrojados de un filtro. df[~<Filtro>.metodos()]<br>
Solo funcion con: `isin.([""])`
  ```python
    #Borra o Excluir todos valores arrojados de un filtro.
    df = df[~df["Nombre Proveedor"].isin(["HAMANN DISEÑO Y CONSTRUCCION S.A.C.","BANCO DE CREDITO DEL PERU"])]
    df.reset_index(inplace=True) #Reinicia el índice del df para que cuente desde 0.
  ```
* **`df.pop()`** : Borra una columna por su nombre.
  ```python
    df.pop("Qualification")
  ```
* **`df.drop()`** : Borra un dato al indicar su nombre o index para borrar columnas y solo nº de index para borrar filas.`,axis=0` para borrar fila o `,axis=1` para borrar columna.
  ```python
    #Para borrar una fila del df indicando un valor de la columna.
    df.drop(df.index[df["Cabin"]=="T"], axis=0, inplace=True)
  ```
* **`df.duplicated()`** : Muestra las filas donde encuentre datos duplicados. Para ello podemos indicarle las columnas de la tabla que queremos que haga la busqueda con `,subset=[]`.
  ```python
    df[df.duplicated(subset=["Date","Rank","Title","Category"], keep= False)]
  ```
* **`df.drop_duplicates()`** : Elimina elementos duplicados.
  ```python
    df.drop_duplicates(inplace=True, ignore_index=True)

    #ignore_index=True : Resetea el index de 0 a n-1. Hace lo mismo que un"reset_index(inplace=True)"

    #Eliminar Columnas duplicadas:
    df_2 = df.loc[:,~df.columns.duplicated()].copy() #Generamos copia para no sobreescribir la data df.
  ```

* **`df.dropna()`** : Borra filas o columnas enteras donde encuentre 1 datos en blanco o un total de fila o columna en blanco *(NaT,NaN)*. Por defecto borrara la filas en donde encuentra 1 dato null, es necesario usar *(inplace=True)* para aplicar el cambio.

  ```python
    df_population_raw.dropna(inplace=True, axis=0, how="any", ignore_index=True)
    #modo por defecto ,axis=0, how="any".
    #ignore_index=True : Resetea el index de 0 a n-1. Hace lo mismo que un"reset_index(inplace=True)"


    df_population_raw.dropna(subset=["Col_1", "Col_2"] inplace=True, axis=0, how="any")
    # ,subset= : Indicamos que columnas con nulos son relevantes para eliminar filas de la tabla.
  ```
  * axis = 0 : fila, 1 : columna
  * how = "all" : toda la fila o columna debe estar llenas de null, "any" : basta 1 solo null que encuentre.<br><br>

* **`df.isnull() o df.isna()`** : Arroja elementos nulos, vacíos (NaN). En combinacion son un operador como `.sum()` arrojará la cantidad de valores nulos de cada columna.
  ```python
    df.isna().sum()
    df.isnull().sum()
    #Ambas formas dan el mismo resultado.

    df['Código'].isnull().unique()
    #Busca elementos nulos únicos en la columna "Código"

    tabla_despv[tabla_despv["Creado Por"].isnull()]
    #Mostrara la tabla conteniendo todas las filas donde la columna tenga valor nulo.

    # Arroja la cantidad total de datos incluidos valores nulos de cada columna
    # Esto es debido a que count contara los valores true y false.
    df.isnull().count().sort_values(ascending=False)

  ```
* **`df.fillna()`** : Reemplaza los valores **NULL** con un valor **específico** a indicar. El método fillna() devuelve un nuevo objeto DataFrame a menos que el parámetro inplace se establezca en True , en ese caso, el método fillna() hace el reemplazo en el DataFrame original. <br>
Se suele utilizar este metodo para no borrar los valores nulo y se suelen sustituir por la **moda** de la columna.<br>
El parámetro `method=` es utilizado para rellenar huecos en serie reindexada relleno/relleno: propagar la última observación válida hacia adelante hasta el siguiente relleno válido/bfill: utilizar la siguiente observación válida para rellenar el hueco. {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}

```python
          df.fillna(0,inplace=True)
          #Reemplazara los valores nulos con un "0"

          df.fillna(method="ffill",inplace=True)
         #Este método propaga los Nan hacia adelante. Completa los nan con valores de fila anterior

          values = {"Col_A": 0, "Col_B": 1, "Col_C": 2, "Col_D": 3}
          df.fillna(value=values,inplace=True)
          #Asignara los valores que hemos definido en cada valor nulo de cada columna del df que encuentre.

```
* **`df.iterrows()`** : Separa el Index de los datos por columna concatenandolos por cada nuevo valor de la columna.
```python
          for row, datos in restdf.iterrows(): #rows= index , datos=df concatenado con todos sus valores de las columnas.
              Orden  = datos["Orden"]
              Tipo  = datos["Tipo"]
              Producto = datos["Productos"]
```
* **`df.round()`** : Redondea los decimales del df al numero que le indiquemos.
* **`df.unique()`** : Arroja el nombre de los elementos únicos.
* **`df.nunique()`** : Arroja la cantidad de los elementos únicos.
```python
          df["Tipos"].nunique()
          # 5 (resultado forma numérica)
```
* **`df['Columna'].value_counts()`** : Arroja una lista indicando el recuento de datos (valores) únicos en la columna. Este es muy usado para explorar el contenido de las columnas de un df.
```python

          df[['Código','Colm_2']].value_counts()
          #arrojara un recuento de los valores únicos de os pares de datos únicos por las 2 columnas.

          df['Código'].isnull().value_counts(sort=False)
          #Arroja buleano con conteo de elementos nulos en la columna "Código"

          #sort=False : Ya no ordena los valores de mayor a menor, sino que muestra el orden del index de menor a mayor tal cual esta ordenada la tabla.
```
  * `(normalize=True)` : Obtiene la "Frecuencia Relativa". Indica su proporción con respecto al total en decimales.
    ```python
          df['Código'].value_counts(normalize=True, dropna=False).round(2)
          #Ejem: Female = 0.55 ; males = 0.45; NaN = 1 (redondeado mostrando 2 decimales)
          #Con dropna establecido en False , nos dará el recuento de elementos NaN.
    ```

* **`df.apply()`** : Crea una columna calculada con la función que le indiquemos. Por lo general va compuesta por `lambda`, pero también podemos indicar una función de python o una propia que hayamos creado.
```python
          df["Dígitos"] = df["Cuenta"].apply(lambda x : len(str(x))) #cuenta nº de caracteres.
          df["Atrittion"] = df["Atrittion"].apply(lambda x : 1 if x == "Yes" else 0 ) #cuenta nº de caracteres.

          #Creamos una columna a partir de separar un texto de una columna por un caracter:
          df["Ciudad"] = df["Provincia"].apply(lambda x: x.split("-")[0]) #cortamos y nos quedamos con el primer segmento.
```
  * Limpiar columna numerica con indicador de miles o millones ("100M").
```python
          #Definimos una funcion a reemplazar datos.
          def unids(x):
            if x[-1] == "M": #Indica ultimo caracter.
              return(float(x[:-2])*1000000) #Multiplique el numero excluyendo la letra "M" y un caracter de espacio.
            else:
              return(float(x[:-2])*1000)

          # Usamos .apply para aplicar la funcion que hemos creado.
          df["Pagos"] = df["Pagos"].apply(unids)

          # Dar formato de dolar a un numero:
          f"${valor:.2f}"  => $ 50.00


```
* **`df.applymap()`** : Aplica cambios o reemplaza datos a las columnas que le indiquemos, segun el patron que designemos.
  Aplicamos un diccionario con comandos que indiquemos.
  ```python
    df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})

      df
      Out[54]:
        AAA  BBB  CCC
      0    1    1    2
      1    2    1    1
      2    1    2    3
      3    3    2    1

      # Asignamos los nombres de las columnas del df
      source_cols = df.columns
      new_cols = [str(x) + "_cat" for x in source_cols] #Creamos 3 columnas con _cat al final.

      categories = {1: "Alpha", 2: "Beta", 3: "Charlie"} #Configuramos nuestra categorizacion.
      df[new_cols] = df[source_cols].applymap(categories.get) # Aplicamos la Categorizacion a nuestras 3 columnas creadas.

      df
      Out[59]:
        AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat
      0    1    1    2    Alpha   Alpha     Beta
      1    2    1    1     Beta   Alpha    Alpha
      2    1    2    3    Alpha    Beta  Charlie
      3    3    2    1  Charlie    Beta    Alpha
  ```

* **`df.isin()`** : Sirve para filtrar por los valores de una columna, Retorna un booleano por lo que es recomendable para una mejor visualización aplicar una función de agregación al final o encerrarlo en un df.
```python
          df[df["Tipo de Orden"].isin(["OC", "OS"])] #Muestra toda la tabla con el filtro.
          df[df["Tipo de Orden"].isin(["OC", "OS"])]["Tipo de Orden"] #Mostrara solo esta columna en la tabla

```

* **`df.where() o np.where()`** : Es una condicional de formato **(Filtro,Valor_if,Valor_else)** que viene de numpy. Ejm: Para crear una columna con condicionales:
```python
    preciosdf["Margen"] = np.where(preciosdf["Ganancias"]>100,"Alto","Bajo")
    #Creamos una columna rellenando en funcion si cumple tendrá un valor de "Alto" else "Bajo".

    # Buscamos el precio donde GlivArea es igual a 4676 (Area de vivienda), también arroja su index.
    df["SalePrice"].where(df["GrLivArea"] == 4676).dropna()

    #Crea una tabla en el que se mostrarán las columnas "Descrición Material","Tipo de Orden"
    df[df["Creado Por"] == "JARCAN"][["Descrición Material","Tipo de Orden"]].where(df["Tipo de Orden"] == "OC").dropna()

    #Reemplaza valores a 0 donde df["A"] es nulo.
    df["A"].where(df["A"].isnull(),0)

```
* **`.stack() & .unstack()`** : Este metodo trabaja sobre Tablas agrupadas por diferentes niveles de columnas, trayendo un nivel de ellas como filas, sin mover el Index incial de la tabla (subindex). El nivel de la columna de más arriba es level=0 y la mas cercana hacia abajo es el de mayo nivel, por defecto trae un level= mayo nivel. `unstack()` hace el paso contrario. Mueve filas a nivel de columna.

  ```python
      df.stack(level=0) #Tomara la columna desde la más arriba como fila.

  ```

* **`df.groupby()`** : Creara una tabla dinámica con las filas de la columna indicada (puede ser más de una Columna).El método se lo agregamos al final. Se puede usar los `groupby()` como input para armar una tabla pivote ``pd.pivot_table()``.
Estos metodos son Iterable con cada valor de la columna dada para posteriormente extraer en un ciclo for.<br>
Al agrupar se generan la llaves y el indice esta escondido, no sale en el output.
*No usa parámetro ,",aggfunc=" pero si .agg({"Columna":"Opercion"},numeric_only=True)*<br>
*Soporta metodos como mean(), sum(), max(), min(), count(), size(), unique(), head(), tail(), idxmax, idxmin*<br>
*Parametro: dfgroupby().filter() y dfgroupby().transform()*
  ```python
            grupo_Tipo = restdf.groupby(["Tipo","Producto"]).mean(numeric_only=True)
            grupo_Tipo = restdf.groupby(["Tipo","Producto"]).head(3, numeric_only=True)
            #numeric_only=True : Aplicara la operacion solo en columnas numericas obiando las demas.
            #head() mostrara los 2 registros de cada agrupacion.

            grupo_Tipo.loc["Bebida"] #Mostrará las filas de "Tipo" -> "Bebida"
            llaves.loc[llaves.index=='2019-B'] #Mostrara donde el indice es '2019-B'

            #Podemos generar más de una operacion a una o varias columnas de la agrupacion.
            grupo_Tipo = restdf.groupby(["Tipo","Producto"], as_index=False).agg({"InvoiceNo":["unique", "sum", "count"]}, numeric_only=True).unstack() #valor unicos, suma y contar
            grupo_Tipo.loc["Bebida"]

            # ,as_index= True : En modo False hace que la agrupacion no lo tome como indice en su default (2 indices), sino que agrupa y resetea un indice nuevo.
            # ,numeric_only= False : En modo True solo aplicara la funcion en columnas de valores numericos.
            # unstack() : Este metodo convierte mueve los index secundarios a columnas o viceversa. (reformatea el agrupamiento incial del groupby())

            #Agrupamos por mes y comprador:
            pandas_df.groupby([pd.PeriodIndex(pandas_df["Creado El"], freq='M'),"Creado Por"]).agg({"Pedido":"sum"})

  ```
    * Para agrupar según una columna tipo de "kind" `datetime64[ns]` asignando una operación a las columnas afectadas. Tanto pd.PeriodIndex como pd.Grouper son validos y funcionan cuando la fecha es o no el indice de la tabla.
      ```python
            grupo = df_new.groupby(pd.PeriodIndex(df_new["Fe, de picking"], freq='M')).sum(numeric_only=True).sort_index(ascending=True).reset_index()
            #Le ponemos reset_index para que no me tome las fechas como index, pero las ordenamos antes de resetear.

            #Otra manera de lograr lo mismo usando pd.Grouper (se utiliza solo en combinacion con .groupby)
            grupo = df_new.groupby(pd.Grouper(key="Fe, de picking", freq='M')).sum(numeric_only=True).sort_index(ascending=True).reset_index()

      ```
    * Para agrupar valores que contengan todas estas combinacion de columnas
      ```python
          df = df.groupby(["Date","Rank","Title","Rating"])["3 stars","2 stars","1 stars"].mean(numeric_only=True)
          #Tomara todas las columnas como valor unico y agrupara en caso de haber duplicados promdeiando los valores en
          #las columnas ["3 stars","2 stars","1 stars"].
      ```
    * **``df.get_group("cat")``** : Tras tener un df agrupado por **`groupby`** podemos filtrar con `df.get_group("Elemento_a_Filtrar")`.

    * Podemos poner mas de una funcion en agg():
      ```python
        df.groupby("Nombre").agg([np.mean, np.std])["Valor_1","Valor_2"]
      ```
    * Parametro df.groupby().filter(): Aplicamos una funcion por la que queremos que cada valor de la se filtre. Se suele usar cuando queremos hacer agrupaciones de 2 columnas agregando una condicional a la segunda columna del df, como si fuera una sustitucion del ``.where()`` ya que ``groupby`` no soporta ese metodo. Acortando la tabla por el filtro.
      ```python
        df.groupby().filter(<funcion>).["Valor_1","Valor_2"] #Recortamos columnas procesadas a mostrar.

        #Obtener los datos de los estudiantes en cuyo salón, el máximo número de victorias fue 25.

        def calcular(group):
        return group["Victorias"].max()==25 #Maximo 25 en columna "Victorias" del df.

        ajedrez.groupby("Salon").filter(calcular).groupby("Salon").max() #Volvemos a agrupar ya que nos da todos los maximos que en suma son 25.
      ```
    * Paramtro dfgroupby().transform(): Aplicamos una funcion por la que queremos que cada valor de la agrupacion se procesara. Arrojando la tabla entera con los valores procesados.

      ```python
        #Mostrar como porcentaje de la suma total de la columna por la cual agrupemos.
        def porcentaje(x):
          return 100*x/x.sum()

        dfgroupby("Catergoria").transform(porcentaje)["Valor_1","Valor_2"] #Recortamos procesadas columnas a mostrar.
      ```

* **`df.pivot()`** : Devuelve el DataFrame remodelado (reshape) organizado por valores, sin agrupar, de índice/columna dados.<br>
Solo tiene 3 parámetros: `,index=` `,columns=` `,values=`, `,aggfunc=`.

  ```python

    # unstack() : Este metodo convierte mueve los index secundarios a columnas. (reformatea el agrupamiento incial del pivot())

  ```
* **`df.crosstab()`** : Cruza 2 campos de un df poniendo una en index y otra como columna mostrando sus valores unicos de ambos campos y poniendo el conteo donde se cruzan los datos unicos ambos campos.<br>
Para vizualizacion de conteos cruzando 2 campos es mejor `.crosstab()` pero podemos modificar sus valores y su aggfunc. Es muy usado como tabla resumen de entre 2 columnas o campos pudiendo ver su % con respecto al total usando `,normlazing=`
Parámetros: `,index=` `,columns=`. De usar ``,values=`` tenemos que indicarle su `,aggfunc=`.<br>

  ```python
    pd.crosstab(index = train_data['Survived'],columns = train_data['Pclass'],
                 margins= True, margins_name= "Total")
          #,Normlazing= : True, "index", "columns". En modo True indicara los valores en porcentaje del total general.
          #En modo index indicara los valores en porcentaje del total de cada indice, etc.

    #Se puede combinar con mas metodos ejemplo .apply()
    pd.crosstab(index=train_data['Survived'], values=train_data['Pclass'], aggfunc="count",
            columns=train_data['Pclass'], margins=True, margins_name="Total", normalize=True).apply(
            lambda r: r*100, axis=0).round(2) #Redondea los datos a un 100% 2 decimales
  ```
* **`df.pivot_table()`** : Crea una tabla dinámica agrupando sus valores por la operación que indiquemos por `,aggfunc=`.<br>
El "`,aggfunc=`" que traen por defecto es promedio. Puede hacer una tabla pivote de una `groupby()` <br>
***(DataFrame,Valores,Filas target,(jerarquía)columnas mostrada en valor precio,aggfunc= función a aplicar,margins = subtotales)***

  * `,index = ` : Columna que usara como index, para multi index englobar en corchetes [].
  * `,columns = ` : Indicamos la columna que ira en horizontal.
  * `,value = ` : Columna con valores a aplicar funcion.
  * `,aggfunc =` : Indicamos el método que operara con los valores. *aggfunc=np.sum o aggfunc="sum"; aggfunc="mean"; "idxmax",etc aggfunc= pd.Series.nunique #Devuelve un valor distinct count; aggfunc=np.count_nonzero #Cuenta valores no nulos*
  * `,fill_value = 0` : Reemplaza todos los valores nulos de df encontrado por el numero o string que le indiquemos.
  * `,margins = True` : Muestra los Subtotales de las columnas como filas.
  * `.unstack()` : Este metodo trabaja sobre Tablas agrupadas por diferentes niveles de columnas, trayendo un nivel de ellas como filas, sin mover el Index incial de la tabla (subindex). El nivel  más alto de columnas es level=0 y la mas cercana hacia abajo es el de mayo nivel.

  ```python
    vent_tipo_mesero = df.pivot_table(values="Precio",index=["Tipo_x","Categoria"],
                        columns="Mesero",aggfunc=np.idxmax,margins=True, fill_value=0).round(0)
                        #arroja el indice con valores maximos.

    # Tabla agrupada por columnas en Meses del conteo distinto (no repetidos) de los ID de compras.
    df.pivot_table(index=["Creado Por",df["Tipo de Orden"]], columns=[pd.PeriodIndex(df["Creado El"], freq="m")],
                values=["Pedido"], aggfunc=pd.Series.nunique) #Devuelve un valor distinct count

        data3 = df.pivot(columns="OverallQual", values="SalePrice")

    #agrupando por index de 2 niveles con columnas por mes:
    table = df.pivot_table(index=[df[df["Creado Por"].isin(["JARCAN","JPEREDAA","LMOSTACEROR"])]["Creado Por"],
                      df[df["Tipo de Orden"].isin(["OC", "OS"])]["Tipo de Orden"]], #Es un Index de 2 Dimensiones.
                      columns=pd.PeriodIndex(df["Creado El"], freq="m"), values="Pedido",
                      aggfunc=pd.Series.nunique, fill_value=0).unstack()

                    #Devuelve un valor distinct count de "Pedido", margins=True no funciona, da error.
    table


  ```
* **`df.melt()`** : Aloja a todas las columnas de la tabla en una sola columna haciendo que la columna que indiquemos como `,id_vars=["col"]` se repita por cada columna restante que existe en la tabla, alojando estas columnas en una sola columna y los valores de estas, en otra columna independiente. Mantiene el index inical, no lo modifica ni sustituye.<br>
En una tabla pivote tenemos los siguientes elementos: Columna con filas repetidas (id_vars), columnas (values), values (value_name).


  * `,id_vars = ` : (filas) Nombre de la columna por la que fue agrupada la tabla, esta es la que va en index o filas.
  * `,values = ` : (Columnas) Nombres de las columnas de la tabla pivote, para no digitar en una lista nombre por nombre de todas las columnas es recomendable sacar sus nombres con una compresión de listas y guardarla en un variable para luego asignarla a este campo de values.
  * `,var_name = ` : (Encabezado Columna) Asigna un nombre o encabezado al campo de las columnas de la ex tabla pivote. De no asignarle un nombre pondrá un nombre por defecto.
  * `,value_name = ` : (Encabezado Valores) Dado que separara los valores en una columna aparte es necesario asignarle un nombre como "Ventas".
  ```python

    df.melt(id_vars=["day_weeks"], var_name="Cols",value_name="Temperature")

    #Despivoteamos una tabla agrupada por productos con columnas por año y ventas en valores.
    data3 = df.melt(id_vars=["Producto"], values=["2010","2011","2021"], value_name="Ventas", var_name="Año")

    #Primero para listar todos los nombres de la columna values y no estar escribiendo año por año el nombre.
    años = [col for col in df if col!="Producto"] #devuelve todos los nombres de la columna menos la primera "Producto"

    #Tras hacer esto podemos igualar ,values= años

    #Podemos tambien Usar varias columnas en valores
    df.melt(id_vars=["day_weeks"], var_name="Cols",value_name=["Temperature","Instrument"])
    #Nos arroja pares de 2 filas por le mismo id_vars=["day_weeks"]

  ```
* **`df.merge()`** : Es la union de tablas como se efectúan en ``SQL``.<br>
Se puede precisar la columna de union con `,on=["Columna"]`.<br>

No posee parametro Axis dado que nos traera las columnas del otro df a unir.
  * **`,how=`** indica el tipo de "kind" de union, por defecto esta en ``INNER JOIN`` (intersección) donde en ambas tablas existan datos.
  * **`,on=`** : (col_ID) Por defecto unira las columnas con igual nombre. Indicamos la Columna por la cual uniremos las tablas.
  * **`left_on= 'lkey', right_on='rkey'`** : Por defecto junta en una columna de igual nombre. Con este parametro indicamos el nombre de la columna izquierda y luego derecha con la que queremos que haga match. Se usa este modo cuando las columnas que queremos hacer match no se llaman igual.

  * **`left_index= True, right_index=True`** : Usamos estos parametros cuando la queremos que haga match no con el nombre de una columna sino con el index de la tabla. Podemos combinar el especificar el nombre de una columna con un index.

  * **`suffixes=('_high','_low')`** : Añade sufijo a las columnas con nombre repetido empezando por left y right index o nombre de columna o combinacion de estas: "Ranges_high",	"Ranges_low"
  * **`,indicator=False`** : En modo True agrega una columna indicando si hallo datos en ambas uniones colocandolo como "both" y si hallo solo datos en "left_only" y "right_only" (dejando NaN donde no hallo datos).
  * **`validate='m:1'`** : Indica el tipo de tablas que vamos a unir: Ejm: Tabla dimenciones con hechos.
    * one to one :  "1:1"
    * one to many : "1:m"
    * many to one : "m:1"
    * many to many :"m:m"
```python

          #LEFT JOIN (columna izquierda completa)
          df = df_A.merge(df_B,on=["Producto"],how="left")

          #OUTER JOIN Preserva todos los valores Tabla A y B. Podemos mostrar una columna especifica de la union.
          df = df_A.merge(df_B["name"],on=["Producto"],how="outer")

          #Unir Filas Por Index: Esto nos dara una columna para los index A y otra para B; combinando sus index. Rellenara con nulls
          df = df_A.merge(df_B, left_index=True, right_index=True)

          #Especificamos la union en 2 columnas de diferentes nombres.
          df1.merge(df2, left_on='lkey', right_on='rkey')

          #Manera completa:
          pd.merge(df1, df2[['rkey','name']], left_on= 'lkey', right_index=True , how='left', validate = 'm:1')
          #Validate es opcional. Traemos las columnas df2[['rkey','name']] a la union y no todo el df2
```
* **`df.sample()`** : Coje una parte de la data del DataFrame de manera aleatoria o de la semilla que nosotros indiquemos.
```python
          muestra_df = california_df.sample(frac=0.1, random_state=17)

          # Creamos una nueva columna en df_2 con los datos de df donde solo tomaremos 100 valores de manera aleatoria
          # del total.
          df_2["network"] = df["network"].sample(n=100).tolist()
```
* **`df.resample()`** : Afecta a datos de tipo fecha, para ellos es necesario tener la fecha en el index. Este metodo es parecido a la funcion ``pd.PeriodIndex()``, el cual agrupa el indice de fechas sea a "d", "m", ""
```python
          df.resample("m").mean()
          #Podria conjugar con funcion min(), max() (minimos o maximo mensuales, etc)
```

* **`df.corr()`** : Devuelve una tabla matriz de correlaciones de todos los campos de la tabla de Columna vs columnas, excluyendo NA/valores nulos y textos.
  ```python
          Correlacion = restdf.corr(method='pearson', min_periods=1, numeric_only=True)

          rest.corr()["AMZ"].sort_values(ascending=False)
          #Muestra la correlación de una sola columna con los demás campos de manera descendente
  ```
  * method= pearson : standard correlation coefficient; kendall : Kendall Tau correlation coefficient; spearman : Spearman rank correlation
  * min_periods=1 : (Opcional, solo para "pearson" y "spearman") Número mínimo de observaciones requeridas por vs.
  * numeric_only=True : Es necesario indicar que solo actue sobre datos numericos para evitar errores.

* **`df.corrwith()`** : Correlacion de una columna con respecto a las demás. La correlación por pares se calcula entre filas o columnas con otro df de la misma dimension. `,method=` "pearson", "kendall", "spearman". Ejm: Correlación de 1 campo con respecto al resto de un mismo df:
  ```python
          df.corrwith(df["SalePrice"], method="pearson", axis=0).nlargest(10).sort_values(ascending=False)
          #Va a correlacionar una columna del df con todas las demás. Entregando los 10 campos más correlacionados.
  ```
  * method= pearson : standard correlation coefficient; kendall : Kendall Tau correlation coefficient; spearman : Spearman rank correlation
  * min_periods=1 : (Opcional, solo para "pearson" y "spearman") Número mínimo de observaciones requeridas por vs.

* **`df.plot()`** : Muestra una gráfica lineal. Se puede llamar el tipo de "kind" de gráfica con ``df.plot.GRÁFICA()`` o ``df.plot(kind=GRÁFICA)`` Tiene la siguiente configuración:
    ```python
          axes = temps_df.plot(x="Fahrenheit", y="Celsius",style=".-",figsize=(10,8))
          y_label = axes.set_ylabel("Celsius") #Mostrar etiqueta "Y"
    ```
  * **`df.plot.scatter()`** :Gráfico de Dispersion: "Scatter Plot"
    ```python
          axes = restdf.plot.scatter(x="Producto",y="Propina")

    ```
  * **`df.plot.bar()`** : Muestra una gráfica de barras. Tiene la siguiente configuración:
    ```python
          axes=datos.plot.bar(x='palabra',y='frecuencia',legend=-False)  #Invierte la legenda del eje "x" La pone en Vertical.
    ```

---
### Propiedades:
El indexado es igual que numpy trabaja con corchetes **[ : : ]** para indicar los rangos de datos que queremos que nos arrojen. No trabaja con paréntesis. *Ejm: pd.DataFrame.loc*

* **`.index[]`** : Hace referencia al index. A diferencia de `loc` y `iloc` Devolverá el nº de index del valor indicado. Podemos tambien manipular el index ya que esta siendo referenciado, ejemplo cambiar el nombre ``.names``, sirve mucho para multi-index.<br>
Con esta propiedad referenciamos al index, el cual obtiene todos los metodos que usamos con cualquier columna o df.
  ```python
    #Borra el valor "T" de la columna. (Más usada)
    train_features.drop(train_features.index[train_features["Cabin"]=="T"],axis=0) #Elmina toda las filas.

    #Se puede usar en combinación con loc y iloc.
    train_features.drop(train_features.loc[train_features["Cabin"]=="T",:].index,axis=0)

    #Mostrar el indice de un agrupamiento:
    llaves.loc[llaves.index=='2019-B'] #Mostrara donde el indice es '2019-B'

    #Renombrar el index (Multi-index)
    df.index.name = "id"
    df.index.names =["Region", "Comuna", "Organismo"]a

    #Cambiar el tipo de dato del index
    df.index.astype(int)

    # Cambiar el tipo de dato de Multi-index
    df.index = pd.MultiIndex.from_tuples([(x, int(y)) for x, y in df.index])
  ```
* **`.columns[]`** : Muestra el nombre de todas las columnas del ``df`` . Podemos indicar el numero de columna del df que queremos que nos muestre.
Podemos también cambiar el nombre de las columnas en el orden que estas están con esta propiedad. EJM:
  ```python
          kchouse.columns=["AreaFt","P.V"] #Cambia el nombre en el orden que están las columnas de df.

          #Funciona igual con la propiedad .names
          kchouse.columns.names =["AreaFt","P.V"] #Cambia el nombre en el orden que están las columnas de df.

          df.columns[1] #Mostrara la segunda columna del df.

  ```

* **`.loc[]`** : Se usa para visualizaciones. Imprime una columna por su 'nombre'. Para ello es necesario indicar primero el indice luego el nombre de la columna. Imprime también intersecciones con columnas. **['fila','columna']**. También puede cambiar datos del DF. Para seleccionar más de un index a la vez usamos doble **[[]]**

  ```python
            df.loc[:,"SalePrice"] #Siempre indicamos primero el indice y luego el nombre de la columna.
            df.loc[nºIndice,'columna'] or [1:2,"Producto"] #Rango de filas de una columna o sin ella.
            df.loc['fila']
            df.loc['fila','Fila dentro de fila'] #Encaso este en un groupby o alguna tabla dinámica.
            df.loc['fila':'fila']

            #Arreglos de Numpy
            X_train[:, :5] selecciona las primeras 5 características de cada fila.
            X_train[:, 2:] selecciona todas las características a partir de la tercera columna en adelante en cada fila.

            #para poder seleccionar más de un index a la vez usamos doble [[]]
            df_population_sample = df_population.loc[["1980","1990","2000","2010","2020"]]

            #También puede usarse el siguiente código para indicar columnas.
            df = df[df.index.isin(["2020","2021"])] #usa corchetes porque usa lista.

            #Podemos usar el indice de otra columna para filtrar otra:
            df.loc[df['TotalBsmtSF']>0,'HasBsmt'] = 1 #Escribe 1 en todos los indices donde "TotalBsmtSF" sea mayor a 0 en "HasBsmt".

            #Otra manera de hacer lo mismo sin "iloc". Reemplazamos los valores 0 de "HasBsmt" por "1" donde "TotalBsmtSF" sea mayor a 0.
            df[df['TotalBsmtSF']>0]['HasBsmt'] = 1
            df.loc[df['TotalBsmtSF']>0,'HasBsmt'] = 1

            #Un filtrado en 2 dimensiones fila,columnas[Columna a mostrar].
            df.loc[df["Tipo de Orden"].isin(["OC", "OS"]),["Tipo de Orden"]]["Tipo de Orden"]] #Añadimos esta columna a mostrar al final

            #Del ejercicio anterior otra manera de hacer el filtrado sin loc:
            df[df["Tipo de Orden"].isin(["OC", "OS"])]["Tipo de Orden"]]

            #Añade una columna "Total" en el df
            tabla_total["Total"] = tabla_total.sum(axis=1)

            # Otra forma con .loc Añade una fila "Total" en el df
            tabla_total.loc["Total"] = tabla_total.sum(axis=0)

            # Filtrado con cadena de condiciones "y" (&).
            df.loc[df["Series"].str.endswith("Slam")&(df["Surface"]=="Clay")&(df["Winner"]=="Federer")]

            #Filtrando por el indice de un agrupamiento:
            llaves.loc[llaves.index=='2019-B'] #Mostrara donde el indice es '2019-B'

            #Equivalente a "BuscarX". Buscar un numero en una columna arrojando el valor de otra.

            df.loc[df['OC'] ==4500101020, 'PROYECTO'].values[0] #Nos quedamos solo con el valor obiando el index.

            df[df["OC"]=="4500101008"]["PROYECTO"].values[0] #sin usar .loc

            df[df['Nombre de la Columna'].str.contains(Regex)]['Columna_Mostrar'].values[0] #usando ".contains()"


  ```
    También se puede usar para filtrar:
  ```python
            restdf.loc[restdf["Propina"]>0.05]
  ```
  Para filtrar Multi-Index, usamos Tupla para referenciar el nombre del index: df.loc[("Primer nivel", "Segundo nivel"):, ("Index_col1", "Index_col2")]
  ```python
  df.loc[("Primer nivel"):,:] # Este caso hay 3 niveles de index.

  df.loc[("Primer nivel", "Segundo nivel"):, ("Col1")] #Los niveles escritos van dentro de la tupla.

  # Para Filtrar en rango de años es necesario cambiar a intero los años de las fechas:

  df.columns.set_levels(levels= df.columns.levels[0].astype(int), #referencia a convertir a int la primera fila de columas
                        level= 0, inplace=True) #Aplica lo mismo en caso los años esten como filas usariamos ".index".
  df.loc[("Primer nivel", "Segundo nivel"):, (2010):(2013)] #Los rangos deben estar en ruplas independientes.
  ```

* **`.iloc[]`** : Se usa para visualizaciones por indice. Imprime una fila por su indice con todas las columnas correspondientes.

```python
          df.iloc['nºfila']
          df.iloc['nº.fila':'nº.fila2'] #hasta fila 2
```


* **`.at[]`** : imprimirá solo el valor específico en la intersección de fila y columna señaladas por su 'nombre'. ['fila','columna'].También podemos asignarle **"="** un nuevo valor a la intersección.

* **`.str[]`** : Referencia a la librería ``String``, convierte todos los resultados de sus métodos a formato String. Nos permite utilizar todos los metedos que usa una lista en python. Habilita métodos regex. *(Expresiones Regulares)*
  ```python
    #Mostrar la primera letra de la columna Cabin.
    train_features["Cabin"] =  train_features["Cabin"].astype(str).str[0] #Pasamos a formato string antes.

  ```
  * **`.str.upper()`** : Convertira todos los textos en mayusculas.
  ```python
      df["Catergory"] = df["Catergory"].str.upper()
  ```
  * **`.str.title()`** : Reformatea el texto poniendo la primera letra de cada palabra en mayuscula.

  * **`.str.strip()`** : Elimina Todos los Espacios en blanco tanto a la izquierda como a la derecha de la lista, no elimina espacios intermedios entre palabras. Elimina tambien todos los caracteres que le indiquemos:
  ```python
      txt = ",,,,,rrttgg.....banana....rrr"
      x = txt.strip(",.grt")
      out: "banana"

  ```
  * **`.str.cat()`** : Reemplaza todos los espacios entre palabras por lo que le indiquemos.
  ```python
      df.str.cat(sep=" ") # reemplaza todos los espacios de los caracteres intermedios por un solo espacio.
  ```

  * **`.str.split()`** : Separa una cadena de texto segun el caracter que le indiquemos, por defecto entrega una lista con los 2 valores separados. Para que nos entrege 2 listas separadas mediante el caracter que hemos indicado usamos el parametro: ``,expand=True``
    ```python
      df[["ColumA","ColumB"]] = df["Address"].str.split("-", n=1, expand=True)[0] #Creamos 2 culmnas en el df donde se alojara las 2 listas con los textos separados, a la primera coincidencia (n=1)."[0]" Tomamos el primer corte.

      #,n=1 : Indicamos que haga el corte a la primera aparicion del caracter de izquierda a derecha.
      #,expand=True : Nos entrega en 2 listas los textos separados para asignarlos a 2 columnas nuevas en el df.

      #Usamos get() para seleccionar de la lista cortada segun el numero de posicion que queremos mostrar.
      deportes.str.split(",").str.get(2) #Mustra tercera columna del df segun su indice de columna.

    ```

  * **`.str.contains()`** : Este método Habilita el poder hacer un búsqueda por Expresiones Regulares en los valores del df o columna. Se puede lograr activando el parametro `regex=True` (viene asi por defecto) o invocandolo antes como `.str.contains(r'')` en donde escribimos la Expresion Regular dentro del **r''**.<br>
  Buscara en cada string el patron de búsqueda indicado (r''). No se ve afectado si hay mas datos después del patron que queremos hallar. Vendria a ser un **``re.findall()``**
    ```python
      df[df["Pedido"].str.contains("^451", regex=True)]

      #Buscar X, buscar un dato y arrojar solo el valor de la columna indicada en referencia con el valor buscado.
      resultado = df[df['Nombre de la Columna'].str.contains(i)]['Otra Columna'].values[0]


      #Para hacer uso de flags=re.IGNORECASE necesitamos importar la libreria ``re``
      import re
      df[df["Pedido"].str.contains("OC|OS", regex=True, flags=re.IGNORECASE, case=False, na=False)]

      #regex=True : Por defecto esta en modo True el cual activa lectura en modo regex. En False modo String.
      #flags=re.IGNORECASE : Ignora mayusculas y minusculas (desde la libreria "re" previamente importada)
      #case=False : Ignora mayusculas y minusculas. Buscara la coincidencia de lo indicado en modo "True" (default). En modo "False" lo omitara arrojando un resultado obiando lo indicado. Seria su paralelo al simbolo "~".
      #na=False : Rellena los valores nulos con False haciendo que no arroje error si se topa con un NaN.
      # "nombre\.|nombre2\." : Despues del "\" indica que lo que sigue son simbolos de REGEX.
    ```
  * **`.str.match(r'')`** : Busca que el elemento contenga unicamente lo especificado en la búsqueda. Si hay un dato mas a lado botara como falso. Vendria a ser un **``re.fullmatch()``**

  * **`.str.replace("$","")`** : Reemplazara el valor existente por uno indicado después de la coma, en este caso por nada (indica borrar símbolo "$".). Vendria a ser un **``re.sub()``**
    ```python
          #Elimina todas las coincidencias del corchete [] como del "18%" unido por el operador OR "|".
          df["Total"] = df["Total"].str.replace(r'[,$PENUSDIGVTOTALSUBCargos]|18%', '',regex=True)

    ```
  * **`.str.extract()`** : Este método Habilita el poder hacer un búsqueda por Expresiones Regulares y Extraer texto segun lo indicado sin remover de la fuente original los valores.
  ```python
          #Extraemos nombres y apellidos en 3 columnas nuevas manteniendo la fuente ["fullname"] con sus datos.
          df["First Name"] = df["Full Name"].str.extract("(\w+)")
          df["Middle Name"] = df["Full Name"].str.extract("(?:[\w]+\s)(\w[.]?)(?:\s\w+)")
          df["Last Name"] = df["Full Name"].str.extract("([\w-]+$)")

  ```

* **`.dtype[]`** : Mostrará el tipo de "kind" de dato de cada columna. ***(int, float, object,datetime64[ns])***.

* **`.values[]`** : Arroja solo los valores omitiendo los index. Arrojara los valores de la columna seleccionada en forma de array ([1,2,3]).

  ```python
              kc_house["Areaft"].values
  ```
* **`.dt[]`** : Referencia a la librería ``datetime``. Se aplicará a todos las columnas de tipo de "kind" ``datetime64`` y ``timedelta64`` (Diferencia entre 2 datetime64). Este tipo de "kind" de dato son el resultado de operar columnas de tipo de "kind" fecha como restar 2 columnas de tipo de "kind" `datetime64`.

  ```python
    #Convertir columna datetime a date
    df['Order Date'] = df['Order Date'].dt.date

    df['Order Date'] = df['Order Date'].dt.strftime("%d/%m/%y")
    #Arrojara valores de formato string.

    df['Order Date'] = df['Order Date'].dt.strftime("%b")
    #Arrojara los meses en abreviados.

    df['Order Date'] = df['Order Date'].dt.month_name(locale='es_ES.utf8') # en_US (ingles)
    #Arrojara nombre de los meses en Español

    df['Order Date'] = df['Order Date'].dt.month_name(locale='es_ES.utf8').str.slice(stop=3)
    #Arrojara nombre de los meses en Español abreviados.

    df["Lead Time Entrega"] = (df["Creado El"] - df["Fecha entrega SAP"]).dt.days
    # Devolverá la diferencia en valor de formato entero (int32), sin el dt devolvera un timedelta64.
  ```
---
### Parámetro:

Van ligados a un método por una **"`,`"**

* **`,index=`** : Añade o reemplaza indices en forma de key a los datos de una serie, son encabezados para una fila. Este parámetro hace lo mismo que un diccionario pero de una forma más rápida a la hora de digitar.
* **`,columns=`** : Asigna nombre a las columnas del DF.
* **`,axis=`** : Dirige el procesamiento a filas o columnas de la matriz. **0** para **filas** y **1** para **columnas**.
  ```python
          #Creamos una nueva columna que sea el promedio con todas las columnas:
          df["Valor_prom"] = df.mean(axis=1)

  ```
* **`,ascending=`** : Ordena a ascendente, por defecto viene en True.
* **`,inplace=`** : Viene por defecto en `False`, en modo `True` toma un método que no reemplaza al archivo o valor raíz, como un filtro, para reemplazarlo por la modificación hecha o método aplicado.
* **`,key=`** : Podemos añadir una función `lambda` que procesará previamente nuestro método.
* **`,na=`** : En modo ``false`` ignorará los datos vacíos.
* **`,case=`** : En modo ``false`` ignorará mayúsculas y minúsculas.
* **`,regex=`** : En modo ``true`` entenderá los operadores de `re` como el "or" ("``|``").
* **`,legend=`** : En modo ``-false`` Pondrá la leyenda del eje "x" en Vertical en el `.plot.bar()`.

## [7. openpyxl](#indice)
Es una librería que nos permite abrir los archivos excel y operar como si estuviéramos dentro del software. Cabe resaltar que pandas corre archivos excel bajo esta librería en su interior.<br>
*pip install openpyxl*

Se importa de la siguiente manera:
```python
import openpyxl
```
Otras librerías dentro de openpyxl
```python
from openpyxl.chart import BarChart, Reference #Gráfico de cuadros
from openpyxl.styles import Font #Fuentes de excel
```
Para el manejo de las celdas de excel es necesario la librería `string` el cual nos traerá el abecedario `.ascii_uppercase`.
```python
import string

  abecedario = list(string.ascii_uppercase)
```

### clase, Función:

* **`load_workbook()`** : Carga un archivo de excel `.xlsx`

  ```python
          wb = load_workbook('sales_2021.xlsx')#abrimos resultado de pandas
          pestaña = wb['Report'] #asignamos a la variable la pestaña existente
  ```
### Librerías, Métodos:

* **`Reference()`** : Librería para referenciar los datos que se graficará. Primero se referencia los datos luego se gráfica. Previamente debemos indicar con las propiedades de `.active` donde están los datos con los que se referenciará la gráfica.
  ```python
          data = Reference(pestaña, min_col=min_col+1, max_col=max_col, min_row=min_fila, max_row=max_fila)
          #data hace referencia a Columnas a tomar.
          categorias = Reference(pestaña, min_col=min_col, max_col=min_col, min_row=min_fila+1, max_row=max_fila)
          #categorias hace referencia a Filas a tomar.

  ```
* **`BarChart()`** : Librería de Gráfico de barras de Excel. Es necesario asignar esta librería a una **variable** para usar sus siguientes **Métodos**:
  ```python
        barchart = BarChart()
  ```
  * **`.add_data()`** : Añade el `Reference` con respecto a columnas (`data`) al gráfico.
    ```python
          barchart.add_data(data, titles_from_data=True)

    ```
  * **`.set_categories()`** : Añade el `Reference` con respecto a Filas (`categorias`) al gráfico.
    ```python
          barchart.set_categories(categorias)

    ```
  * **`.title=`** : Asigna un titulo al gráfico. Ejm: "Nombre"
  * **`.style=`** : Asigna un estilo al gráfico. Ejm: 5
  <br><br>
* **`.add_chart()`** : Este método añadira el grafico previamente creado a la pestaña de nuestro workbook cargado (`pestaña = wb["Nombre_Pestaña"]`)
```python
          pestaña.add_chart(barchart, 'B12')
```

### Propiedad:

* **`.active`** : Entra a la pestaña activa de la variable donde previamente hemos cargado el archivo excel, tiene las siguientes propiedades: Cabe resaltar que el conteo de las columnas y filas activas las arroja en forma de numero empezando con el valor "1" y no "0" como lo es en python.
  ```python
          min_col = wb.active.min_column # Cuenta empezando por "1"
          max_col = wb.active.max_column
          min_fila = wb.active.min_row
          max_fila = wb.active.max_row
  ```
  * **`.min_column`** : Columna minima de nuestro cuadro o tabla.
  * **`.max_column`** : Columna maxima de nuestro cuadro o tabla.
  * **`.min_row`** : Fila minima de nuestro cuadro o tabla.
  * **`.max_row`** : fila maxima de nuestro cuadro o tabla.
  <br><br>
* **`.style`** : De la librería ``openpyxl.styles`` da formato a los gráficos.
  ```python
            pestaña[f'{i}{max_fila+1}'].style = 'Currency'
            #Convierte una celda a tipo de "kind" moneda en excel.
  ```
* **`.font`** : De la librería ``openpyxl.styles`` da formato a la celda.
  ```python
            pestaña['A1'].font = Font('Arial', bold=True, size=20)
  ```

## [8. matplotlib ("*plt*")](#indice)
Es una librería que da una representación gráfica de nuestros datos en cuadros estadísticos en 2D.<br>
*pip install matplotlib*

Se importa de la siguiente manera:
```python
    import matplotlib.pyplot as plt
```
### clase, Función:

* **`plt.plot()`** : Son las figuras o graficas que van dentro del lienzo.

  ```python
      re_x = np.arange(100,14000,1) #rango eje "x"
      re_y = re_x*reglin.slope + reglin.intercept #regresion para cada valor "y"
      ax.plot(x,y,"r--")
      #Previamente definimos "x" , "y" con tipo de "kind" de linea y color

      # 2 trasos dentro de una misma grafica:

      plt.plot(fp_roc, tp_roc,"b--", label="ROC Curve") #primer trazo x,y
      plt.plot([0,1],[0,1],"k--", label="Escuadra") #Dibujamos una linea punteada de 0,0 a 1,1. Segundo Trazo
      plt.xlabel("FPR")
      plt.ylabel("TPR")
      plt.title("ROC Curve")
      plt.legend(loc="upper right",bbox_to_anchor=(1,0.9)) #Leyenda en parte superior derecha y le bajamos un pelin.
      plt.grid() #Activamos el Grid
      plt.show()

  ```
* **`plt.figure()`** : Crea un lienzo para insertar un gráfico dentro. Podemos indicar nºfilas (nrows=), nªcolumnas (ncols=), tamaño del lienzo (figsize=): La variable `axes` o `ax` es la que dibujará en el lienzo para ello indicamos el tipo de "kind" de gráfico que queremos e indicaremos con que datos graficará:

  ```python
      plt.figure(figsize=(16,9))

  ```

* **`plt.subplot()`** : Crea multiples minigraficos. estas no poseen el atributo `plot`. Añade `axes` a la figura existente (lienzo) para insertar un gráfico dentro. Podemos indicar nºfilas (nrows=), nªcolumnas (ncols=), tamaño del lienzo (figsize=): La variable `axes` o `ax` es la que dibujará en el lienzo para ello indicamos el tipo de "kind" de gráfico que queremos e indicaremos con que datos graficará:

    ```python
    figure, axes = plt.subplot(nrows=4, ncols=6, figsize=(6,4)) #para multicuadros.
    ```
  * Para regresión lineal:

    ```python
      fig, ax = plt.subplots(figsize=(6,5))
      ax.scatter(x=kchouse.AreaFt, y=kchouse["P.V"]) #scatter de datos
      re_x = np.arange(100,14000,1) #rango eje "x" (necesario numpy)
      re_y = re_x*reglin.slope + reglin.intercept #regresión para cada valor "y"
      ax.plot(re_x,re_y,"r--") #variable ax es la que dibuja el gráfico.
      plt.show()

    ```
* **`plt.title()`** : Inserta titulo al gráfico.
* **`plt.xlabel()`** : Inserta titulo al eje x.
* **`plt.ylabel()`** : Inserta titulo al eje y.
* **`plt.legend()`** : Insertara la leyenda a nuestra gráfica.

    * **``,loc=``** : Indica la posición donde se ubicara la leyenda ejemplo ("upper left","upper right", etc)

## [9. seaborn ("*sns*")](#indice)

Es una librería para visualización de datos, esta esta desarrollada sobre **matplotlib**. Además está integrada con la librería de **pandas** por lo cual puede leer **DataFrame** y campos directamente como argumentos de las funciones de visualización:
___
### Clase, Función:

* **`sns.set()`** : Cambia el tamaño de las fuentes en las gráficas.
```python
        sns.set(font_scale=2)
```
* **`sns.set_style()`** : Cambia los ajustes a la configuración de la librería Seaborn.
```python
        sns.set_style("whitegrid")
```
### Graficos:

* **`sns.barplot()`** : Diagrama de barras.
```python
        axes =sns.barplot(x=valores,y=frecuencias, palette="bright")
```
* **`sns.regplot()`**: Muestra un diagrama de regresión lineal de 2 variables.
```python
        axes = sns.regplot(x=kchouse.AreaFt, y=kchouse["P.V"], scatter=True,order=1,color="blue", label="order 1", line_kws={"label":"y={0:.1f}x+ {1:.1f}".format(reglin.slope,reglin.intercept)})
        #order= nº de polinomio de la regresión.
```
* **`sns.heatmap`** : Diagrama o mapa de calor.
```python
        axes = sns.heatmap(confusion_df, annot=True, cmap='nipy_spectral_r')
```
* **`sns.displot()`** : Histograma con curva normal. Muestra la distribucion en barras de la grafica seguida de una curva lineal de como deberian de distribuirse los datos.
  ```python
        sns.displot(df["Col"])
        #,hist=False : Quita las barras del histograma para solo vizualizar la linea.
  ```

* **`sns.pairplot()`** : (Gráfico_Matriz) "Parcela" Un diagrama de pares traza una relación por pares en un conjunto de datos. La función ``pairplot`` crea una cuadrícula de ejes de modo que cada variable en los datos se compartirá en el eje y en una sola fila y en el eje x en una sola columna. Eso crea parcelas
```python
        cuadricula = sns.pairplot(data= iris_df, vars=iris_df.columns[:4],hue="Especie")
#vars = (lista de nombres de variables)Son en total 4 columnas del df. (data + target)
#huw = Agrupa la gráfica por la columna que le indiquemos.
```
___
### Métodos:
___
### Propiedad:
___
### Parámetro:
* **`,hue=`** : "matiz" (opcional), Este parámetro toma el nombre de la columna para la codificación de colores.
* **`,palette=`** : Cambia los colores del gráfico. Ejm: "cool"

## [10 plotly ("*px*")](#indice)

### CUFFLINKS
[Fuente y guia de uso Git](https://github.com/santosjorge/cufflinks)<br>
[cufflinks.tools](https://jpoles1.github.io/cufflinks/html/cufflinks.tools.html)

Es una librería que da una representación gráfica de nuestros datos de manera interactiva y dinámica <br>
*pip install plotly*

También necesitaremos un librería llamada `cufflinks` que va a hacer de intermediario entre `pandas` y `plotly` el cual va a hacer mas fácil de escribir y leer el código cuando grafiquemos nuestras visualizaciones.
*pip install cufflinks*

Usando **Shift** sin soltar podremos desplazarnos dentro de la grafica, y dando doble click derecho regresaremos a la visualizacion inicial o home.

Se importa de la siguiente manera para hacer gráficas con pandas:
```python
    import pandas as pd
    import cufflinks as cf
    from IPython.display import display,HTML

    cf.set_config_file(sharing="public", theme="space", offline="True")

    #para ver todos los temas disponibles para las gráficas usamos: cf.getThemes()
    #space da color oscuro.

    setattr(plotly.offline, "__PLOTLY_OFFLINE_INITIALIZED", True)
    #Esto soluciona el error de attributeerror: module 'plotly.offline' has no attribute '__plotly_offline_initialized'

```

### Funciones:

* **`help(df.iplot)`** : Nos arrojara todos los metodos y parametros que podemos usar con la version actual: help(df.iplot), help(cf.iplot), help(cf.subplots), help(cf.scatter_matrix). [cufflinks.tools](https://jpoles1.github.io/cufflinks/html/cufflinks.tools.html)
* **`cf.go_offline()`** : Gráficos offline. Para que no redireccione a la pagina de ``plotly``.
* **`cf.datagen. ... ()`** : Genera DataFrames preparados para el tipo de "kind" de gráfica con valores aleatorios que querramos, dentro del ``( )`` indicamos cuantas DataFrames queremos generar. Para graficarlar usamos ``.iplot()``. Examinando como están armados los DF podemos guiarnos para las gráficas.
  ```python
    cf.datagen.lines(1)
    cf.datagen.histogram(4)
    cf.datagen.bubble() #Para esta grafica se toma minimo 4 variables. (x=,y=,size=,categoria=)
  ```

* **`cf.subplot()`** : Grafica un conjunto de graficas previamente enlistadas y concatenadas entre **``[ ]``** (Suma de ``figure()``).<br>
Se usa ``figure()`` en vez de `iplot()` para cargar la grafica e imprimir la suma de estas con un ``cf.iplot(grafica)``.
Una comprension de listas no es mas que una concatenacion de listas: [""]+[""]


    ```python
    #Ejemplo de Boxplot. Tiene que usarse .figure() para que se pueda hacer el "merge".
      figs = [df[df['Col_Agrupar']=="A"][['Col1','Col2']].figure(kind='box',asFigure=True),
          df[df['Col_Agrupar']=="B"][['Col1','Col2']].figure(kind='box',asFigure=True)]

      subplots = cf.subplots(figs, subplot_titles=['title_1'],vertical_spacing=.20,horizontal_spacing=.05)

      cf.iplot(subplots)

      #cf.iplot(cf.tools.merge_figures(figs)) #Unir las graficas en una sola
      #Con Asfigure comparten el Eje Y las graficas


      # Con figure (Multigraficas independientes)
      #Ponemos cada una de las gráficas -> df.figure(kind=,) dentro de un cf.subplots().
      figs = [df1.figure(kind='scatter'),df2.figure(kind='hist'),df3.figure(kind='box'),
                          df4.figure(kind='barh')]

      subplot = cf.subplots(figs, subplot_titles=['Bar 1','Bar 2','Bar 3','Bar 4'],
                                  vertical_spacing=.20,horizontal_spacing=.05)

      #subplot es un diccionario donde se divide "data" y "layout", en este ultimo configuramos: Formato y Titulo
      subplot['layout'].update(showlegend=True, title='Subplots Graphics Comparasion', width=1000, height=500,
                              font=dict(color='#ffffff'))

      #Imprimimos la gráfica.
      cf.iplot(subplot)
    ```
  Para el caso de unir varios **figure** en una sola grafica:

    ```python
      data = pd.DataFrame({'FPR': fp_roc, 'TPR': tp_roc, 'Umbrales': threshold}) #Creo el df de las 3 variables.
      df_linea = pd.DataFrame({'Linea': [0, 1],}) #creo una linea que ira del (0,0) al (1,1)

      #Enlistamos las figuras de "data" y "df_linea" con sus parámetros
      figs = [data.figure(kind='line', x=["FPR"], y=["TPR"]),
              df_linea.figure(kind='scatter', mode="lines+markers",color="Blue",dash="dash",width=4)]

      #Encerramos la union de las graficas en un subplot para luego poner el titulo.
      subplot = cf.subplots(cf.tools.merge_figures(figs))

      subplot['layout'].update(title='ROC Curve : FPR vs TPR',showlegend=True)

      cf.iplot(subplot)


    ```

* **`cufflinks.to_df(Figure)`** : Extraerá de un gráfico ``plotly`` alojado en una variable, el dataframe que contiene el gráfico.

### Métodos:

* **`df.figure()`** : Sirve para hacer multi-gráficas de grafías independientes en una sola gráfica.

* **`df.iplot()`** : Armara la gráfica según los parámetros. Considerar que para el eje **X** seran los **Index** del df.
  * **`Gráfico de Lineas`** : Se suele utilizar cuando hay tiempos involucrados, con el fin de ver la evolución de los tiempo (Fechas).

  ```python
      df_population.iplot(kind="line",mode='lines+markers', dash="dot",width=2, x="contry", y=["Population_A","Population_B"], title="Year vs Population" ,xTitle="Country", yTitle="Population",fill=True,,error_y=5, error_type='percent', legend=True, xrange=[0,10],yrange=[0,1])

      # ,x="contry" : Tomara la columna "contry" como index para situarlo en el eje "x".
      # ,y="contry" : Tomara las columna "Population_A","Population_B" como en el eje "y" cruzando sus valores.
      # ,mode='lines+markers' : Genera puntos con los datos a lo largo de la linea
      # ,dash='solid' : Tipo de linea, por defecto esta en "solid", pero hay: "dash", "dashdot", "dot".
      # ,width=2 : Modifica el **grosor** de la linea.
      # xrange=[],yrange=[] : Configuramos el rango de los ejes "x", "y".
  ```
  * **`Gráfico de Lineas Diferencial`** : Este es un Gráfico de Lineas normal con la adición que en el pie del gráfico se añade un gráfica que muestra la diferencia entre las gráficas bien sea en una diferencia numérica (`spread`) o proporcional (`ratio`). Esta mas orientado a la comparación entre 2 campos.

  ```python
      #Diferencia en Valor Numérico.
      df_population[["United States","Brazil"]].iplot(kind='spread',mode="lines+markers",size=10, title="Diferencia Numérica") #colocara a  [["United States","Brazil"]] en valores.

      #Diferencia en Proporción.
      df_population[["United States","Brazil"]].iplot(kind='ratio',mode="lines+markers",size=10, title="Diferencia Numérica")

  ```

  * **`Gráfico de Dispersión (ScatterPlot)`** : Se suele utilizar para hacer comparación entre 2 variables.

  ```python
      df_population.iplot(kind="scatter", mode="markers", xTitle="Year", yTitle="Population", title="Year vs Population",
                                size=12,symbol=['x-open',"circle-open","square-open","diamond-open","triangle-up-open"]))

      # scatter o line da gráfica de lineas por ello usamos mode="markers para forzar los puntos de dispersion.
      # size=12 : Tamaño de los puntos.

      #Para exploración de datos "Scatter" se dimensiona la gráfica en un cuadrado.

      data2.iplot(kind='scatter', x='TotalBsmtSF', y='SalePrice', mode='markers', size=9, xTitle='TotalBsmtSF', yTitle='SalePrice',
                  title='TotalBsmtSF vs SalePrice', bestfit=True, bestfit_colors=["red"], dimensions=[700,700], legend=False)

  ```
  * **`Gráfico de Barras (BarPlot)`** : Se puede usar para casi todo tipo de "kind" de contexto.<br>

    * **Barras verticales** se suele usar cuando las etiquetas caben.
    * **Barras Horizontales** se usan cuando el titulo no cabe (es grande). También cuando queremos presentar elementos de un conjunto. Ejm: ventas por Categoria

  ```python
      #Gráfica de Barras Verticales
      df_population_2020.iplot(kind="bar", title="Year vs Population" ,xTitle="Countries", yTitle="Population", colors="red")

      # ,kind="barh" = Barras Horizontales.
      # ,bargap = .2 : Afecta la anchura de las barras.
      # ,barmode="stack" : Crea barras segmentadas por categorías y las va sumando unas con otras poniendo a las mas pequeñas en el tope, es un acumulado. "overlay" : Similar a "stacks" pero no las suma sino da su valor tal cual (comparativo con otros) las presenta ordenada según estén las categorías en la leyenda.
  ```
  * **`Gráfico de Cajas (BoxPlot)`** : Es usado para encontrar los valores estadísticos como la media, mediana los qurtines (Q1,Q3) Q1 se refieren menor igual al 75% de datos y Q3 al 25%.

  ```python
      df_population.iplot(kind="box",boxpoints="all", title="Year vs Population",dimensions=[1200,500])

      #boxpoints="all" : Muestra la concentración de datos en forma scatter a lado del boxplot.

      #Sacando 2 columnas del df, para ordenar por columna - valor y saque un boxplot por calidad.
      data3.pivot(columns="OverallQual",values="SalePrice").iplot(kind="box",
             title="OverallQual vs SalePrice", dimensions=[1100,600]) #no muestra etiqueta X ni Y.

  ```
  * **`Gráfico Histograma (Histogram)`** : Sirve para ver la Distribución de data Numérica. Para ejecutar este gráfico es necesario tener los campos (incluido el index) ordenado por columnas y solo tener valores en las filas.

  ```python
      df_population[["United States","Indonesia"]].iplot(kind="hist",title="Year vs Population",xTitle="Countries", yTitle="Population",orientation='v',bins=0, subplots=False,sortbars=False, barmode="overlay", histfunc="sum")

      # ,barmode="stack" : Da el Acumulado; "overlay" (por defecto) da su valor tal cual para comparación. "group" Agrupa por pares de barras.
      # ,bins=50  : Aumentara el número de barras (sample) para el gráfico.
      # ,orientation='h' : Hará un histograma con barras Horizontales.
      # ,histnorm= "percent", "probability", "density", "probability density". Establece el tipo de "kind" de normalización para una traza de histograma. Por defecto la altura de cada barra muestra la frecuencia de ocurrencia, es decir, el número de veces que se encontró este valor en el recipiente correspondiente
      #,histfunc= "count", "sum", "avg", "min", "max". Establece la función de agrupación utilizada para un seguimiento de histograma.

  ```

  * **`Gráfico de Pie (PieChart)`** : Se puede usar esta gráfica hasta con 4 variables. No es muy usada, es mas visual. Es necesario usar un index numérico de no tenerlo reseteamos el index con `reset_index(inplace=True)`

  ```python
      df_population.reset_index(inplace=True)

      df_population_2020.iplot(kind="pie",labels="country", values="2020", title="Population 2020")
  ```

  * **`Gráfico Mapa de Calor (Heatmap)`** : Mapa de calor.

  ```python
      df.corr().iplot(kind="heatmap",title="Correlation Matrix",colorscale="reds",dimensions=[1200,1000])
      #columns vs columns; Toma una escala de colores automatica con respecto a la data hallada.
      #,center_scale= 0  : Indicamos el valor central de la escala de colores con su valor numerico.
      #,zmin= -1 ,zmax= 1 : Indicamos el valor minimo y maximo de la escala de colores con su valor numerico.

  ```
  * **`Scatter_matrix (Heatmap)`** : Muestra un Scatter Matrix muy sencillo. En la diagonal arroja un histograma de cada Serie.
    ```python
        df.scatter_matrix(bins=10, color='grey', size=2)

        df[['Age', 'Sex', 'BP']].scatter_matrix()
    ```

* **`Multi-Gráficas`** : Subplot's varias gráficas.

  ```python
    # Creación de 4 DataFrames.
    df1=cf.datagen.heatmap()
    df2=cf.datagen.heatmap()
    df3=cf.datagen.heatmap()
    df4=cf.datagen.heatmap()

    #Ponemos cada una de las gráficas -> df.figure(kind=,) dentro de un cf.subplots().
    figs = cf.subplots([df1.figure(kind='bar',),df2.figure(kind='bar'),df3.figure(kind='bar'),df4.figure(kind='bar')],
                  subplot_titles=['Bar 1','Bar 2','Bar 3','Bar 4'],
                  vertical_spacing=.20,horizontal_spacing=.05)

    #Formato y Titulo
    figs['layout'].update(showlegend=True, title='Subplots Graphics Comparasion', font=dict(color='#ffffff'))

    #Imprimimos la gráfica.
    cf.iplot(figs)

  ```

### Parámetros:

* **`,kind=`** : "line", "scatter", "bar", "box", "hist", "pie".
* **`,x=`** : Pondra el **Index** del df que le indiquemos como eje "X". Puede ser mas de uno en caso de tener multi-index.
* **`,y=`** : Pondra las **columnas** del df que le indiquemos como eje "Y".
* **`,dash=`** : Modifica el tipo de linea por defecto esta en "solid", pero hay: "dash", "dashdot", "dot".
* **`,width=`** : Modifica el **grosor** de la linea.
* **`,legend=False`** : Quita las Leyendas.
* **`,categories="Columna"`** :(groupby) Agrupa demanera distinct_count las columnas categoricas y las coloca en la leyenda del grafico.
* **`,colors=`** : para más de un dato en el gráfico usamos.
*
    ```python
      ,color=["blue","red","yellow"]
    ```
* **`,colorscale=`** : Escalas de color: "accent", "-accent", "blues", "-blues", "reds", "-reds", "purples","-purples", "paired", "-paired", "spectral", "-spectral", "brbg", "-brbg"

* **`,rangeslider=True`** : Añade es un desplazador de rango al pie del gráfico para desplazarse en la visualización de datos.
* **`,annotations`** : Añade una marca con respecto al index(date) , con su respecto formato de texto. Ejemplo Series de tiempo en el index.

    ```python
      ,annotations={'2015-02-02':'Market Crash', '2015-03-01':'Recovery'}, textangle=-70,fontsize=13,fontcolor='grey'
    ```

* **`,error_y=5, error_type='percent'`** : Añade un rango de error de la data graficada de +- 5%. Error_type puede ser también `error_type='continuous_percent'`.

* **`,sortbars=True`** : Ordena las gráficas de Barras manera descendente.

* **`,subplots=True`** : Crea una gráfica por cada Categoria del DataFrame, desglosa la grafica en varias mini-graficas. En el caso de tener varios gráficos de un tipo de "kind" alojadas en una variable podemos hacer muchas minigráficas.<br>
El parametro ``shape=(fila,Columna)`` indicara la forma en la que se ordenaran las graficas separadas, este solo funciona cuando `,subplots=True`.
  ```python
    cf.datagen.histogram(4).iplot(kind='histogram',subplots=True,bins=30)

    cf.datagen.lines(4).iplot(subplots=True,shape=(4,1),shared_xaxes=True,vertical_spacing=.02,fill=True)
    #son 2 mosaicos de gráficas independientes
  ```
* **`subplot_titles : bool or list`** : Podran en la cabezera de cada grafico su titulo o podemos cambiarselo usando una lista para indicar los titulos de cada grafico.

* **`,asFigure=True`** : Compilas las gráficas creadas por `subplot()` en una gran gráfica sin mezclarlas.

* **`,shape=(5,1)`** : Según el nº de gráficas o categorías que tengamos ejecutara una gráfica aparte. Hace lo mismo que ``,asFigure=True``

* **`,shared_xaxes=True, shared_yaxes=True`** : Fuerza el mostrar etiquetas en eje X & Y. Amplia la vision sea que activemos en X o Y (mejora las multigraficas subplot=True).

* **`,hline or vline`** : Inserta una linea bien en horizontal `hline` o vertical `vline` en la grafica de cualquier tipo de "kind". Sirve para intervalos de confianza en estadistica cuya aplicacion podria darse en controles de calidad por ejemplo.
  ```python
      df.iplot(kind="line",hline=[2,4],vline=['2015-02-10'])
  ```

* **`,hspan or vspan`** : Inserta un rango sombreado bien en horizontal `hspan` o vertical `vspan` en la grafica de cualquier tipo de "kind". Para dar formato a cada linea insertada es necesario Enlistar cada diccionario el cual representa una cada lineacon el fin de darle formato a la linea dibujada con los parametros: `color`, `width`, `dash`

    ```python
      #Data
      df=cf.datagen.lines(3,columns=['a','b','c'])

      cf.datagen.lines(3).iplot(kind="line", hspan=dict(y0=-1,y1=2,color='orange',fill=True,opacity=.4))

      #Tipo intervalos de confianza con media en el centro
      df.iplot(hline=[dict(y=-1,color='blue',width=4),dict(y=1,color='pink',dash='dash'),
                      dict(y=2,color='blue', width=4)])
        # width=4 : Ancho de la linea a graficar
        # dash="dash" : Indica tipo de linea de trazos "- - - -"
    ```
* **`,fill=True`** : Activa sombreado, ejemplo en el gráfico de linea se vera sombreado todo el area bajo la linea.

* **`,bestfit=True`** : Genera una regresión lineal de los datos implicados en la gráfica.

  ```python
      ,bestfit=True,bestfit_colors=['pink','blue']
      #podemos asignarle un color a la regresión. El color también es tomado por el gráfico.
  ```
* **`,dimensions=[Ancho,Alto]`** : Modificara las dimensiones de la gráfica, para temas de exploración de datos es recomendable tener una medida de figura cuadrado y quitar las leyendas. Ejm: ,dimensions=[700,700]

  ```python
      data2.iplot(kind='scatter', x='TotalBsmtSF', y='SalePrice', mode='markers', size=9, xTitle='TotalBsmtSF', yTitle='SalePrice',
                  title='TotalBsmtSF vs SalePrice', bestfit=True, bestfit_colors=["red"], dimensions=[700,700], legend=False)
  ```
* **`yrange=[min,max], xrange = [min,max]`** : Modificara la escala de la grafica tanto en Y como en X indicandole el valor minimo y maximo de la escala.

* **`,world_readable=True`** : Hace que el gráfico pueda compartirse de forma publica ¿?.

### plotly.express (px)

Es una librería parecida a `cufflinks` es de fácil uso y gráficos de alto nivel de visualización. En complemento con cufflinks para gráficos más avanzados.

[Fuente y guía Gráficos](https://plotly.com/python-api-reference/)

Se importa de la siguiente manera para hacer gráficas con pandas:
```python
    import plotly.express as px

    #Para mantener el tema oscuro usamos la erramienta io:
    import plotly.io as pio
    pio.templates.default = "plotly_dark"
```

### Gráficas

* **`px.histogram`** : Gráfica de histograma muy personalizable. Cufflinks resumen el personalizar todo este código.

```python
  df = px.data.tips() # Data de plotly

  fig = px.histogram(df, x="day", y="total_bill", color="sex",
            title="Receipts by Payer Gender and Day of Week vs Target",
            width=800, height=600,
            labels={"sex": "Payer Gender",  "day": "Day of Week", "total_bill": "Receipts"}, #Cambiamos el nombre de las columnas solo para el grafico.
            category_orders={"day": ["Thur", "Fri", "Sat", "Sun"], "sex": ["Male", "Female"]}, #Indicamos el orden a presentar los datos.
            color_discrete_map={"Male": "RebeccaPurple", "Female": "MediumPurple"},
            template="plotly_dark"
            )

  # Añadir el símbolo de $ a los números del Eje "Y". showgrid=True ya viene por defecto en True
  fig.update_yaxes(tickprefix="$", showgrid=True)

  # Fuente de texto, y legenda centrado dandole el X y Y.
  fig.update_layout(font_family="Rockwell",legend=dict(title=None, orientation="h", y=1, yanchor="bottom", x=0.5, xanchor="center"))

  # Añade la linea horizontal dandole las coordenadas X y Y.
  fig.add_shape(type="line", x0=0, x1=1, xref="paper", y0=950, y1=950, yref="y",
             line_color="salmon", line_width=3, opacity=1, line_dash="dot",)

  # Añade un texto en la posición en el elemento "x" y posición "y" indicada
  fig.add_annotation(text="below target!", x="Fri", y=400, arrowhead=1, showarrow=True) #arrowhead= tipo de flecha; showarrow=Mostrar

  fig.show()

```

* **`px.scatter_matrix`** : Gráfica la correlación entre las propias columnas del DataFrame, no requiere preparación de la data.
```python
  columns = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']

  fig = px.scatter_matrix(df[columns],title="Scatter Matrix",height=1200,width=1200)

  fig.show()
```

### Parámetros:

* **`,height=1200, width=1200`** : Modificara las dimensiones de la gráfica como lo haría ``,dimensions=[Ancho,Alto]`` en cufflinks, para temas de exploración de datos es recomendable tener una medida de figura cuadrado y quitar las leyendas. Ejm: ,dimensions=[700,700].

* **`,template="plotly_dark"`** : Modificara el color del tema de fondo de los gráficos.

## [11. datetime ("*dt*")](#indice)
Este modulo nos permite crear fechas ya sea por rango o partes especficias de fecha y tiempo.<br><br>

**from datetime import date, datetime, timedelta**

### Clase:

* **`date`**: Esta clase representa una fecha (año, mes y día). Proporciona varios métodos y atributos para trabajar con fechas. Por ejemplo, puedes usar date.today() para obtener la fecha actual, y los atributos year, month y day para obtener el año, el mes y el día de un objeto date, respectivamente:
  ```python
    fecha = date(2022, 12, 31)
    fecha_hoy = date.today()

    #Atributos:
    fecha.year()
    fecha.month()
    fecha.day()

  ```

* **`datetime`**: Representa un punto en el tiempo, incluyendo la fecha y la hora. Proporciona varios métodos y atributos para trabajar con fechas y horas. Por ejemplo, puedes usar datetime.now() para obtener la fecha y hora actuales, y los atributos year, month, day, hour, minute, second y microsecond para obtener el año, el mes, el día, la hora, el minuto, el segundo y el microsegundo de un objeto datetime, respectivamente.

  ```python
    fecha = datetime(2022, 12, 31, 13, 45)
    fecha_hoy = datetime.now()

    #Atributos:
    fecha.year()
    fecha.month()
    fecha.day()
    fecha.hour()
    fecha.minute()
    fecha.second()
    fecha.microsecond()

  ```
* **`timedelta`**: Representa una duración, es decir, una diferencia entre dos puntos en el tiempo. Un objeto timedelta puede ser positivo o negativo y puede representar una duración de cualquier longitud. Puedes usar objetos timedelta para realizar cálculos con objetos date y datetime. Por ejemplo, puedes sumar o restar un timedelta a un date o datetime para obtener una nueva fecha o fecha y hora.

  ```python
    #Restamos 5 dias a una fecha date.
    fecha_modificada = date(2022, 12, 31) - timedelta(days=5)

    #parametros
    ,days=
    ,seconds=
    ,microseconds=
    ,milliseconds=
    ,minutes=
  ,hours=
    ,weeks=

  ```

## [12. selenium](#indice)

Esta Libreria permite poder hacer "**Web Scraping**" usando el navegador de desarrollo de chrome, por lo que a diferencia de otras librerias como BeautifulSoup o Scrapy, esta permite interactuar con codigo **JavaScript** o tablas dinamicas que se van cargando datos a medida que interactuamos con la pagina, encambio las anteriores se suelen usar para paginas **HTML Estaticas**.<br>

Para ello es necesario descargar el **ChromeDriver** (es un ejecutable ) para que python pueda conectarse por chrome. [ChomeDriver](https://chromedriver.chromium.org/downloads).<br><br>

[Descargar_ChomeDriver_Actual](https://googlechromelabs.github.io/chrome-for-testing/)<br>
[Agent_User_GitHub](https://gist.github.com/gpolaert/b2d28784fa50d15f029b8f290f5cb839)

Esta libreria tambien puede operar en paginas web desarrolladas con **JavaScript**.<br>
Considerar que esta libreria no es tan rapida a la hora de operar como si lo es la libreria **`Scrapy`**.

*pip install selenium*
*pip install ChromeDriverManager* #permite usar el webdriver actualizado automaticamente

```python

from selenium import webdriver
from selenium.webdriver.common.by import By #Para especificar el elemento que le pasamos
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import NoSuchElementException #Para el try except NoSuchElementException: pass

from webdriver_manager.chrome import ChromeDriverManager #permite usar el webdriver actualizado automaticamente

En caso no quieras que se muestre la ventana emergente del webdriver:

from selenium.webdriver.chrome.options import Options
opts = Options()
opts.add_argument("--headless")  # Modo headless activado (hace que el chrome se ejecute en segundo plano)
# opts.add_argument("Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; Touch; LCJB; rv:11.0) like Gecko") #Osulta nombre de buscador


```
___
### Módulo:
Importamos el archivo "**chomedriver.exe**"

```python
    #caso clasico
    driver = webdriver.Chrome("C:/Users/Foster-PC/Downloads/Instaladores/Data Science/ChromeDriver (Web Scraping)/chromedriver.exe", options=opts)

    #Con ChromeDriveManager instalando la ultima version en automatico
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=opts)
    driver.get("https://senatiempresas.com/") #ingresamos a la pagina
```
___
### Métodos:
Se divide en 2 a la hora de referenciar un elemento en la pagina: `.find_element` y `.find_elements`, traera 1 o mas resultados segun el que usemos.

* **``get.("URL")``** : Cargará la página en el **"chromedriver.exe"**.
* **``driver.quit()``** : Cerrará el **"chromedriver.exe"**.

* **``driver.find_element(By.Tipo, 'path')``** : Buscara un elemento, es neceario especificar el tipo de elemento que le estamos pasando con "**By.XPATH**": Tipos de `By`:

* **``driver.find_elements(By.Tipo, 'path')``** : Buscara varios elementos, devuelve una lista de elementos ([WebElement, ...]) es neceario especificar el tipo de elemento que le estamos pasando con "**By.XPATH**": Tipos de `By`:

  * **``By.ID``**: busca por "element_id". Ejemplo `driver.find_element(By.ID, "element_id")`
  * **``By.NAME``**: busca por "element_name". Ejemplo `driver.find_element(By.NAME, "element_name")`
  * **``By.CLASS_NAME``**: busca por "class_name". Ejemplo `driver.find_element(By.CLASS_NAME, "class_name")`
  * **``By.TAG_NAME``**: busca por "div". Ejemplo `driver.find_element(By.TAG_NAME, "div")`
  * **``By.LINK_TEXT``**: busca por "Texto completo del enlace". Ejemplo `driver.find_element(By.LINK_TEXT, "Texto completo del enlace").click()`
  * **``By.PARTIAL_LINK_TEXT``**: busca por "Parte del texto". Ejemplo `driver.find_element(By.PARTIAL_LINK_TEXT, "Parte del texto")`
  * **``By.CSS_SELECTOR``**: busca por id="nombre_id" elemento. Ejemplo `driver.find_element(By.CSS_SELECTOR, "nombre_id li")`
  * **``By.XPATH``**: busca por Path"//div[@id='element_id']". Ejemplo `driver.find_element(By.XPATH, "//div[@id='element_id']")`


```python
          ('//tag[@atributo="Valor"]')
```
* **``driver.find_element_by_class_name()``** : Buscara elementos por su tag "**Class**".
* **``driver.find_element_by_id()``** : Buscara elementos por su tag "**id**".
* **``driver.find_element_by_tag_name()``** : Buscara elementos por su tag. Ejm: "**tr**" (Table Rows).
* **``...click()``** : Ejecuta un click en el elemento previamente encontrado.
___

### Busqueda en el navegador:

Para buscar cadenas de coincidencia en el navegador, entramos en modo desarrollador con F12.

Usamos en la consola el siguiente codigo para buscar entre las etiquetas y en sus atributos de estas etiquetas, los atributos siempre tienen un nombre ejemplo: **@class="Nombre"**<br><br>
  ```
  $x('Tag/etiqueta/div/ol/il/[@atributo="nombre"]/text()').map(x=>x.wholeText)

  $x('//div[@class="card-body"]/il[2]/text()').map(x=>x.wholeText)

  ```

### Propiedades:
Las propiedades en python trabajan con **[ ]**.
* ``.page_source`` = Muestra todo el codigo `html` de la página en formato de texto sin respetar las indentaciones. Podemos asignar esto a una variable para posteriormente trabajarlo en BeautifulSoup.
___
Para Seleccionar Listas desplegables es necesario la clase **`Select`**:

```python
    from selenium.webdriver.support.ui import Select
```
```python
    box = driver.find_element_by_class_name("panel-body")
    #En este caso el atributo es un "id" usamos "element_by_id"
    dropdown = Select(box.find_element_by_id("country")) #Encontrar 1 solo Element.
    #selecciona segun el texto que aparece en la lista despegable.
    #tambien puede ser por indice con "select_by_index("0")" Contando desde el "0"
    dropdown.select_by_visible_text("Spain") # 1 solo Element.
```
___
Para no tener errores con respecto a los tiempos de carga de la pagina tras las acciones que realizamos usamos:

```python
    import time

    time.sleep(5)
    #Con esto nos aseguramos que despues del click duerma 5 segundos hasta
    #recibir el proximo comando.
```

## [13. BeautifulSoup ("*bs*")](#indice)
Esta Libreria permite interpretar los datos y leer el código fuente de los datos extraido de la pagina web.<br>
**``BeautifulSoup``** parsea el codigo extraido del html en formato de texto, por lo tanto siempre nos devolvera el html en texto, por ello si queremos interactuar con el codigo html se sugiere usar la libreria **`request`**.<br>
Considerar que esta libreria no opera con paginas web desarrolladas con **JavaScript**<br><br>
*pip install beautifulsoup4*

* A la hora de extraer elementos de los HTML es importante respetar la jerarquía de estos:

  1. ID
  2. Class name
  3. Tag name, CSS Selector
  4. Xpath

```python
from bs4 import BeautifulSoup as bs
```
___
### Clase:
* **`bs()`** : Pondremos dentro del paréntesis la **variable** donde previamente tenemos asignado el código fuente de la página web.
Es necesario usarlo con `Request` para que extraiga el URL en formato de texto, junto con el uso de un **Parse** ("lxml").
```python
        soup = bs(requests.get("URL").text, 'lxml')

```
___
### Métodos:

* **`.find_all()`** : Buscará y Traerá más de un elemento de la web, todo lo que le indiquemos.Para traer todos los links de la página usamos ("a"). En html asi se llama a los links con "a".<br>
*("tipo de "kind" de elemento, atributo={"":""}<-"class=postingCardInfo")*
```python
        soup.find_all("div", attrs={"class":"postingCardInfo"})
```
* **`.find()`** : Buscará y Traerá de la web un elemento que le especifiquemos. (Una linea de código).
* **`.prettify()`** : Muestra el código HTML de manera mas legible respetando sus Indentaciones.
* **`.get_text()`** : Convertirá el resultado a formato de texto (string). Modifica a la variable contenedora. Hace lo mismo que la Propiedad "`.text`".

 ```python
        .get_text(strip=True, separator=' ')
        #Elimina los saltos de linea y separa el texto en espacios en blanco.
 ```
* **`.find_next_sibling(Tag)`** : Se usa cuando no hay una referencia (nodo) o ancla que anteceda a nuestro nodo. Buscará un nodo  teniendo como antecesor a un nodo hermano al mismo nivel dentro del código HTML. Para ello necesitamos un ancla a definir de la siguiente manera:

 ```python
        ancla = soup.find('div', id='product_description')
        parrafo = ancla.find_next_sibling('p').get_text(strip=True)

 ```
___
### Propiedad:
* **`.text`**: Convertirá el resultado a formato de texto (string). Modifica a la variable contenedora.

___
### Tags:
* **`.title`**: Mostrar Titulo de la pagina web.


## [14. scrapy ("*scp*")](#indice)
Esta no es una libreria, es un **FrameWork** el cual es la herramienta más completa para **WebScraping**. Es más rapida debido que puede ejecutar solicitudes a paginas web en paralelo (spiders).

Esta libreria puede operar con paginas web desarrolladas con **JavaScript**.
Considerar que requiere un curva de aprendizaje. <br><br>
*pip install scrapy* <br>

```python
import scrapy
```
___
### HTML:
Es un lenguaje de marcado, define la estructura y el significado de una pagina web.
Para indagar elementos en la Página Web podemos usar **Ctrl + f**.
Ciertos elementos claves para indagar en el código HTML.<br><br>

Tiene la siguiente forma:<br>
    ``<(tag) (atributo)=(valor atributo) > (contenido) <(tag)>``

```html
          <h1 class="title"> Titanic (1997) </h1>
```
* Tags:
  * **``<head>``** : Cabeza del html.
  * **``<body>``** : cuerpo del html.
  * **``<header>``** : Contiene una introduccion al contenido y suele ir en la parte superior del cuerpo.
  * **``<article>``** : Contiene articulos.
  * **``<button>``** : Refiere a un boton en la pagina para hacer "**click**"
  * **``<p>``** : Contiene parrafos de articulos, noticias, etc.
  * **``<h1>``** : Contiene al titulo del html.
  * **``<div>``** : Se refiere a divisor. Es un contenedor genérico.
  * **``<nav>``** : Se refiere a navegación. Contiene a la barra de paginación Ejm: pag 1,2,3,...99.
  * **``<li>``** : Contiene elementos de una lista.
  * **``<a>``** : Se refiere a ancla (anchor: enlace). Este contiene al atributo `href` que son enlaces.
  ```html
          <a href="http://example.com"> Texto </a>
  ```
  * **``<table>``** : Representa a una tabla que contiene filas y columnas con la data para ser extraida. Esta contiene 2 elementos:
    * **``<tr>``** : Table row. Son las filas de la tabla.
      * **``<td>``** : Table Data. Son los datos de la tabla.

  * **``<ul>``** : Representa a una lista desordenada.
  * **``<iframe>``** : Permite colocar una página dentro de otra página. Hace complicado hacer Web Scraping debido a que tenemos que cambiar de un `<iframe>` a otro, no es muy común encontrar este tag.
<br><br>
* Formato de Búsqueda:
Contiene la siguiente leyenda para filtrar la informacion:

  * **``/``** = Se refiere a un nodo Ejm: <"div">; <"ul">; <"label">; va de uno en uno cuando ruteamos ('/head/h1').
  * **``//``** = Indica saltar nodos hasta llegar a la coincidencia.
  * **``[]``** = busca un elemento dentro del nodo indicado.
  * **``@``** = indica atributo como "Class" que esta igualado a algo.

En consola de html:
  ```html
          $x('//article[@class="Etiquetas"]//h3/a["ejemplo"]/text()').map(x=>x.wholeText)
  ```
Para obtener las direcciones href filtramos del siguiente modo:
Usamos el atributo al final /@href y en map buscamos por valor (value).
___
  ```html
          $x('//ul[@class="nav nav-list"]/li/ul/li/a/@href').map(x=>x.value)
  ```
* Estado de Respuestas:
Los códigos de estado de respuesta del HTTP indican si se ha completado satisfactoriamente una solicitud HTTP. Se agrupan en 5 clases.

  1. Respuestas Informativas: (100-199)
  2. Respuestas Satisfactoria: (200-299)
  3. Redirecciones: (300-399)
  4. Error del Cliente: (400-499) puede el servidor negar la solicitud.
  5. Error del Servidor: (500-599)



### Terminal:
Escribimos `scrapy` en el terminal mostrara sus comandos. Para ejecutar cada uno de ellos es necesario escribir: **``scrapy commando [option] [args]``**

* Comandos:
  * **``bench:``** Hace un Bench para probar el rendimiento de scrapy.
  * **``fetch:``** Obtiene el marcado HTML de una página Web.
  * **``genspider:``** Crea un nuevo spider dentro de una plantilla predefinida.
  * **``runspider:``**
  * **``settings:``** Muestra la configuracion por defecto de scrapy.
  * **``shell:``** Es un entorno de pruebas para testear nuestro código sin ejecutar a nuestro spider o crear un nuevo Spider.
  * **``startproject:``** Crea un nuevo proyecto en scrapy con sus carpetas necesarias. Ejm: ´scrapy startproject nombre_del_archivo´.
  * **``version:``** Indica la version de scrapy.
  * **``view:``**
<br><br>

* Objetos:
  * **`response`** : Sirve para encontrar elementos (es como un Soup). Este solo puede utilizar los métodos: **``.xpath()``** y **``.css()``** para encontrar elementos den el HTML.
  * **`yield`** : Sirve para devolver valores como lo hace `return`.
<br><br>

* Métodos:
  * **``.xpath()``** : Busca elementos dentro del HTML. Entiende el siguiente parametro.
  ```
          response.xpath('//tag[@atributo="Valor"]')
  ```
  * **``.css()``**   :
  * **``.get()``**   : Obtendra un elemento. Ya sea de la ruta del ``.xpath()`` o ``.css()``
  * **``.getall()``**   : Obtendra una lista con los elementos. Para ellos necesitaremos tener de antemano una **lista vacia** para llenarla con un ciclo `for`.
<br><br>

* Propiedades:
  * **``.body``** : Muestra el codigo HTML previamente extraido.
<br><br>

* Parámetro:
  * **``/text()``** : Este parametro seleccion solo el texto de un nodo de codigo HTML. Para extraerlo usaremos `.get()` o `.getall()` según sea conveniente.
  ```
          response.xpath('//h1/text()').get()
  ```
___

### * startproject:
Para crear un nuevo proyecto escribiremos lo siguiente en el terminal:
```
          scrapy startproject nombre_del_archivo
```

Creara una carpeta que contendrá los siguientes archivos:

- **``scrapy.cfg``** : Este archivo correrá comandos de scrapy. Ejecutara lo que hallamos escrito dentro de los demas archivos "**py**" incluyendo el **spider**.
- **``items.py``** : Ayuda a estructurar mejor la data que extraemos. Puede ser reemplazada con la palabra clave "``yield``" para devolver elementos de la página según su estructura por defecto.
- **``middleware.py``** : Podemos añadir funcionalidades personalizadas para procesar las solicitudes y respuestas. Contiene un "spider-middleware" y un "downloader-middleware".
- **``pipelines.py``** : Almacena la data que extraemos en una base de datos (MongoDB, SQlite).
- **``settings.py``** : Podemos añadir configuraciones extras a nuestro proyecto.

Aparecera en consola 2 lineas una para situarse en el directorio que se creo la carpeta que nombramos, y la otra linea es para crear el spider

```python
    cd scrapy_tutorial #situa en la carpeta
    scrapy genspider example example.com #aqui pegamos nuestro URL de esta manera:
```
`scrapy genspider nombre_spider www.abc.com/halo` (Sin "http" y sin "/" al final).

Esto creará nuestro spider que nombramos dentro de una carpeta **spiders**.
Los **spiders** se dividen en 2 tipos:

  - scrapy-Spider : Se personaliza para extraer data de las webs.
  - CrawlSpider   : Este es para hacer **Clouding** sirve para seguir links.

___
### * shell:
Probamos una solicitud a la página web. Para salir del modo shell escribimos "`exit`".
```
          r = scrapy.Request(url='https://www.pegamoslink.com/')
```
Luego usamos el comando "`fetch`" para ver la respues de nuestra solicitud.
```
          fetch(r)
          response.body (verificamos que tenemos el codigo HTML)
```
___
### * Ejecutar "scrapy.cfg":

Para ejecutar nuestro codigo guardado en los archivos **``py``** escribimos en el terminal:

```
          scrapy crawl nombre_del_spider
```

Para Ejecutar y guardar en un archivo ``.csv`` o ``json`` escribimos el siguiente código:

```
          scrapy crawl nombre_del_spider -o nombre.csv.json
```
___
### Tags:



## [15. request](#indice)
Esta Librería permite hacer solicitudes a las paginas web. Es usado para la técnica de **Web Scraping**.<br><br>

```python
import request
```

___
### Clase, Funcion:
* **`()`** :
___
### Métodos:
* **`.get("URL")`** : Extrae el código HTML (crudo).
* **`.content.decode("utf-8")`** : Codifica el contenido (código HTML) en "utf-8". Codifica mejor los valores "ñ" , tildes y caracteres especiales para evitar futuros problemas.

___
### Propiedad:
* **`.status_code`** : Hace una solicitud de prueba a la página regresando el **estado de respuesta** del HTTP.

## [16. scipy (stats)](#indice)
Esta Libreria permite trabajar con regresion lineal. De esta usaremos el módulo **``stats``**<br><br>

```python
from scipy import stats
```

___
### Clase, Funcion:
* **`linregress()`**: Crea una regresion lineal de 2 variables.
  ```python
        reglin = stats.linregress(x=kchouse.AreaFt, y=kchouse["P.V"])
  ```
* **`st.ttest_1samp()`**: Prueba T de un solo objeto. Tiene un parametro llamado **`,alternative=`** que de no indicar nada significa que tomara las 2 colas, **`,alternative='greater'`** para cola derecha y **`,alternative='less'`** para cola izquierda.<br>
Nos arroja como resultado el Statistic = nivel de dignificación (alfa) y pvalue.<br>
n_significancia > p-valor = Rechaza Hipotesis nula y se Acepta H. alternativa.<br>
**st.ttest_1samp(df.colm, hipotesis_nula,alternative="")**
  ```python
    mu_0 = 170 #media, para prueba de H.
    st.ttest_1samp(df.Estatura, mu_0, alternative="")
    #,alternative= 'greater' : Cola derecha
    #,alternative= 'less' : Cola izquierda

  ```

* **`st.ttest_ind()`**: Prueba t de dos colas para dos poblaciones independientes. Esta prueba contrasta la hipótesis de que las medias de dos poblaciones, supuestas independientes, son iguales. Dicho de otra forma, contrasta la hipótesis de que la diferencia de medias es 0.<br>
Aplica lo mismo para el parametro **`,alternative=`**<br>
**st.ttest_1samp(Poblacion_1, Poblacion_2,alternative="")**
  ```python
    #¿Serán distintas las estaturas de mujeres y hombres?. Ambas colas
    st.ttest_ind(df_F.Estatura, df_M.Estatura)

    #¿Mayor la de las mujeres que la de los hombres? ,alternative= 'greater'
    #¿Mayor la de los hombres que la de las mujeres? ,alternative= 'less'
  ```

___
### Métodos:
___
### Propiedad:
* **`.slope`**: Arroja el valor de la pendiente."m"
* **`.intercept`**: Arroja el valor de la interseccion. Cuando "x es 0" Valor "b"
* **`.rvalue`**: Arroja el valor de la correlacion de los datos. Este evalua la precision de un modelo.

## [17. TextBlob ("*tb*")](#indice)
Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural** como análisis morfológico, extracción de entidades, análisis de opinión, traducción automática, análisis de sentimientos, etc.

 Esta librería ha sido entrenada con textos en ingles debido a ello está más optimizada para ese idioma más que otros.<br><br>
*pip install textblob*

Descargaremos el cuerpo de textos con el que `textblob` funciona. Esto incluye el poder separar las palabra por adjetivos , sustantivos; tambien separar frases y oraciones; separa oraciones del parrafo y tambien trae definicion de sinonimos y antonimos. Para ello necesitamos escribir en la consola o terminal.<br><br>
*ipython -m textblob.download_corpora*

Para importar TextBlob:

```python
    from textblob import TextBlob
```

Para cambiar el algoritmo por defecto **(pattern)** de analisis de sentimientos a otro algoritmo **(NaiveBayesAnalyzer)**.

```python
    from textblob.sentiments import NaiveBayesAnalyzer
    #Ejm:
    blob3 = TextBlob(texto, analyzer = NaiveBayesAnalyzer())
```

Para la **singularidad y pluralidad de palabras**; **Correccion de palabras mal escritas** importamos la libreria "**`Word`**" de "**``TextBlob``**".
```python
    from textblob import Word
```
___
### Clase, Funcion:
* **`TextBlob()`**: Crea o convierte un string a tipo de "kind" TextBlob.

___
### Métodos:

* **`detect_language()`** : Indica el tipo de "kind" de idioma del lenguaje analizado.
* **`translate(to="")`** : Traducira el texto al idioma que le especifiquemos. "es", "en", etc

#### Métodos de la Sub-Libreria Word:

* **`pluralize()`** : Arrojara el plural de la "palabra" indicada.
* **`singularize()`** : Arrojara el singular de la palabra indicada.
* **`spellcheck()`** : Sugiere mediante una lita de palabras con sus probabilidades la posible corrección de la palabra mal escrita.
* **`correct()`** : Arroja la corrección de oraciones de palabras mal escritas.
* **`word_counts()`** : Arroja la cantidad de veces que la palabra indicada es encontrada.
___
### Propiedad:

* **`.sentences`** : Arroja cada una de las oraciones hasta donde terminan con un punto inicia otra. Segmenta por oraciones.
* **`.words`** : Arroja cada una de las palabras únicas del texto. Segmenta por palabras únicas
* **`.word_counts`** : Arroja un diccionario con cada una de las palabras únicas como **Key** y la cantidad de veces que aparecen estas palabras como **Values**.
* **`.tags`** : Arroja una lista con cada palabra segmentada indicando su tipo de "kind" (Sustantivos, Pronombres, Verbos, Adjetivos, Conjunciones, Adverbios, Interjecciones, Preposiciones)
* **`.noun_phrases`** : Arroja las frases-sustantivos encontrados en el texto.

#### Propiedades de la Sub-Libreria Word:

* **`.definitions`** : Arroja el significado de la palabra en cuestion.
* **`.synsets`** : Arroja una lista de posibles sinonimos para la palabra.
___
### Parámetro:

* **`,analyzer=`** : Podemos cambiar el analizador de sentimientos a la funcion `TextBlob` . Ejm: NaiveBayesAnalyzer
```python
    blob3 = TextBlob(texto, analyzer = NaiveBayesAnalyzer())
```

## [18. nltk](#indice)
**Natural Language Toolkit** Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural**. No requiere "pip install" python ya lo trae.<br>

Para importar nltk:

```python
    import nltk
```

Para usar "**Stop Words**" descargamos el siguiente paquete para "nltk".

```python
    nltk.download("stopwords")
```
Ahora importamos el "**stopwords**"
```python
    from nltk.corpus import stopwords
```
Para segmentar por palabras y  asignar las "**stopwords**" a una variable:
```python
    stops = stopwords.words("english")
    print(stops)
    #Podemos usar idioma "spanish" o cualquier otro.
```

## [19. spacy](#indice)
Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural**.<br>
*pip install spacy*<br><br>

Para trabajar con el idioma ingles es necesario descargar por consola:<br>
*py -m spacy download en*

Para importar spacy:

```python
    import spacy
```
Cargamos un paquete de spacy en lenguaje ingles entrenado con paginas web; tambien podemos usar cualquier otro paquete por Ejm: Textos en español entrenados con noticias ("es_core_news_sm"). A nuestra variable "nlp" (Natural Language Processing):
```python
    nlp = spacy.load("en_core_web_sm")
    #cargamos el lenguaje ingles ("en_core_web_sm") a nuestra variable.
```
Asignamos un nlp con texto a una variable:
```python
    documento = nlp("TEXTO")
```
Para sacar de un texto sus entidades y tipos:
```python
    for entity in documento.ents:
        print(f"{entity.text}:{entity.label_}")
```
### Propiedades:

* **`.ents`** : Arroja las entidades halladas en el texto.
* **`.text`** : Arroja el texto  en formato de string con sus comillas ('').

## [20. sklearn](#indice)
**Scikit Learn** Es una librería para **Machine Learning**.<br>
Esta libreria ofrece un catálogo de modelos ya predefinidos en el que podremos configurar sus hiper-parámetros para ajustar más su precición. Para la mayoria de modelos son suficientes lo hiper-parámetros excepto para Redes neuronales el cual podremos usar `tensorflow.keras` para con figurar el modelo de redes a nivel de capas para una mejor personalizacion y precicion.

*pip install sklearn* (version antigua de pip)<br>
*pip install scikit-learn* (Version actual)<br>
*pip install -U scikit-learn* (Actualizar version de scikit-learn)

### > sklearn.datasets:

Son bases de datos que ya incorpora la librería de `sklearn` y las podemos cargar llamándolas de la propia librería de sklearn. Estos Datasets vienen la data repartida en 2 tomas `.data` y `.target`.

  ```python
    from sklearn.sublibreria import sublibreria

    set_config(transform_output="polars") #Arroja todos los outputs de los estimadores a formato dataframe.polars de manera global. Por Default arroja un "ndarray"

    # O podemos indicar la salida de un estimador de forma manual
    scaler = StandardScaler().set_output(transform="pandas")
    scaler.fit(X_train)
    X_test_scaled = scaler.transform(X_test)
    X_test_scaled.head() #Esta variable ya es un df

  ```


#### load_digits:
Esta librería trabaja con un dataset de reconocimiento óptico de digitos (números) escritos a mano.
Con un pixel de 8x8 y un rango de (0,16) valores diferentes por pixel. Este dataset funciona con arrays de 2 dimensiones.

Para importar esta librería:<br>
```python
    from sklearn.datasets import load_digits
```
Para ver la información de lo que hace el dataset importado:
```python
    digist = load_digits()
    print(digist.DESCR)
```
#### load_iris:

Son un dataset de 3 tipos de flores con 4 tipos de medidas para determinar que tipo de "kind" de flor pertenecen. Vienen con sus respectivas Etiquetas.

```python
    from sklearn.datasets import load_iris
```

##### Propiedades:

* **`.target`** : Son las **etiquetas** con el resultado de lo que debe arrojar el modelo. Posteriormente lo usará para corroboración con lo arrojado por el modelo.
* **`.data`** : Se refiere a toda la lista de **datos en cuestion a aprender**. En este caso son imagenes en formato array (1x64) con las que se ha entrenado el modelo para corroborar con su respectivo target (Etiqueta).
* **`.images`** : Contiene la **data** en arrays de 8x8 **listas para ser representadas** en imagenes.
* **`.shape`** : Mostrara ca cantidad de (filas,columnas), bien sea de un target, data, etc.

#### fetch_california_housing:

Dataset de precios de las casas del estado de California en 1990. Los métodos, y comandos usados en el dataset anterior funcionan para todos los datasets.

```python
    from sklearn.datasets import fetch_california_housing
```
##### Propiedades:

  * **``dataset.feature_names``** = Muestra el nombre de las columnas del dataset para train.
  * **``dataset.target_names``** = Muestra el nombre de las columnas del target para test.
___

#### from sklearn import preprocessing:

En el caso de trabajar con datos que no sean propios de `sklearn.datasets` es necesario preprocesar los valores de los datos para eliminar o controlar valores atipicos o otorgar a nuestros datos distribuciones lo cual ayudan a que el modelo entrenado no tenga sesgos por datos inapropiados.

Es modulo esta orientado a la estandarización de los datos para una correcta distribucion de los datos, con el fin de posteriormente evaluar si la data posee una distribucion normal. Tener encuenta que el output de todos estos metodos son vectores - arrays de numpy el cual debemos volver a converti a dataframe si asi lo queremos.

  * **``preprocessing.MinMaxScaler()``** : Escala en función del mínimo y máximo. Toma el valor maximo como 1 y el valo minimo como 0 y el resto de valores en proporcion entre **1 y 0**. Tener en cuenta que los valores atipicos afectan al resultado de este metodo.
  ```python
    #datos es un df con 2 columnas (ingreso,	carros)
    datos_min_max = preprocessing.MinMaxScaler().fit_transform(datos) #datos = variable de 2 Dimensiones.

    datos_min_max

    #Podemos Forzar que nos arroja valores entre -1 y 1:

    datos_min_max = preprocessing.MinMaxScaler().fit_transform(datos, feature_range=(-1, 1))

  ```
  * **``preprocessing.Normalizer()``** : No se refiere a distribucion normal sino que normaliza en función de la Norma del Vector. Con esto aseguramos de que ningun elemento tenga una magnitud mayor a la que tendria todo el elemento. Por defecto usa la normal L2.<br>
  Itera en cada valor de las columnas sacando la raiz cuadradada de la suma de cada valor al cuadrado. Al igual que `MinMaxScaler()` da como resultado valores entre **0 y 1**.<br>
  Sin embargo `.Normalizer()` no estan utilizado debido a que comprime demasiado los datos dejandolo casi lineal.
  ```python
    #Es necesario pasar las columnas a filas para ejecutar la funcion por ello lo transponemos.
    datos_normalizer = preprocessing.Normalizer().transform(datos.T)
    datos_normalizer = datos_normalizer.T #volvemos a transponer para mantener nuestro formato df.
    # normalizado = X / raíz_cuadrada( X_1^2 + X_2^2 + X_3^2 + ...)

  ```
  * **``preprocessing.StandardScaler()``** : Estandariza (desv_std = 1, media = 0). Cambia la distribucion de nuestros datos para que tengas una medio = 0 y desv= 1. Se busca tener este resultado debido a que las tecnicas de aprendizaje de maquina operan con datos que posean **distribucion normal**. Es un homogenizador aproxima la media en 0 y la desviacion entre -1 y 1 pero de manera aproximada a diferencia de ``MinMaxScaler()`` que  si fuerza que los extremos esten en -1 y 1.<br>
  Este metodo a deferencia de los 2 anteriores si nos dara un rango entre valores negativos y positivos.<br>
  Para lograr esto hay 2 metodos: ``StandardScaler()`` y ``RobustScaler()``, la diferencia radica en que RobustScaler() usa el rango intercuatilico (Q1,Q3), el cual quita los datos atipicos, por lo que hace que nuestros datos esten menos distribuidos por ellos es uno de los mas usado.<br><br>

  Otros de los usos de ``.RobustScaler()`` es usarlo junto a `.MinMaxScaler()` para que una vez teniendo una distribucion normal de nuestros datos, escalarlo de forma 0 a 1.s

  ```python
    # Ejemplo con .StandardScaler()
    datos_standard_scaler = preprocessing.StandardScaler().fit_transform(datos)
    # estandarizado = (X - media) / std

    # Ejemplo con .RobustScaler()
    datos_robust_scaler = preprocessing.RobustScaler().fit_transform(datos)
    # estandarizado = (X - rango_intercuartílico) / std

    datos_standard_scaler, datos_robust_scaler


    # Otro ejemplo con .StandardScaler()
    saleprice_scaled = StandardScaler().fit_transform(df[['SalePrice']]) #Transformamos el valor de "SalePrice" a 2D.
    saleprice_scaled #Nos arroja la distribución de los datos ya ordenado de menor (-) a mayor (+).

    low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10] #Nos arroja los 10 valores mas bajos.
    high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:] #Nos arroja los 10 valores mas altos.
    print('outer range (low) of the distribution:')
    print(low_range)
    print('\nouter range (high) of the distribution:')
    print(high_range)

  ```
  * **``preprocessing.OrdinalEncoder()``** : Sirve para Categorizar Variables Ordinales las cuales son categorizaciones que estan relacionadas a su proximidad entre ellas como: "Alto, Medio, Bajo". Convertira estos datos situados en el df a [0.0, 1.0, 2.0].<br>
  A diferencia del **OneHotEncoder** no nos categoriza en 0 y 1, es por ello que necesitamos escalarlo bien sea con Max-Min, o cualquier otra tecnica de Escalamiento para evitar ruido en el entrenamiento del modelo.<br><br>

    Diferencia con OneHotEncoder:

    * **OrdinalEncoder** : asigna valores numéricos a las categorías en función de su posición, generando una codificación ordinal. Arroja un array
    * **OneHotEncoder**  : crea columnas binarias, una por cada categoría única, donde el valor 1 indica la pertenencia a una categoría específica. Hay que usar objeto.toarray() para que nos arroja el array.

    ```python
      from sklearn.preprocessing import OrdinalEncoder

      Categorias_Servicio = ["Alto", "Medio", "Bajo"]
      Categorias_Calidad = ["Malo", "Mediocre", "Bueno"]

      #Creamos el Objeto bajo los criterios de las listas creadas.
      codificador = OrdinalEncoder(categories = [Categorias_Servicio, Categorias_Calidad],min_frequency=None ,handle_unknown="error")

      #,min_frequency=None : Indica la frecuencia minima de apariciones para que se considere una categoria minima. No la Excluye solo la categoriza como una categoria minima. Estas categorias minimas se pueden referenciar con otros parametros.
      #,handle_unknown="error" : Espicifica como se maneja las cateogiras desconocidas durante el fit.transform.
      #  "ignore" : Ignorara todas las categorias no indicadas con ``,categories=["A","B"]``
      #  "infrequent_if_exist" : Ignorara todas las categorias infrecuentes configuraro previamente con ``,min_frequency=6``

      #Podemos Crear un nuevo df o reemplazar el existente aplicando el "fit_transform" al df convirtiendo las categorias en valor numerico.
      df_categorizado = pd.DataFrame(codificador.fit_transform(df), columns=["Servicio", "Calidad"])

    ```
    * **``objeto.categories_``** : Sirve para ver la categorizacion que ha hecho.

    ```python
        codificador.categories_
    ```
  * **``preprocessing.LabelEncoder()``** : Sirve para Categorizar Variables Ordinales las cuales son categorizaciones que **NO** estan relacionadas a su proximidad entre ellas como: "Alto, Medio, Bajo". Convertira estos datos situados en el df a [0.0, 1.0, 2.0].<br>
  A diferencia del **OneHotEncoder** no nos categoriza en 0 y 1, es por ello que necesitamos escalarlo bien sea con Max-Min, o cualquier otra tecnica de Escalamiento para evitar ruido en el entrenamiento del modelo.<br><br>

      ```python
          from sklearn.preprocessing import LabelEncoder
          def label_encoder(datos_categoricos):
              le = LabelEncoder()
              df[datos_categoricos] = le.fit_transform(df[datos_categoricos]) #Sobrescribe en la columna del df la codificación.

          for i in df.describe(exclude=['category']).columns:  #Usamos solo columnas categoricas obiando las numericas. Ya es una lista
            label_encoder(i)

          df.head()
      ```
  * **``preprocessing.TargetEncoder()``** : Solo Sirve para variables categoricas. Saca el valor promedio de cada categoria unica que encuentre. Ejemplo se tiene 2 columnas una de categorias y otra de valores numericos, en base a las categorias unicas sacara el valor medio para cada categoria.

      ```python
        import numpy as np
        from sklearn.preprocessing import TargetEncoder

        #Data:
        X = np.array([["cat"] * 30 + ["dog"] * 20 + ["snake"] * 38], dtype=object).T #Ordenamos a que tenga 88 filas
        y = [90.3] * 30 + [20.4] * 20 + [21.2] * 38

        enc = TargetEncoder(random_state=0, smooth="auto") #,smooth="auto" : sensibilidad de cuantos datos tomara para promediar.
        X_trans = enc.fit_transform(X, y)

        print(np.unique(X),enc.encodings_)
        #enc.encodings_ : Arroja los valores promedio de cada categoria.

      ```

  * **``preprocessing.OneHotEncoder()``** : Sirve para Categorizar Variables Ordinales las cuales crea columnas binarias, una por cada categoría única, donde el valor 1 indica la pertenencia a una categoría específica y las demas columnas las deja en 0.<br><br>

  Diferencia con OneHotEncoder:

  * **OrdinalEncoder** : asigna valores numéricos a las categorías en función de su posición, generando una codificación ordinal. Arroja un array
  * **OneHotEncoder**  : crea columnas binarias, una por cada categoría única, donde el valor 1 indica la pertenencia a una categoría específica. Hay que usar objeto.toarray() para que nos arroja el array.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    encoder = OneHotEncoder(handle_unknown='ignore', drop='first')

    df_categorizado = pd.DataFrame(encoder.fit_transform(df))

    # handle_unknown = por defecto esta en "error" para que si encuentra una categoria extraña que no pertenece al patron arroje un error. al cambiarlo a "ignore" lo va a ignorar llenandolo de '0' y corriendo el modelo.
    # drop='first' : Elimina la primera columna que se codifica para evitar la multicolinealidad para evitar una una alta correlación entre ellas. Se suele aplica en regresion lineal o  la regresión logística.

    # Convertir el DataFrame a una matriz
    df_array["a_1", "a_2", "a_3"] = df.values

    # Aplicar la codificación one-hot
    encoded_array = encoder.fit_transform(df_array)

    # Crear un nuevo DataFrame a partir de la matriz codificada
    df_categorizado = pd.DataFrame(encoded_array.toarray())

    # Opcional: asignar nombres de columnas al DataFrame resultante
    column_names = encoder.get_feature_names_out()
    df_categorizado.columns = column_names

    ```
    * **``objeto.toarray()``** : Sirve para convertir y ver ver la matriz que ha generado en un array.
    * **``objeto.get_feature_names_out()``** : Arroja el nombre de las columnas del diccionario.


#### from sklearn.impute import SimpleImputer:

Esta Sub-libreria nos ayuda a reemplazar datos faltantes NaN con datos mas comunes de nuestra data o indicandole manualmente el texto a rellenar. Se puede aplicar a strings como datos numericos.<br><br>

La Ventaja es que aplica a todas las columnas indicadas evitando usar un ciclo for en caso lo hagamos con: ``.fillna()`` de pandas.

```python
    from sklearn.impute import SimpleImputer

    #Reemplazar NaN con valor mas comun
    imputer = SimpleIpmuter(strategy="most_frequent")
    imputer.fit_transform(x) #Aplicamos a la variable

    #,strategy = "most_frequent" (valor que más se repite), "median" (mediana), "mean" (promedio), "constant"(Texto).

    #Reemplazar NaN con valor indicado
    imputer = SimpleIpmuter(strategy="constant", fill_value="No_Encontro") #Reemplaza los NaN con "No_Encontro"
    imputer.fit_transform(x)

```
#### from sklearn.pipeline import Pipeline:

Esta libreria nos permite secuenciar varios procesos usando las herramientas de `sklearn` alojando todos los pasos en una sola variable. Se convierte en un Mix de Funciones.<br><br>

**Pipeline([("Nombramos_proceso", herramienta_sklearn(parametro="median"))])**


  ```python
    from sklearn.pipeline import Pipeline

    #Escribimos todos los procesos usando herramientas de sklearn que entrara en secuencia.

    pipeline = Pipeline([
            ("rellenar", SimpleImputer(strategy="median")), #Rellenemos los Nan con el valor median de la columna correspondiente.
            ("escalar", StandardScaler())
                        ])
    pipeline.set_output(transform="pandas") #Aseguramos que la salida nos arroje un df.
    df_pipline = pd.DataFrame(pipeline.fit_transform(df[df.describe().columns]))


  ```

#### from sklearn.compose import ColumnTransformer:

Esta libreria nos permite aplicar las funciones o Pipelines (mix de funciones) a ciertas listas (columnas de df) que queremos transformar.<br><br>

**ColumnTransformer([("Nombramos_proceso", herramienta_sklearn(parametro="median"), Nombre_Lista)])**


  ```python
    from sklearn.compose import ColumnTransformer

    #Primeramente convertimos a lista las columnas del df que vamos a transformar.
    col_a =list(df["a"])
    col_ohe =list(df["category"])

    transform = ColumnTransformer([
            ("num", pipeline, col_a), #Aplicamos la variable donde esta guardado el Pipeline
            ("Ohe", OneHotEncoder(), col_ohe) #Categorizamos
                        ])
    transform.set_output(transform="polars")
    df_limpio = pd.DataFrame(transform.fit_transform(df)) #Unira estas listas transformaras al df original. Quedaria quitar las columnas "a" y "category".


  ```

### > Estimadores (Modelos):
Son los que ejecutan las pruebas a los datasets(train, test) arrojando resultados de predicción.

#### Métodos para los modelos:

* **`model.fit()`** : Carga la data para entrena el modelo en base a la data previamente separa para el entrenamiento (train). Devuelve como resultado una expresión que indica que ha cargado el modelo para entrenar pero aún no lo ha ejecutado.<br><br>

**model.fit(X=Variables, y=lista_target)**

```python
    model.fit(X=X_train, y=Y_train) #Aprende del Train (1347 datos)
    #Asignamos el dataset train para el entrenamiento.
    #Epochs=1 Indica el numero de veces que debe procesar todo el set de datos.
```
* **`model.transform()`** : Este método ajusta y transforma los datos de entrada al mismo tiempo y convierte los puntos de datos. Si usamos ajuste y transformación separados cuando necesitamos ambos, disminuirá la eficiencia del modelo, por lo que usamos fit_transform() que hará el trabajo de ambos.
* **`model.fit_transform()`** : Ajusta el X en un espacio incrustado y devuelva esa salida transformada. Usa en los datos de **entrenamiento** para que podamos escalar los datos de entrenamiento y también aprender los parámetros de escala de esos datos. La variable que aplicaremos fit_transform() debe de estar en 2 dimensiones.<br>
A diferencia de `fit()` solo acepta un conjunto de datos que transformara (,X=X_Train), no usa target (,y=y_train). Por lo que si tenemos una columna target lo correcto seria usar primero `.fit()` y luego `.transform()`.
```python
    datos_reducidos = tsne.fit_transform(digits.data)
    #Usa la Data para entrenarse sin el Target.
```
* **`model.predict()`** : Ejecutara la predicción de nuestra data que previamente separamos para esta función

```python
    prediccion = model.predict(X=X_test)
    #predice el Test (450 datos)
```
* **`model.score()`** : Mostrará que tan preciso fue nuestra predicción (X_test) con respecto al resultado que debería haber arrojado. **model.score(X,Y)**

```python
    print(f'{model.score(X_test,Y_test):.2%}')
    #97.78%, se evaluá los arrays de imágenes (X_test) con sus targets.
```
#### 1.- sklearn.neighbors:

```python
    from sklearn.neighbors import KNeighborsClassifier
```
Esta clase usará el algoritmo de los K números mas cercanos para aprender.
Es necesario guardar la clase en una variable para usar sus métodos.

```python
    knc = KNeighborsClassifier()
```
#### 2.- sklearn.svm:

Modelo **Support Vector Machine**. Este modelo de aprendizaje Supervisado, se puede usar para **Clasificacion** como para **Regresion** (aunque para regresion no tiene diferencia con un `LinealRegresion()`) por ello se usa más para Clasificacion:

* **SVC** : (Support Vector Classifier) : Se usa para Clasificacion Binaria. Usa la Clasificacion 1 vs 1. Por lo que hay que forzar la Clasificacion Multiclase con `OneVsRestClassifier()`.

  ```python
    svm = Pipeline([
    ("escalar", StandardScaler()),
    #Haz un SVC=Support Vector Classifier
    ("svc", SVC(kernel="linear", C=1)), # Costo comp: nº elem x nº variables (mxn)
    ("svc", SVC(kernel="poly", C=5, degree=10, coef0=1)), #m2 x n, #,degree= Gredo del polinomio.
    ("svc", SVC(kernel="rbf", C=1, gamma=5)) #m3 x n Funciona mejora cuando hay muchas variables y no muchas filas.
                    ])

  ```
    * **``,Kernel="linear``** : Utilizará un kernel lineal para realizar la clasificación. Kernel Es una funcion que nos permite obtener los beneficios de mandar los datos a un espacio de mayores dimensiones sin aumentar los costos computacionales.
    no todos los modelos de aprendizaje automático tienen la noción de kernel. El uso de kernels es más común en modelos basados en Máquinas de Vectores de Soporte (SVM) y algunos algoritmos de reducción de dimensionalidad, como el PCA (Análisis de Componentes Principales).  Son principalmente utilizados en modelos que involucran clasificación o regresión basados en hiperplanos de separación
    * **``,C=1``** : Controla el equilibrio entre el ajuste perfecto a los datos de entrenamiento y la maximización del margen. Un valor más bajo de C permite un margen más amplio pero puede haber más puntos de entrenamiento clasificados incorrectamente, mientras que un valor más alto de C ajusta más los datos de entrenamiento pero puede resultar en un margen más estrecho
    * **``,probability=True``** : Nos arrojara probabilidades como output.

* **SVR** : (Support Vector Regressor) : Busca minimizar la diferencia entre los valores de salida predichos y los valores reales, permitiendo cierta tolerancia. Para poder insertarle los datos al modelo es necesario transformar los datos con StandardScaler() para que esten homogenizados entre -1 y 1 de manera aproximada.

  ```python
      #Importar SVR= Supporr Vector Regressor
      from sklearn.svm import SVR
      svm = Pipeline([
          ("escalar", StandardScaler()), # Estandarizamos los datos entre -1 y 1 de manera aproximada con StandardScaler().
          ("svr", SVR(kernel="lienar", epsilon=1.5))  #Tambien puede ser ,kernel="poly" para polimoniales.
      ])

  ```


El objetivo de un SVC lineal (clasificador de vectores de soporte) es ajustarse a los datos que proporciona, devolviendo un hiperplano de "mejor ajuste" que divide o categoriza sus datos. ... Además de los paquetes de visualización que estamos usando, solo necesitará importar svm de sklearn y numpy para la conversión de matriz.

```python
    from sklearn.svm import SVC
    svc = SVC(gamma="scale") #Por defaul fiene en "scale": se ajustará automáticamente en función de las características de los dato

    svc = SVC()
    svc.fit(x_train,y_train) (1 vs 1)

    #Ahora, utiliza el clasificador multiclase
    from sklearn.multiclass import OneVsRestClassifier

    svc = OneVsRestClassifier(SVC()) #Encerramos el modelo SVC adentro para forzar el (1 vs rest)
    svc.fit(x_train,y_train)
```
#### 3.- sklearn.naive_bayes:

  * **``GaussianNB()``** : Un algoritmo Gaussian Naive Bayes es un tipo de "kind" especial de algoritmo NB. Se usa específicamente cuando las características tienen valores continuos. También se supone que todas las características siguen una distribución gaussiana, es decir, una distribución normal

  ```python
      from sklearn.naive_bayes import GaussianNB
      GNB = GaussianNB()
  ```
#### 4.- sklearn.linear_model:


  * **``LinearRegression.fit(X="data son variables q se tendra en cuenta", y="columna target para aprender y predecir")``**

  ```python
      from sklearn.linear_model import LinearRegression
  ```
  Regresión lineal con sklearn:

    ```python
        model_rl = LinearRegression()
        model_rl.fit(X=x_train, y=y_train) #Previo "train_test_split"

        predice = model_rl.predict(y_train) #Evaluamos con y_test.
    ```
    * Propiedades:
        * **``.coef_``**       : "m" (pendiente) angulo de inclinacion.
        * **``.intercept_``**  : "b" (sesgo o bias) se mueve en eje Y.
    * Formula:
        * **y=mx+b**
    <br><br>

  * **``SGDRegressor()``** : Es un modelo de regresión lineal que utiliza el descenso de gradiente estocástico para encontrar los coeficientes óptimos y realizar predicciones basadas en una función lineal. Es la version más eficiente posible para una regresion lineal dado que puede trabajar con muchas Variables sin consumir muchos recursos computacionales. La contra es que se aproxima al resultado optimo, no es tan preciso, pero para grandes cantidades de variables es aceptable.<br>
  Los Pasos que da son  aleatorios hasta que se va a aproximando al minimo valor de error.

     ```python
      from sklearn.linear_model import SGDRegressor

      sgd = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)  #,tol= :  es el límite menor al valor de la suma de los errores al cuadrado.
      sgd.fit(x, y.ravel())  #.ravel: te genera una lista normal, quita las lista de lista en el output de los datos.ff
      sgd.intercept_, sgd.coef_   #Sacar los parámetros, la intersección y la pendiente
    ```

  * **``SGDClassifier()``** : Es un clasificador binario basado en el algoritmo de descenso de gradiente estocástico (Stochastic Gradient Descent, SGD). Una de las ventajas de SGDClassifier es su eficiencia computacional, especialmente en conjuntos de datos grandes, ya que realiza actualizaciones de parámetros en función de una sola muestra a la vez. Sin embargo, esta característica también puede hacer que el algoritmo sea sensible a la elección de hiperparámetros.<br>
  Este metodo posee el Hiperparametro de ``.decision_function()`` el cual arroja la valoracion positiva o negativa con la cual ha tomado la decicion de clasificar si algo es A o B.

     ```python
      from sklearn.linear_model import SGDClassifier
    ```

    ```python
        sgd = SGDClassifier(random_state=42) #Tiene atributo de aleatoriedad.
        sgd.fit(X=X_train, y=y_train) #Previo "train_test_split"

    ```
    * **``.decision_function(x_train)``** : arroja la valoracion positiva o negativa con la cual ha tomado la decicion de clasificar si algo es A o B.

  * **``LogisticRegression()``** : Es un clasificador binario basado en el algoritmo de Regresion logistica (no existe LogisticClassifier)<br>.
  La regresión logística se usa típicamente para la clasificación binaria, pero con la función Softmax se puede extender para admitir múltiples clases.Cuando se utiliza la función Softmax con la regresión logística, el modelo puede clasificar instancias en más de dos clases. La función Softmax asigna probabilidades a cada clase en función de las características de entrada y los parámetros aprendidos durante el entrenamiento del modelo. Esto permite que la regresión logística multinomial se utilice para la clasificación de múltiples clases en lugar de solo la clasificación binaria.

    ```python
      from sklearn.linear_model import LogisticRegression
      clf = LogisticRegression(multi_class="multinomial") # multi_class="multinomial" : Es el Multicalss usando funcion Softmax.
      clf.fit(x,y)
      y_pred = clf.predict(x)

    ```

#### 5.- sklearn.manifold :

  ```python
      from sklearn.manifold import TSNE
  ```
T-Distributed Stochastic Neighbor Embedding
```python
    tsne = TSNE(n_components=2, random_state=11)
    datos_reducidos = tsne.fit_transform(digits.data)
```
#### 6.- sklearn.cluster:

Se usa para agrupacion por grupos. Sus usos son los siguientes:

  *  **Analisis de Datos** : Dado que encuentra grupos que no habiamos encontrado.
  *  **Reduccion de Dimenciones**  : Ya que al agrupar reduce la dimensionalidad a pocos features.
  *  **Detector de Anomalias**  : Podemos tener en una agrupacion data legitima y en otra dudosa o fraudulento.

* **``KMeans()``** : Reparte los puntos de una grafica en grupos. Es un modelo simple y con bajo costo computacional.<br>
Para ello se necesita previamente que los datos esten aproximadamente circulares y los grupos tienen que ser casi del mismo tamaño. `KMeans` Necesita de manera boligada que siempre se especifique el numero de Cluster que se va agrupar.
  ```python
      from sklearn.cluster import KMeans
      kmeans=KMeans(n_clusters=4, init=ini, n_init=1)
      #,n_clusters=4 : Indicamos que genere 4 grupos de la data.
      #,ini=ini : Determinamos manualmente los centroides (opcional) : ini = np.array([[-2,8], [-1,5], [4,-4], [5,-10]])
      #,n_init=1  : Numero de vueltas (corridas) que dará.
      y_pred=kmeans.fit_predict(x)

      #Visualizar los Centros de gravedad de los Clusters
      z=kmeans.cluster_centers_
      z

      #Visualizar el score que nos ha arrojado. Este score desminuye mas mientras mas Clusters aumentemos y no siempre esto es lo optimo. Por lo que tenemos que interpretar cuantos Cluster son los necesarios o calcular el codo donde ya no hay mas mejora.
      kmeans.inertia_ #Mientas mas cercano a 0 mas preciso.
      kmeans.score(x) #Da lo mismo pero en Negativo. Mientas mas cercano a 0 mas preciso.
  ```
* **``MiniBatchKMeans()``** : Aplica un Kmeans a cada BATCH. Pierde un poco de fidelidad pero nos ahorra costo computacional y tiempo de ejecucion es mas rapido. Se suele usar en aprendizaje online donde se estan cargando datos constantemente.
  ```python
      #Importar de Sklearn.Cluster el algoritmo de MiniBatchKMeans
      from sklearn.cluster import MiniBatchKMeans
      mb_kmeans = MiniBatchKMeans(n_clusters=4)
      mb_kmeans.fit(x)

      mb_kmeans.score(x) #Da lo mismo pero en Negativo. Mientas mas cercano a 0 mas preciso.


  ```
* **``DBSCAN()``** : Tiene un alcanze más profundo que `KMeans` y puedes agrupar datos organizados de manera irregular. Esto es debido a que el modo en el que opera es encerrando a los datos en circulos como datos centrales y scaneando la cantidad de datos que tiene dentro del circulo. De esta manera discrimina datos centrales de los que no lo son, podremos configurar estos parametros del tamaño del circulo y cantidad de datos dentro de ellos.<br>
Es un algoritmo no supervisado y sirve solo para el pre-procesamiento en separar la data en clusters y apoyandose en otro modelo poder clasificar la data, por ello este modelo no tiene una Funcion de `.predict()` por lo que solo transforma los datos para ser intermediado de un modelo de clasificacion como `KneighborsClassifier()`

    ```python
      from sklearn.cluster import DBSCAN

      dbscan = DBSCAN(eps=0.05, min_samples=5)
      #,eps=0.05 : Radio del circulo
      #,min_samples=0.05 : Cantidad de datos dentro del Circulo
      dbscan.fit(x)

      #Visualizamos la cantidad de agrupaciones que genero y en base a esto podemos ajustar los Hiperparametros.
      dbscan.labels_

      #Importa KneighborsClassifier para clasificar los datos de DBSCAN
      from sklearn.neighbors import KNeighborsClassifier

      knn = KNeighborsClassifier(n_neighbors=50)
      knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])
    ```
  * MeanShift():

    ```python
      from sklearn.cluster import MeanShift
    ```
  * SpectralClustering():

    ```python
      from sklearn.cluster import SpectralClustering
    ```
  * AgglomerativeClustering():

    ```python
      from sklearn.cluster import AgglomerativeClustering
    ```

#### 7.- sklearn.decomposition:

* **`PCA()`** : (Principal Component Analysis) Sirve para reducir las variables o dimensiones de un set de datos manteniendo la mayor cantidad posible de varianza en los datos. Pueden haber 3 motivos por el que es necesario hacer esto:<br>
1.- Eliminar variables poco relacionadas o con poca significancia.<br>
2.- Juntar variables en una sola que esten fuertemente relacionadas o por reciprocidad.<br>
3.- Para que puedan calzar y entenderse dentro de una grafica o tipo de visualizacion.<br><br>

No siempre el reducir la dimensionalidad ayuda a mejorar el modelo, por lo que se necesita estar haciendo pruebas.<br>
PCA() para reducir las dimensiones necesita cargar toda la data en **BATCH** (en memoria) por lo que se usa `IncrementalPCA()` para cargar la data en **MiniBatches**.

  ```python
        from sklearn.decomposition import PCA , IncrementalPCA, KernelPCA
        pca = PCA(n_components=2)
        pca = PCA(n_components=0.90) #En este caso tomara los features que expliquen el 90% (varianza-dispersidad de los datos en realcion a la media) de los datos (info original).

        #,n_components=2 : Indica el numero de variables que queremos que tome de la data. Tomara solo 2 features. Este parametro comprime la informacion para luego descomprimirla y tener todas las demas variables que se omitieron.

        x_nueva = pca.fit_transform(x)
        x_nueva[0:5]

        #Calula la distribución de la varianza
        pca.explained_variance_ratio_  #Indica la importancia de cada variable en un score.

        #Descomprimir la informacion (data)
        x_recuperada = pca.inverse_transform(x_nueva)
        x_recuperada[0:5]

        #PCA INCREMENTAL (cargar la data en memoria por MiniBatches), hara un "fit" del modelo con "features" parciales (batchs)

        from sklearn.decomposition import IncrementalPCA
        subsets = 3  #Haremos 3 batches de la data originial
        ipca = IncrementalPCA(n_components=1)   #Vamos a reducir a 1 dimension los features

        for subset in np.array_split(x, subsets): # Indicamos que carge los features en 3 subsets o batches para "fit"
            ipca.partial_fit(subset) #Hacemos el partial_fit de los features.

        x_nueva = ipca.transform(x)   #Finalmente una vez que se entreno con los features parciales, transformamos los features.
        x_nueva[0:5]

        #KERNEL PCA : Dado que Kernel finje que sube las dimensiones para segmentar correctamente usamos luego de esto PCA para reducir las dimensiones y comprimir la informacion de manera eficiente para no cargar al CPU.

        from sklearn.decomposition import KernelPCA
        pca_rbf = KernelPCA(n_components=2, kernel="rbf") # rbf= Radial Basis function

        x_nueva=pca_rbf.fit_transform(x)
        x_nueva[0:5]

  ```

* **`LocallyLinearEmbedding()`** : (Locally Linear Embedding) Sirve para reducir las variables o dimensiones de un set de datos. Se suele usar para **Preservación de relaciones locales**, **Visualización y análisis exploratorio**, **Datos no lineales**. Se puede usar en combinacion con el `PCA()`.

  ```python
        from sklearn.manifold import LocallyLinearEmbedding

        #Seleccionar el número de dimensiones, componentes y "vecinos"
        lle = LocallyLinearEmbedding(n_components=2, n_neighbors=5)
        x_nueva = lle.fit_transform(x)
        x_nueva[0:5]

  ```

#### 8.- sklearn.tree:

"Arbol de Decicion", encontramos tanto para prediccion lineal como para clasificacion y clustering. El Principal Problema con los arboles de decicion es que se **sobreajustan** demasiado para darnos una puntuacion perfecta minimizando el error a 0, por ello muchos de los parametros destinados a los arboles de decicion son para reducirle la precicion (Añadir Bias) para que se pueda generalizar el modelo y no se sobreajuste solo con datos de entramiento.

* **`DecisionTreeRegressor()`** : Arbol de decicion lineal.

```python
        from sklearn.tree import DecisionTreeRegressor

        reg_arbol = DecisionTreeRegressor()
        reg_arbol.fit(X=tabla_limpia, y=df_label) #entrenamos el modelo

        prediccion_arbol = reg_arbol.predict(tabla_limpia) #Predecirá el "average_price" usando variables a contemplar

        #calculamos el % de la raiz del error cuadratico medio.
        error = mean_squared_error(df_label,prediccion_arbol)
        error = np.sqrt(error)

        error/df_label.mean()
```
* **`DecisionTreeClassifier()`** : Árboles binarios de decisión.

  ```python
      from sklearn.tree import DecisionTreeClassifier
      arbol = DecisionTreeClassifier()
      arbol.fit(x,y)

    #,max_depth=3 : Indica el numero maximo de niveles que tendra el arbol. Si no indicamos este parametro el modelo se sobreajustara hasta tener una puntuacion perfecta sin errores.
    #,Min_samples_split= 4 : En los nodos menores al nº indicado hara que se dejen de dividir las ramificaciones hasta la ultima capa.
    #,Min_samples_leaf= 5 : Hara que cada capa tenga nº indicado de samples y en donde ya no lo pueda mantener ira cesando hasta la ultima capa.
    #,Max_leaf_nodes= 4 : Indica el numero maximo de nodos o capas (niveles) que podra tener el modelo.
    #,Max_features= 3 : Tomara de "X" (features) el nº indicado de caracteristicas (variables) mas relevantes para su ramificacion.


  ```
    Conceptos que debemos tener en cuenta al evualuar la grafica del arbol de decicion:<br>
    * **Variable** : El modelo eligira una de las columnas o variables para la toma de desicion al discriminar. (Peso, Experiencia, Tamaño, etc)
    * **Gini** : (Impureza Gini) Mide la diferencia que hay entre los datos, mientras que [3,2] haya valores Gini medida la diferencia. Cuando Gini es 0 (un lado sea 0: [0,4] o [4,0]) significa que ya no seguira abriando esa rama y acabara el modelo. Gini es equivalente a MSE.
    * **Sample** : Numero de Filas de la tabla, va disminuyendo conforme avanza la separacion.
    * **Value[F,T]** : El primer valor son lo que segun el criterio del modelo no cumplen y el segundo son los que cumplen para seguir separando hasta que el coeficiente de **Gini** sea 0.

  ```python
      #Gráfica el árbol de decisión
      from sklearn import tree
      tree.plot_tree(arbol,feature_names=list(x.columns),rounded=True,filled=True)

      #,rounded=True : Redondea los valores numericos

  ```

  * **`predict_proba()`** : Nos permite que el modelo haga una prediccion de los datos que le indiquemos segun el orden de las columnas o variables de "X" (Caracteristicas)

    ```python
      arbol.predict_proba([[40,6,7,1]]) #40 Edad, ranking 6, años exp 7,etc
      output: [False,True] : Lado izquierdo clasifica como negativo y positivo el derecho.

    ```

#### 9.- sklearn.ensemble:

* **`RandomForestRegressor()`** : (Regresion) "Bosque Aleatorio". Es un conjunto de varios arboles de decicion por lo cual recibe su nombre de Bosque (Forest).<br> Considerar que al usar varios arboles de decicion usa mas recursos computacionales para entrenarse, por lo que suele tambien ser más preciso. Encontramos tanto para prediccion lineal como para clasificacion (clustering).<br>

**RandomForest** realmente es varios arboles de deicion con muestreo con reemplazo (DecisionTree + Bagging) y los parametros de ambos se aplican en **RandomForest**. Gracias al **Bagging** que da la capacidad de entrenar varios arboles de decicion mitiga una la mayor deficiencia de lo arboles de decicion que son el **SobreAjuste**.

```python
      from sklearn.ensemble import RandomForestRegressor

      reg_forest = RandomForestRegressor(random_state=42) #Semilla 42
      reg_forest.fit(x_train_limpia, y_train)

      prediccion_forest = reg_forest.predict(x_train_limpia)

      #calcular el error
      error = np.sqrt(mean_squared_error(y_train,prediccion_forest))
      error/y_train.mean()
```
* **`RandomForestClassifier()`** : (Clasificacion). Es similar a RandomForestRegressor, solo que el output es para clasificacion. Soporta Clasificacion Multiclase.

      ```python
      from sklearn.ensemble import RandomForestClassifier

      cla_forest = RandomForestClassifier(random_state=42, n_estimators=100, max_leaf_nodes=4, max_features=2, n_jobs=4)

      #,max_depth=3 : Indica el numero maximo de niveles que tendra el arbol. Si no indicamos este parametro el modelo se sobreajustara hasta tener una puntuacion perfecta sin errores. Para RandomForest ajusta este parametro no es necesario.
      #,Min_samples_split= 4 : En los nodos menores al nº indicado hara que se dejen de dividir las ramificaciones hasta la ultima capa.
      #,Min_samples_leaf= 5 : Hara que cada capa tenga nº indicado de samples y en donde ya no lo pueda mantener ira cesando hasta la ultima capa.
      #,Max_leaf_nodes= 4 : Indica el numero maximo de nodos o capas (niveles) que podra tener el modelo.
      #,n_estimators=100 : cantidad de veces que correra el estimador
      #,max_samples=10 : La cantidad de datos que tomará de una muestra
      #,bootstrap=True : Si es TRUE usara "Bagging" y FALSE para  "Pasting". Viene en TRUE por DEFAULT.
      #,oob_score=True : Activa la medicion del score de evaluar el modelo con data para test (data nueva con la que no se entreno) .
      #,max_features=2 : Toma la cantidad de columnas (Features) que le indiquemos, asi podemos reducir la cantidad de variables de la data para procesar.
      #,n_jobs=4 : Usara 4 nucleos del procesador para ejecutar la tarea o entrenamiento.


      cla_forest.fit(x_train, y_train)
      cla_forest.oob_score_ #Arroja el score de entrenar con la data que no se selecciono de la muestra con reemplazo.
      ```
      Podemos visualizar que feature de nuestras variables son las que mayor impacto tienen sobre otras:

      ```python
      random = RandomForestClassifier(n_estimators=100)
      random.fit(x_train,y_train)

      for nombre, score in zip(x.columns, random.feature_importances_):   #Solo para ordenar con el print()
        print(nombre,score)

      ```

* **`VotingClassifier()`** : Es un metodo introductorio para entender RandomForest, no es frecuenmente usado. Combina los resultados de varios estimadores (clasificadores) para producir una predicción final. Sin embargo, no arroja el "mejor parámetro" de los estimadores individuales, sino que utiliza una estrategia de votación para tomar la decisión final.<br>
La idea detrás del ensamble de votación es que cada clasificador individual tiene sus fortalezas y debilidades, y al combinar sus predicciones a través de votos mayoritarios o promedios, se pueden obtener resultados más robustos y generalizados.

  * **`Hard Voting`** : (Votación Mayoritaria): Cuando se utiliza la votación mayoritaria, cada clasificador en el ensamble emite una predicción y la clase que obtiene la mayoría de los votos se selecciona como la predicción final del ensamble.

  * **`Soft Voting`** : (Votación Basada en Probabilidades): Cuando se utiliza la votación basada en probabilidades, los clasificadores en el ensamble emiten sus probabilidades de pertenencia a cada clase para una instancia dada. Luego, estas probabilidades se promedian para cada clase y se selecciona la clase con la probabilidad más alta como la predicción final del ensamble.

      ```python
      from sklearn.ensemble import VotingClassifier
      clf1 = DecisionTreeClassifier()
      clf2 = SVC(probability=True)
      arbol = DecisionTreeClassifier(max_depth=2)

      # Creamos el VotingClassifier
      voting_clf = VotingClassifier(estimators=[('dt', clf1), ('svm', clf2), ('arbol', arbol)], voting='hard')

      #,voting= : "hard" : Votacion mayoritaria ; "soft" : votación basada en probabilidades

      voting_clf.fit(X_train, y_train)
      y_pred = voting_clf.predict(X_test)
      ```

* **`BaggingClassifier()`** : (Muestreo Con Reemplazo): Saca una muestra de la data y crea multiples agrupaciones de muestras en el que la data en cada agrupacion puede ser repetida. Para la evaluacion en clasificacion se usara el **promedio de las probabilidades** y para regresion se usara **el promedio de los puntajes**. Se suelen usar con arboles de decicion, pero tambien se usan para otros modelos. Este metodo es mejor que el `VotingClassifier()`.<br><br>

Este metodo se suele usar con arboles de deicion `DecisionTreeClassifier()` debido a que logra resolver el problema de sobreajuste ya que entrena muchos arboles con varias muestras haciendo que realmente aprenda a costa de tener una mayor carga computacional.<br>
Debido a que son muestras con reemplazo, hace que muchos datos no se seleccionen como muestras para entrenar al modelo, a esto se le denomina **Out-Of-Bag** (OOB) el cual podemos usar como datos de prueba para evaluar el modelo; para ello usamos el parametro `,oob_score=True`. <br>
El metodo `.oob_score_` nos arroja la medicion del score que nos dio con `oob_score=True` (Muestras de Test), mientras mas similares sean con la funcion `accuracy_score()` podemos evidenciar que el modelo esta aprendiendo ya que mantiene el mismo score.<br><br>

**BaggingClassifier(Estimador(), n_estimators=100, max_samples=10, bootstrap=True)**

  ```python
      from sklearn.ensemble import BaggingClassifier
      bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=10, bootstrap=True, ,oob_score=True,max_features=2)

      #,n_estimators=100 : cantidad de veces que correra el estimador
      #,max_samples=10 : La cantidad de datos que tomará de una muestra
      #,bootstrap=True : Si es TRUE usara "Bagging" y FALSE para  "Pasting"
      #,oob_score=True : Activa la medicion del score de evaluar el modelo con data para test (data nueva con la que no se entreno) .
      #,max_features=2 : Toma la cantidad de columnas (Features) que le indiquemos, asi podemos reducir la cantidad de variables de la data para procesar.

      bagging.fit(x_train,y_train)
      y_pred = bagging.predict(x_test)
      #Aunque dará un puntaje similar al clasificador de votaciones, esto se debe al set de datos pequeño.
      accuracy_score(y_test,y_pred)
      bagging.oob_score_

  ```

* **`HistGradientBoostingClassifier()`** : Es un **clasificador** de gradiente boosting basado en **histogramas**, es una variante del tradicional Gradient Boosting que utiliza aproximaciones basadas en histogramas en lugar de árboles de decisión, donde cada nuevo modelo intenta corregir los errores del modelo anterior.<br><br>

Las principales ventajas del HistGradientBoostingClassifier son:<br>
* Manejo eficiente de datos dispersos y categóricos.
* Entrenamiento paralelo que lo hace más rápido que el **Gradient Boosting** convencional.
* Resistencia al sobreajuste gracias a las aproximaciones por histograma.
* Buen rendimiento en conjuntos de datos grandes.

Es comúnmente utilizado en problemas de **clasificación y regresión**, especialmente cuando se tienen grandes cantidades de datos y características. Su implementación basada en histogramas lo hace más robusto y escalable que los árboles de decisión tradicionales

  ```python
    import polars as pl
    from sklearn.ensemble import HistGradientBoostingClassifier

    X_train = df_train.select(  #2 columnas
                    F.col("Pclass").cast(pl.String).cast(pl.Categorical), #Pasamos de string a categorica
                    pl.col("Age").fill_null(strategy="mean")
               )

    y_train = df_train['Survived']

    model = HistGradientBoostingClassifier(categorical_features="from_dtype") #Usamos este parametro para q lea la colum categorica de polars

    model.fit(X_train, y_train)

  ```
* **`HistGradientBoostingRegressor()`** : Es un modelo adecuado para problemas de **regresión** cuando se tienen grandes volúmenes de datos y características. Su implementación basada en histogramas en lugar de árboles permite un mejor balance entre capacidad predictiva y velocidad de entrenamiento.

  ```python
  # Cargar los datos
    df = pl.read_csv("boston.csv")  # Supongamos que tienes un archivo CSV llamado boston.csv
    X = df.drop("target")  # Eliminamos la variable objetivo
    y = df["target"]  # Obtenemos la variable objetivo

    # Dividir los datos en conjuntos de entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Crear el modelo HistGradientBoostingRegressor
    model = HistGradientBoostingRegressor(max_iter=100, random_state=42)

    # Entrenar el modelo
    model.fit(X_train, y_train)

    # Realizar predicciones en el conjunto de prueba
    predictions = model.predict(X_test)

    # Calcular el error cuadrático medio (MSE)
    mse = mean_squared_error(y_test, predictions)
    print("Error cuadrático medio (MSE):", mse)
  ```


#### 10.- sklearn.mixture:

* **`GaussianMixture()`** : Es de los mejores modelos para hacer **Clustering**. Usa Distribucion gaussiana el cual es una combinacion de varias distribuciones normales.  Puede utilizarse para la clasificación, agrupación (clustering), análisis de datos atípicos (outliers), y más. La limitante de este modelo es que la data debe tener forma elipsoide. En casos donde los datos tienen formas no elipsoidales o estructuras más complejas, pueden ser más apropiados otros algoritmos de agrupamiento, como el `DBSCAN`.

  ```python
    from sklearn.mixture import GaussianMixture
    gm = GaussianMixture(n_components=3, n_init=10)
    #,n_components=3 : Especificamos la cantidad de Cluster
    #,n_init=10 : Cantidad de corridas para que converja en la solución

    gm.fit(x)

    #Obten los pesos que le asignó a los datos
    gm.weights_

    #Obten las medias de los datos
    gm.means_

    #Obten la matriz de covarianzas
    gm.covariances_

    #Verifca si convergió
    gm.converged_

    #Obten el número de iteraciones
    gm.n_iter_

    #Predicé el cluster de los primeros 5 datos
    gm.predict(x)[0:5]  #Arroja las predicciones

    #Obten las probabilidades de una clustering correcto de cada uno de los datos
    gm.predict_proba(x)[0:5] #Arroja las probabilidades de la prediccion.

    #Genera nuevos datos para evaluar la eficiencia del clasificador
    x_nuevo, y_nuevo = gm.sample(100)
    #Grafica los nuevos datos
    plt.scatter(x_nuevo[:,0],x_nuevo[:,1]) #Si la grafica es similar a los datos iniciales significa que segmento bien.

    #Visualiza las anomalías
    densidad=gm.score_samples(X) #Obtenemos los puntajes (score) de cada de los datos.
    umbral_densidad=np.percentile(densidad,4) #Mientras mas pequeño el numero significa que arroja outliers mas alejados.
    anomalias=X[densidad<umbral_densidad]
    anomalias
  ```
* **`BayesianGaussianMixture()`** : Este modelo ayuda a encontrar la cantidad correcta de **Clusters** (,n_components=) para indicar al modelo. La limitante de este modelo es que la data debe tener forma elipsoide.
  ```python

    from sklearn.mixture import BayesianGaussianMixture
    #Adivina y asigna la cantidad de clusters al algoritmo
    bgm = BayesianGaussianMixture(n_components=10, n_init=10) #De lo 10 clusters que le indicamos usara solo los necesarios.
    bgm.fit(x)
    #Redondea los pesos con np.round
    np.round(bgm.weights_) #Solo se toma los valores diferentes a 0 del output, son la cantidad de clusters correctos.
  ```

##### BOOSTING


* **`AdaBoostClassifier()`** : Toma una forma simple del modelo indicado para entrenarse y sobre los errores detectados, le otorga un mayor peso para volver a entrenarse e ir mejorando el modelo de manera lineal hasta que ya no tiene mas errores (SobreAjuste) o hasta que la siguiente mejora ya no es significante. Suele tener problemas de SobreAjuste.<br>
La matematica detras de AdaBoost no es tan sofisticada como `GradientBoostingRegressor()`.

  ```python

    from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
    ada = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=100, learning_rate=0.1)
    ada.fit(x_train,y_train)

    #Calcular el puntaje de exactitud para adaboost
    y_pred=ada.predict(x_test)
    accuracy_score(y_test,y_pred)
  ```

* **`GradientBoostingRegressor()`** : Es el mejor Booster para ML. No se suele usar el `GradientBoostingRegressor()` que trae **sklearn** debido a que trae su propia libreria el cual es `xgboost` que aplica este metodo de manera mas eficiente, el cual trae su propio **early stopping**.

  ```python
    from sklearn.metrics import mean_squared_error
    from sklearn.ensemble import GradientBoostingRegressor

    x_train, x_test, y_train, y_test = train_test_split(x,y)

    gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)  #indicamos la profundidad del arbol. Usualmente son 2, 4 o 6.
    gbrt.fit(x_train, y_train)

    errores = [mean_squared_error(y_test,y_pred) for y_pred in gbrt.staged_predict(x_test)]
    mejor = np.argmin(errores)
    mejor
  ```

* **`XGBRegressor()`** : Viene de la libreria externa `xgboost` el cual es más rapido y eficiente, trae su propio **early stopping**.
  ```python

    !pip install xgboost

    from xgboost import XGBRegressor
    xgb = XGBRegressor()
    xgb.fit(x_train, y_train, eval_set=[(x_test,y_test)], early_stopping_rounds=1)   #Evalua el early stopping automáticamente
    y_pred = xgb.predict(x_test)

  ```
* **`StackingRegressor()`** : Combina los resultados de diferentes modelos (Estimadores) y los junta en un nuevo modelo para arrojar finalmente un nuevo resultado. Para esto usa la **Regresion de Ridge** la cual aplica una validacion cruzada penalizando a las variables que tienen poca relevancia con los datos.
  ```python
    #Importa los regresores necesarios StackingRegressor,LinearRegression, RandomForestRegressor
    from sklearn.ensemble import StackingRegressor
    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error

    arbol = DecisionTreeRegressor(max_depth=20)
    lineal = LinearRegression()
    random = RandomForestRegressor()

    #Genera los tres modelos
    stacking = StackingRegressor(estimators=[("arbol", arbol),
                                             ("lineal", lineal),
                                             ("random", random)])
    #Utiliza Stacking
    stacking.fit(x,y)


    y_pred = stacking.predict(x_test)
    error = np.sqrt(mean_squared_error(y_test, y_pred))
    print("Error:", error, "Acierto", error/y_test.mean())


  ```

___
### Técnicas de regularización (sklearn.linear_model)

Las técnicas de regularización se aplican en el aprendizaje automático para evitar el sobreajuste y mejorar la generalización de los modelos. Las técnicas de regularización introducen términos de penalización en la función de costo del modelo, lo que ayuda a controlar la complejidad del modelo y limitar la magnitud de los coeficientes de las variables predictoras o simplemente eliminan variables que no tienen relación o peso.<br>
En otras palabras cuando se tiene muchas variables ayuda a repartir el peso según el impacto que tienen o simplemente pueden eliminar el numero de variables de las que no se tiene relación significativa.<br><br>

Estas técnicas de regularización como **Ridge**, **Lasso** y **Elastic Net** modifican la matriz de características "X", pero no modifican directamente el vector de salidas "y". Para que después de aplicar estas técnicas, los datos X e y pueden alimentar otros modelos como árboles de decisión u otros algoritmos, pero con un conjunto reducido y más relevante de variables predictoras. Esto ayuda a que el nuevo modelo se entrene solo con las variables realmente relevantes, evitando sobreajuste.

  * **Ridge Regression** (Regresión de Ridge): Aplica regularización L2 para penalizar la suma de los cuadrados de los coeficientes. Toma todas las variables y penaliza a las variables que tienen poca relevancia con los datos. No elimina variables.

    ```python
      from sklearn.linear_model import Ridge
      ridge = Ridge()
      ridge.fit(x,y)

    ```

  * **Lasso** (Least Absolute Shrinkage and Selection Operator): Aplica regularización L1 para penalizar la suma de los valores absolutos de los coeficientes. Elimina las variables que tienen poco relevancia, reduciendo asi el numero de variables o columnas que contemplara como relevantes.


    ```python
      from sklearn.linear_model import Lasso
      lasso = Lasso(alpha=0.1) #Alpha 0.1 indica que las variables debajo de ese umbral seran eliminadas
      lasso.fit(x,y)

    ```

  * **Elastic Net**: Combina las regularizaciones L1 y L2 para tener un equilibrio entre la selección automática de características y la estabilidad de los coeficientes. Combina `Ridge()` y `Lasso()` teniendo el parametro ``,l1_ratio=0.5`` el cual cuando es 0 elimina a ``Lasso()`y cuando es 1 elimina a `Ridge()`. Y en 0.5 configura mitad a mitad tanto `Ridge()` y `Lasso()`.

    ```python
      from sklearn.linear_model import ElasticNet
      net = ElasticNet(alpha=0.1, l1_ratio=0.5)
      net.fit(x,y)

    ```
___
### > Comandos sklearn:

#### from sklearn.multiclass import OneVsRestClassifier:

Es una Estrategia que permite que podamos usar **clasificadores binarios** y usarlos como **clasificación multiclase** para que en el entrenamiento podamos pasarle todas las variables categorizadas de una columna de golpe.<br>
Por default si soporta que le pasemos todas las variables categorizadas de una columna de golpe, por lo que lo resolvera usando la estrategia (1 vs 1), el cual consume recursos computacionales. Para forzar que utilice la estrategia (1 vs Rest) usamos **``OneVsRestClassifier()``**.
Algunos de los modelos Binarios que se favorecen con este metodo son:

* **``LogisticRegression()``** : Regresión logística, que es un modelo lineal utilizado para la clasificación binaria.
* **``RandomForestClassifier()``** : Bosques aleatorios, que es un conjunto de árboles de decisión utilizados para la clasificación binaria.
* **``GradientBoostingClassifier()``** : Gradient Boosting, que es un conjunto de árboles de decisión entrenados secuencialmente utilizado para la clasificación binaria.
* **``KNeighborsClassifier()``** : K Vecinos más cercanos (K-Nearest Neighbors), que es un algoritmo de clasificación basado en la proximidad a los vecinos más cercanos utilizado para la clasificación binaria.
* **``GaussianNB()``** : Naive Bayes Gaussiano, que es un algoritmo de clasificación probabilístico utilizado para la clasificación binaria.
* **``SVC()``** : Support Vector Classifier, que es un modelo basado en máquinas de vectores de soporte utilizado para la clasificación binaria. Necesita que los datos de entrada esten bien escalados `StandarScaler()`

  ```python
  from sklearn.multiclass import OneVsRestClassifier

  svc = OneVsRestClassifier(SVC()) #Encerramos el modelo SVC adentro para forzar el (1 vs rest)
  svc.fit(x_train,y_train)

  ```

#### from sklearn.model_selection:

* **`train_test_split()`** : (Para modelos Supervisados). Esta función nos permite dividir un dataset en 2 bloques (train y test), sin importar cuantos arrays le pasemos siempre divida en pares de 2; uno destinado al aprendizaje (**entrenamiento**) y otro para hacer las **pruebas**. Por defecto esta division le asigna 75% de los datos a  **entrenamiento** y 25% a **pruebas**. Podemos cambiar el porcentaje orientado a pruebas con `,test_size=`.<br>

Por lo general se suele usar 2 variables para dividir en 2 pares de X (caracteristicas) y Y (target) para un modelo supervisado. Una vez que se ha realizado la división, puedes utilizar los conjuntos **X_train** (caracteristicas) y **y_train** (etiquetas) para entrenar tu modelo y los conjuntos **X_test** y **y_test** para evaluar su rendimiento.<br><br>

  **X_train,X_test,Y_train,Y_test = train_test_split(Array_X, Array_Y, test_size=0.3, random_state=11, shuffle=True)**

  **Variables:** x=features ; y=Target <br>

  * **X_train** = Es el conjunto de datos de entrada (características) para el **entrenamiento** del modelo.
  * **X_test**  = Es el conjunto de datos de entrada (características) para la **evaluacion** del modelo
  * **Y_train** = Es el conjunto de etiquetas (target) para la **entrenamiento** del modelo
  * **Y_test**  = Es el conjunto de etiquetas (target) para la **evaluacion** del modelo

  ```python
      from sklearn.model_selection import train_test_split

      X_train,X_test,Y_train,Y_test = train_test_split(digist.data,digist.target,test_size=0.3, random_state=11,shuffle=True)
      #test_size=0.3 = separa un 30% de la data para test .
      #,random_state=11 : Semilla
      #shuffle=True : barajea los datos antes de hacer el corte.

      train, test = train_test_split(data, test_size=0.3, random_state=45)

  ```

* **`StratifiedShuffleSplit()`** : Se usa para poder separar el 30% de los datos por Categoria para pruebas y de manera barajeada, en otras palabras toma en consideración las etiquetas de clase en los datos y realiza la división de train y test de manera aleatoria manteniendo la proporcion de categorias para ambas (en caso de hacer 1 corte).<br>
En caso de usar en conjunto una validacion crusada no es necesario usa `,shuffle=True` en `Kfold`, podemos presendir y usar el parametro `,cv=` de `cross_val_score`.<br>
Ejemplo: tenemos un columna categorica "1,2,3" va a separar mantiendo el 30% para cada categoria como test.<br>
Arroja el numero de index del df que contienen el % indicado de separacion.<br><br>

    ```python
        from sklearn.model_selection import StratifiedShuffleSplit

        dividir = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=45) #Es similar a train_test_split solo que indicamos que solo haga 1 corte.

        # Usamos el metodo .split de StratifiedShuffleSplit para iterar sobre el df teniendo la columna del df donde estan las categorrias. Esto devuelve el index de los cortes.

        for train_idx, test_idx in dividir.split(data,data["salary_cat"]):
            cat_train = data.loc[train_idx] #arroja el index del split dentro del df
            cat_test = data.loc[test_idx]

        #Otra manera de usarlo directamente:

        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
        train_index, test_index = next(sss.split(X, y)) #X= Data ; y=target

        # Realizamos 'cross_val_score' con los índices generados por 'StratifiedShuffleSplit'
        scores = cross_val_score(model, X, y, cv=[(train_index, test_index)])
    ```

* **``KFold()``** : Esta clase Segmenta la data en k partes el cual usará una de las partes para ``test`` y las otras para ``train`` hasta haber terminado de conjugar con cada una de las segmentaciones previamente dada logrando asi aumentar el entrenamiento del modelo para mejorar su precision.

    Cuando haya mucha cantidad de datos o no haya patrones difíciles por descubrir `k` será más pequeño. Este es un método de validación, el modelo final será entrenado con toda la tabla.

  ```python
      from sklearn.model_selection import KFold

      kf = KFold(n_splits= 10 , random_state= 11, shuffle= True)
      #n_splits = nº de dobleces (De acuerdo a la repartición de los datos)
      #random_state = nº de semilla
      #shuffle = barajeara los datos antes de ponerlos en los dobleces.
  ```
* **``cross_val_score()``**

"Validacion Cruzada" Devuelve puntajes de rendimiento para evaluar el modelo insertado. Se suele usar junto con `kfold` para cortar la data en segmentos y entrenar el modelo dejando un segmento libre para testear el resultado y asi va iterando por cada segementos dejando siempre uno libre para test. Eso significa que correra el modelo segun la cantidad de segmentos que hayamos indicado para cortar.<br>
Ejemplo si cortamos la data en 10 segmentos simula el tener 10 veces mas datos que los originales, por ellos se usa para lidiar con el **OverFitting** en el caso que el origen sea la insuficiencia de datos.<br>
Descarta el overfitting que podria estar teniendo el modelo o si no estamos seguros del error que nos arroja el modelo pues va a entrenarse k veces por el numero de segmentos que indiquemos, en otras palabras nos da una mayor seguridad del verdadero error del modelo.
<br><br>

**cross_val_score(estimator= modelo, X=variables, y=variable_target, scoring="tipo_error", cv=nº_dobleces_data o kf)**


  ```python
      from sklearn.model_selection import cross_val_score
  ```
Es mejor usar `kfold` para cortar la data activando el ``shuffle`` en True para barajear la data e igualar el parametro `cv=kf` al corte manual que hicimos con kfold ya que por defecto si igualamos cv=5 a un numero de cortes no tendra los parametros manuales del shuffle y mas de kfold.

  ```python
      #Hacemos los cortes manualmente usando los paramtros de barajear data para mayor singularidad de los datos.
      kf = KFold(n_splits= 10 , random_state= 11, shuffle= True)

      val_cruzada = cross_val_score(estimator= knc, X=x_train, y=y_train,scoring="neg_mean_squared_error" ,cv=kf)

      #estimator = Modelo de estimador.
      # X = data (x_train y x_test)
      # y = targets (y_train y y_test)
      # scoring="neg_mean_squared_error" :indica el tipo de error, en este caso "error cuadratico medio" en negativo. "accuracy": arroja % de Exactitud.
      # cv = nº de segmentos train y 1 test.

      #Calcular el porcentaje de acierto de validación cruzada
      error = np.sqrt(abs(val_cruzada)) #Convertimos 'abs' ya que usamos el negativo "neg_mean_squared_error"
      error.mean() / df_label.mean()   #Promedios predicción/real para ver %precision del modelo.
  ```
* **``cross_val_predict()``** : Devuelve las predicciones del modelo insertado para cada instancia en cada pliegue. A diferencia de cross_val_score, este metodo se usa exclusivamente para modelos de **clasificacion**.<br><br>

  **cross_val_predict(estimator= knc, X=x_train, y=y_train,cv=3, method="predict")**

  ```python
      from sklearn.model_selection import cross_val_predict
  ```
    ```python
      prediccion_cruzada = cross_val_predict(estimator= knc, X=x_train, y=y_train,cv=3, method="predict")

    ```
  * **``,method="predict"``** : Podemos elegir la metrica que queremos que nos arroje, por defecto esta en ``"predict"``. El cual arroja las predicciones segun las caracteristicas ingresadas, previamente de un modelo entrenado.
    *  ``"predict_proba"`` : arroja una lista de las probabilidades (positiva y negativa) con los que el modelo ha **clasificado** de las predicciones. Arroja el siguiente array de probabilidades: **``[Negative,Positive]``**
    *  ``"decision_function"``  : Arroja una lista de las puntuaciones (positiva y negativa) con los que el modelo ha **clasificado** de las predicciones.

* **``GridSearchCV()``** :  Sirve para poder pobrar diferentes **Hiperparametros** de los modelos con diferentes valores y elegir el que menor error arroje. Indicamos el nombre tal cual de los parametros del modelos con el que probaremos. El algoritmo probará todas las combinaciones posibles de valores de hiperparámetros en la cuadrícula.

  ```python
      from sklearn.model_selection import GridSearchCV
  ```
**GridSearchCV(modelo, parametros_dic, scoring=tipo de error, cv=nº segmentos para cross_val_score)**
  ```python
      hiper_grid = [{                 #Definimos los Hiperpárametros del modelo con lo que probara en un diccionario.
            "n_estimators":[3,10,30], #Parametros del modelo ("RandomForestRegressor")
            "max_features":[2,4,6,8]
                }]
        #hiper_grid=[dict(n_estimators= Range(2,100))] #Otra manera de ingresar datos > {'n_estimators': range(2, 100)}
        grid_train = GridSearchCV(reg_forest, hiper_grid, scoring="neg_mean_squared_error", cv=5)
        grid_train.fit(tabla_limpia,df_label)



        #Mejor combinacion de parametros:
        grid_train.best_params_

        #Arroja el error cuadratico medio
        grid_train.best_score_

        #Obtene el puntaje del GridSearch (Grid.score)
        grid_train.score(x_test, y_test)

        #Tomamos el modelo con los mejores "Hiperparametros"
        modelo_final = grid_train.best_estimator_
        prediccion = modelo_final.predict(X_preparada)
  ```
* **``RandomizedSearchCV()``** :  Funciona igual de `GridSearchCV`. Realiza una búsqueda más eficiente y aleatoria en el espacio de hiperparámetros utilizando distribuciones. Se especifica una distribución para cada hiperparámetro, en lugar de valores concretos.

  ```python
      from sklearn.model_selection import RandomizedSearchCV
      model_rnds_cv = RandomizedSearchCV(keras_reg, param_distributions=param_distribs, n_iter=10,cv=3,verbose=2)

      #Hacer Feed
      model_rnds_cv.fit(X_train, y_train, epochs=100,
                  validation_data=(X_valid, y_valid),
                  callbacks=[keras.callbacks.EarlyStopping(patience=10)]) #Que salte a los siguientes parámetros de la rejilla si no mejora
        #Mejor combinacion de parametros:
        model_rnds_cv.best_params_

        #Arroja el error cuadratico medio
        model_rnds_cv.best_score_

        #Obtene el puntaje del GridSearch (Grid.score)
        model_rnds_cv.score(x_test, y_test)
  ```

* **``LearningCurveDisplay()``** :  Sirve para hacer la grafica **train-test**. La cual se suele usar con caracteristicas (``x``) y target (`y`).

  ```python
      from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit

      LearningCurveDisplay.from_estimator(reg_lin, x_train, y_train, cv=ShuffleSplit(n_splits=5, test_size=0.2, random_state=0),scoring= "neg_mean_squared_error", line_kw= {"marker": "o"},n_jobs=-2)
      plt.show()

      #,scoring="score" : Elegimos el tipo de error que queremos que nos muestre en el eje "y".
      #,line_kw= {"marker": "o"} : Indicamos que grafique una linea con puntos o nodos.
      #,n_jobs=1 : Indica el nº  de procesadores que usara, -1 significa todos los CPU y -2 dejara 1 CPU libre.

  ```

#### from sklearn.metrics:

Arroja herramientas para calcular metricas de los resultados obtenidos tras el entrenamiento y predicciones de los modelos. Los parametros de las metricas es:<br>
**metrica(y_train, prediccion_modelo)**<br><br>

De tener un acierto muy elevado o de 100% debemos asegurarnos de que no tengamos **OverFetting**, usando previamente un `cross_val_score` o `cross_val_predict` (clasificacion) para luego verificar con las metricas.

* **``accuracy_score()``** : Es una métrica comúnmente utilizada en problemas de clasificación y representa la proporción de instancias clasificadas correctamente sobre el total de instancias. En otras palabras, mide la tasa de aciertos del modelo en predecir las etiquetas correctas. **Precisión = (Número de predicciones correctas) / (Número total de instancias)**

  ```python
      from sklearn.metrics import accuracy_score
      accuracy_score(y_test,y_pred)

      #Tras haber usado el VotingClassifier para compara 2 modelos independiente vs combinado.
      for i in (svm, arbol, votingClassifier): # 2 modelos y el votingClassifier
      i.fit(x_train,y_train)
      y_pred = i.predict(x_test)
      print(i.__class__.__name__,accuracy_score(y_test,y_pred)) #el VotingClassifier tiene un mayor puntaje "0.916".
  ```

* **``r2_score()``** : (Coeficiente de Correlación cuadrada r²),Saca el r2. Esta se mide de 1- a 1 mientras más cercana a 1 indica absoluta precision del modelo en cambio con -1 es inversamente proporcional. Valores cercanos a 0 indica que no hay relación.
  ```python
              metrics.r2_score(esperado o real y_train, prediccion)
  ```
* **``mean_squared_error()``** : (Promedio de Errores Cuadrados). Mientras más cerca de "0" indica mayor precision del modelo.<br>
  La Metrica mas relevante es la **Raiz del Error Cuadratico medio** por lo que usamos `np.sqrt(metrics.mean_squared_error(esperado o real, prediccion))` para luego dividirlo entre la media de la lista `esperado o real` y viazualizar el % de error del modelo.
  ```python
              metrics.mean_squared_error(esperado o real, prediccion)

              #La Metrica mas relevante es la Raiz del Error Cuadro:
              prediccion_tabla = reg_lin.predict(tabla_limpia) #predecimos la tabla con las columnas a contemplar.

              error = mean_squared_error(df_label,prediccion_tabla)
              error = np.sqrt(error) #arroja en valor numerico el promedio del error cuadrado bajo raiz: 68747.58

              #Calcular el porcentaje de acierto dividiendo los promedios de Prediccion/reales.

              error/df_label.mean()
  ```
* **``precision_score()``** : (Precision) Arroja la precision del modelo en función de los aciertos entre el totalprocesado (solo los Positivos): Nº A / Muestras analizadas<br>
**TP / (TP + FP)**

* **``recall_score()``** : (Recuperacion) Arroja la precision del modelo en función de los los aciertos entre el total dela clase de toda la data:<br>

* **``f1_score()``** : Mide el equilibrio de **Precision** y **Recall**, combina estas 2 métricas en una sola medida. Un Valor cercano a 1 indica una buen equilibrio entre **Precision** y **Recall** señalando que ha minimizado tanto los**Falsos Positivos** (FP) como **Falsos Negativos** (FN) mientras que un valor cercano a 0 indica un mal equilibrio.

* **``precision_recall_curve()``** : Es una función que ayuda a evaluar y comprender el rendimiento de un **clasificador** calculando los valores de **precisión** y **recall** en diferentes valores del puntaje que ha arrojado la prediccion del modelo **umbral** (thresholds).<br>
Siempre en la intersección entre **"Presicion"** y **"Recall"** es el punto más balanceado, pero esto no siempre es lo que se busca, dependiendo el tipo de problema o indicaciones del cliente, se puede ir cambiando el umbral hasta tener una diferencia de **"Presicion"** y **"Recall"** deseado priorizando una de las 2.<br>
Esta metrica es solo paraclasificacion y no para regresion.
La funcion nos arroja 3 variables como output:

  * **precision**: Un array que contiene los valores de precisión para diferentes umbrales.
  * **recall**: Un array que contiene los valores de recall para diferentes umbrales.
  * **umbrales** : (thresholds) Son las puntuaciones que el modelo ha arrojado de la predicción (x_train,y_train) generados por un clasificador o modelo. Considerar que el array de umbrales tiene un valor menos que los arrays de **precisión** y **recall** debido a cómo se calculan los valores en la función por lo que al graficar es necesario quitar una fila a **precisión** y **recall**.

  ```python

      presicion, recalls, umbrales = precision_recall_curve(y_train_bin,y_scores) #(target,predicciones)

      #Grafica:
      # Convertimos a DataFrame y le restamos 1 fila a presicion y recalls para que tengan el mismo tamaño con umbrales
      data = pd.DataFrame({'Precision': presicion[:-1], 'Recall': recalls[:-1], 'Umbrales': umbrales})

      # Indicamos que la columna Recall lo tome como index.
      data.iplot(kind='line', xTitle='Umbrales', y=["Precision","Recall"], x=["Umbrales"],yTitle='Precision', title='Curva de Precisión y Recall', legend=True)

      #Buscamos mover el valor umbral donde nos arroje la presicion>=.90
      umbral_90 = umbrales[np.argmax(presicion>=.90)] #Filtra en indice 83.
      umbral_90  #arroja el valor del umbral

      #Creamos un nuevo target donde esten solo valores en el la puntuacion de y_score sea >= a la puntuacion del umbral_90
      y_train_90 = (y_scores>=umbral_90)

      #Evaluamos:
      p = precision_score(y_train_bin, y_train_90)
      r = recall_score(y_train_bin, y_train_90)

      print(p,r)

  ```
* **``roc_curve()``** : (Receiver Operating Characteristic) Similar al `presicion_recall_curve` arroja 2 variables metricas y 1 umbral. La curva ROC analisa solo el lado de los positivos arrojando la relación entre la **tasa de verdaderos positivos** (TPR) y la **tasa de falsos positivos** (FPR) a medida que se varía el **umbral de clasificación**.<br>
El modelo que tenga mayor area bajo la curva es el que mejor relacion TP y FP tiene entre los positivos. Lo ideal es que la grafica tenga forma de escuadra.

  ```python
      from sklearn.metrics import roc_curve

      tpr, fpr, umbral = roc_curve(y_train_bin,y_scores)
  ```
  * **``roc_auc_score()``** : Calcula el área bajo la curva ROC (Receiver Operating Characteristic) para evaluar el rendimiento de un modelo de clasificación binaria. Toma como entrada las etiquetas reales (y_train_bin) y las puntuaciones de predicción (y_scores) del modelo, y devuelve el puntaje del área bajo la curva ROC.<br>
  Un valor de AUC-ROC cercano a 1 indica un buen rendimiento del modelo en términos de separación de las clases positiva y negativa

  ```python
      from sklearn import roc_auc_score

      roc_auc_score(y_train_bin,y_scores) #Nos arroja un puntaje. Mas cercano a 1 es mejor los TP del modelo.
  ```
  * **``RocCurveDisplay()``** : Genera el grafico de la curva ROC usando por detras a `matplotlib`.<br>
  Puede hacer las graficas de 2 maneras:<br>
    * **``RocCurveDisplay.from_predictions``** : Usando las predicciones. **RocCurveDisplay.from_predictions(y_train_bin, y_scores)**
    * **``RocCurveDisplay.from_estimator``** Usando el estimador para calcular las predicciones y graficarlas. **RocCurveDisplay.from_estimator(clf, X_test, y_test)**

  ```python
      from sklearn.metrics import RocCurveDisplay

      RocCurveDisplay.from_estimator(clf, X_test, y_test) #Usando el estimador
      RocCurveDisplay.from_predictions(y_train_bin, y_scores) #Usando las Predicciones

      #Se le puede añadir la siguiente linea ya que esta dibujado sobre `matplotlib`
      plt.plot([0,1],[0,1],"k--")
  ```

* **``confusion_matrix()``** : Muestra en una matriz de los datos acertados y erróneos del la prueba (1_Diagonal: Positivos, 2_Diagonal: Negativos). En el siguiente orden:<br>
    **Verdadero Negativo (TN), Falso Positivo (FP), Falso Negativo (FN), Verdarero Positivo (TP)**<br><br>

    En caso de tener 3 variables la matriz seria de 3x3.

    ```python
        from sklearn.metrics import confusion_matrix

        confusion = confusion_matrix(y_train=esperado,y_pred=prediccion)
    ```
* **``classification_report()``** : Parecido a la matriz de confusion muestra métricas de la comparación de prueba y target.

    - Precision: Verdaderos positivos/ vp + falsos positivos
    - recall (Sensitividad): Verdaderos positivos/ total positivos
    - f1-score : promedio entre Precision y recall.
    - support : Cantidad de valores reales que tenia para cada dígito para el ejemplo.

    ```python
        from sklearn.metrics import classification_report

        nombres = [str(digit) for digit in digits.target_names]
        #Pasamos los targets de formato array a string.
        print(classification_report(esperado,prediccion,target_names= nombres)) #En ese orden (espera,prediccion,targetnames=)
    ```

#### from sklearn.inspection:

Este modulo tambien contiene graficas displays para `.from_estimator()` o `.from_predictions()` al iguel que `.metrics`<br>
Podemos revisar todas las graficas disponibles con el siguiente comando:

  ```python
    from sklearn.utils.discovery import all_displays

    displays = all_displays()
    displays

  ```
Ejemplo de uso de grafica:

  ```python
  from sklearn.inspection import DecisionBoundaryDisplay
  from sklearn.datasets import load_iris
  from sklearn.svm import SVC
  from sklearn.pipeline import make_pipeline
  from sklearn.preprocessing import StandardScaler
  import matplotlib.pyplot as plt

  iris = load_iris(as_frame=True)
  X = iris.data[['petal length (cm)', 'petal width (cm)']]
  y = iris.target


  svc_clf = make_pipeline(StandardScaler(),
                          SVC(kernel='linear', C=1))
  svc_clf.fit(X, y)

  display = DecisionBoundaryDisplay.from_estimator(svc_clf, X,
                                                  grid_resolution=1000,
                                                  xlabel="Petal length (cm)",
                                                  ylabel="Petal width (cm)")
  plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='w') #Grafica Columna 0 y 1 ('petal length (cm)', 'petal width (cm)'), c=color segun tipo de "y", edgecolors="WHITE".
  plt.title("Decision Boundary")
  plt.show()
  ```

#### from sklearn.feature_extraction.text:

Esta Librería esta orientada a la extracción y tokenización de palabras de un texto.

* **CountVectorizer:**

    ```python
        from sklearn.feature_extraction.text import CountVectorizer
    ```

  Es el constructor que recibe los comandos que indiquemos para la tokenización.

  * **.fit_transform():** Este método extrae y tokeniza las palabras del texto a indicar.

    ```python
        sample_data = ["Hello Word","Hello Hello Word", "Word Word Word"]

        vectorizer = CountVectorizer()
        x = vectorizer.fit_transform(sample_data)

    ```

  * **.get_feature_names():** Este método mostrara las palabras tokenizadas.

    ```python
        print(vectorizer.get_feature_names())
    ```

## [21. TensorFlow](#indice)
Es una librería para **Depp Learning**.<br>
*pip install tensorflow*<br>

```python
    import tensorflow as tf

    tf.config.list_physical_devices('GPU') #configuramos el uso de GPU en Global.
```
Para tensorflow las dimensiones de los diferentes tipos de datos son indicados de las siguiente manera:

* **``Scalar``** : Referencia a un solo valor.
* **``Vector``** : Referencia a una lista tupla o Array (columna en df).
* **``Matriz``** : Referencia a una matriz de 2D en numpy o tabla de 2 columnas en pandas. Imagen a blanco y Negro.
* **``Tensor``** : Referencia a una matriz de 3D a más dimensiones en numpy o tabla de n columnas en pandas.
  * **``Tensor 3D``** : Imagen a color (R,G,B)
  * **``Tensor 4D``** : Coleccion de imagenes a color (fps) el cual se usa para representar los videos.
  * **``Tensor 5D``** : Coleccion de videos.


### Keras:

Es un FrameWork encima de TensorFlow que ayuda a la creación del modelo de manera mas simplificada. Esto es debido a que el diseño del modelo de la Red lo haremos a nivel de capas ya establecidas como Dense, convolucionales ,etc.

```python
    from tensorflow import keras
```

#### Datasets:

Keras trae sus propios datasets para trabajar y entrenar un modelo:

```python
    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
    #X_train : Contiene 60k matrices de 28x28 representando a los pixeles para entrenar.
    #y_train : Son las Etiquetas 0-9 para cada imagen o matriz de X_train.
    #X_test y y_test : Son los datos de prueba para evaluar el aprendizaje del modelo. 10k
```
#### utils:

Son un paquete de Funciones para el pre-procesamiento, preparacion de datos y graficos.

```python
from tensorflow.keras.utils import to_categorical

```
* **`to_categorical()`** : (One Hot Encoding) Expresa los valores numericos en un array donde cada columna es un "0" hasta contar al numero indicado donde lo indicara como un "1". Para una lista tomara el valor maximo para definir la longitud o tamaño de la lista a generar rellenando de 0 hasta indicar con un 1 cada valor de la lista contando las posiciones del array desde el index 0.<br>
Devuelve una matriz de 2D en donde cada array tendra el valor maximo de la lista hallada, contando desde la posicion 0 hasta el valor maximo hallado. Ejemplo si el valor maximo de nuestra lista es 9 tendremos Arrays de 10 elementos en nuestra matriz. **(nº de elementos, valor maximo de la lista)**
 *  **`,num_classes=`** : Podemos indicarle la longitud del array a devolver para que llene de 0's siempre y cuando indiquemos un nùmero mayor al maximo valor de la lista dada.
 *  **`,dtype=`** : Indicamos el tipo de dato de la lista de entrada. Por defecto pasa los valores a "float32".

```python
    a = to_categorical([1, 3, 4], num_classes=9, dtype=float32)
    print(a)
#   array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],
#          [0., 0., 0., 1., 0., 0., 0., 0., 0.],
#          [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float32)
    # (nº de elementos, valor maximo de la lista ó num_classes siempre y cuando sea mayor al max valor de la lista)

    y_train = to_categorical(y_train) #Indicara de manera categorica todos los valores numericos del Array de 1D.
```

##### Tipos de Modelos:

* **`Red Neuronal Convolucional (CNN) `** :
* **`Red Neuronal Recurrente (RNN) `** :
* **`Red de Densidad Mixta (MDN) `** :
* **`Auto Encoder Variacional (VAE) `** :
* **`Algoritmos Genéticos (GA) `** :
* **`Estrategias Evolutivas (ES) `** :

##### Models:

Eligiremos el modelo según el tipo de "kind" de modelo que queramos construir teniendo en cuenta nuestros datos y target. Según el tipo de "kind" de modelo que elijamos se suele guardar dentro de una variable llamada **model**.<br>
Armamos el modelo `.Sequential([])`, compilamos `.compile()` el modelo y entrenamos `.fit()`.


* **`model = keras.models.Sequential([])`** : Para armar nuestras capas siempre partimos con `.Sequential`

Métodos para nuestra variable model:

* **`model.summary() `** : Nos arrojara un resumen de todas las capas y tipo de "kind" de capas que hemos ingresado a nuestro `.Sequential` o model.

    ```python
        model.summary()

        # Visualizar Gráficamente en archivo png.

        from tensorflow.keras.utils import plot_model
        from IPython.display import Image

        plot_model(cnn, to_file="covnet.png", show_shapes=True,show_layer_names=True) #Guarda la imagen .png
        Image(filename="covnet.png") #muestra la imagen .png en la notebook.

        #Es necesario descargar e instalar el "graphviz".

    ```
* **`model.layers() `** : Otra manera de ver las capas y acceder a ellas

    ```python
        #en caso de que quiera ocultar una capa
        model.layers[1]
        #Ver el nombre de la capa
        model.layers[1].name
        #ver los pesos de la red neuronal
        pesos, bias = model.layers[1].get_weights()

    ```
* **`model.compile() `** : Compilaremos el modelo indicando el optimizador, función de error y métrica de presición. El tipo de función de error lo definira el tipo de output que tegnamos en el modelo:

    ```python
        model.compile(optimizer="adam", #Es el más balanceado de los Optimizadores
                      loss="binary_crossentropy", #Para Clasificacion Binaria
                      metrics=["accuracy"])

        #Configurando el Optimizador Adam manualmente:
        model.compile(optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999), #Es el más balanceado de los Optimizadores
                      loss="binary_crossentropy", #Para Clasificacion Binaria
                      metrics=["accuracy"])

    ```
* **`model.fit() `** : Tras asignar nuestro `.compile` entrenará el modelo.

    ```python
        history = model.fit(train_features, train_labels, validation_split=0.2, ephocs=100,validation_data=(x_valid,y_valid))

        #,ephocs=100 : Entrenaremos un modelo con 100 ciclos (Desde el incio hasta el final de los registros es una epoca)
        #,batch_size : Indica el numero de muestras a procesar durante un "epochs" (evita el Overfitting). Indica cada cuantos registros procesados recalculara los pesos hasta terminar una epoca.
        #,validation_split : Indica que deje el 20% de los datos para validación
        #,validation_data=(x_valid,y_valid) : Podemos indicar de manera manual la data que Cortamos previamente de la data separada para train para poder hacer que se evalue mientras se entrena, es opcional.

        #Entrenar usando la GPU:

        with tf.device('GPU'): #Tambien se puede configurar el uso de GPU en global con "tf.config.list_physical_devices('GPU')"
          entrenamiento = model.fit(x_train,y_train,epochs=35,validation_data=(x_valid,y_valid))

        #Vemos los valores de las cuatro metricas de evaluacion
        entrenamiento.history

        #Diseñar y entrenar modelo con multiples Entredas y Salidas.
        #Armar el modelo
        input_A = keras.layers.Input(shape=[5], name = "wide_input") #Toma 5 Columnas. Los nombres se verán en el .summary()
        input_B = keras.layers.Input(shape=[6], name = "deep_input") #Toma 6 Columnas.
        hidden1 = keras.layers.Dense(30, activation="relu")(input_B) #Indicamos entreparentesis la variable predecesora.
        hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
        concat = keras.layers.concatenate([input_A, hidden2])
        output = keras.layers.Dense(1, name="output")(concat)
        aux_output = keras.layers.Dense(1, name="aux_output")(hidden2)

        # Creamos la variable "model"
        model = keras.models.Model(inputs=[input_A,input_B], outputs=[output, aux_output]) #Definimos las entradas y salidas del modelo.
        model.summary()

        #compilar los modelos
        model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=1e-3))

        #Entrenamos
        #Correr el modelo. Se sub-dividieron los features del train y valid en 2 para cada uno.
        history = model.fit((X_train_A, X_train_B), y_train, epochs=20,
                            validation_data=((X_valid_A, X_valid_B), y_valid))

        #Evaluar el modelo
        mse_test = model.evaluate((X_test_A, X_test_B), y_test)
        mse_test

        #Predicciones
        y_pred = model.predict((X_new_A, X_new_B))
        y_pred

    ```

* **`model.evaluate() `** : Nos arrojara la presición ("accuracy") del modelo entrenado.

    ```python
        loss, accuracy = model.evaluate(train_features, train_labels, verborse=2)
        # verbose = es la elección de cómo desea ver la salida de su Red Neuronal mientras se está entrenando. valores [0,1,2]
    ```

* **`model.save() `** : Guardara nuestro modelo entrenado en un archivo `.h5`

    ```python
        model.save("modelo_1_binario.h5") #El formato es ".h5"
    ```

* **`model.load_model() `** : Cargara nuestro modelo de un archivo `.h5`

    ```python
        from tensorflow.keras.models import load_model

        model = load_model("modelo_1_binario.h5")

    ```



##### Layers:

Son los tipos de capas que van dentro de los modelos

* **`keras.layers.Input()`** : **Capa de Entrada**. En el caso de trabajar con los DataFrames nuestra entrada seria el **nº de columnas** (features) que tendrá en cuenta el modelo para predecir.<br>
Es la capa de entrada y define la cantidad de columnas o features que tendra en la entrada. No tiene neuronas en la capa de entrada.
**keras.layers.Input((None,nºColumnas_df valor "X",nºfilas_df valor "Y"))**

      ```python
        #.Input = .InputLayer # Ambos hacen lo mismo.
        keras.layers.Input((None,18,)) #Dejamos la coma en "Y" indicando que puede tomar todos los valores de la fila.
        keras.layers.Input(X_train.shape[1:]), #Otra manera de lograr lo mismo. Usara las dimensiones de la data obiando la primera dimension.
        keras.layers.Input(shape=[8]), #Indicamos que tomara 8 columas o features de la tabla como entrada.

        #podemos obiar el `.input` y hacer lo mismo de la siguiente manera:
        model = keras.models.Sequential([
                keras.layers.Dense(30, activation="relu", shape=X_train.shape[1:]), #Obia la primera fila.
                keras.layers.Dense(1)
        ])


        # shape : se refiere a la forma (shape) de las características de entrada para un solo ejemplo. Se usa para especificar la cantidad de características o atributos que tiene cada ejemplo. En este contexto, la notación X_train.shape[1:] se utiliza para capturar las dimensiones de las características de entrada omitiendo el número total de ejemplos de entrenamiento.

        #Input para aplanar imagenes:
        keras.layers.Flatten(shape=[32,32,3]), #imagenes RGB de 32x32x3


    ```

* **`keras.layers.Dense()`** : **Capas Ocultas**. Son tipo de capa "Fully-Connected" lo que significa que son capas de coneccion entre la capa anterior y las que indicaremos en esta capa. Aquí podremos elegir el **nº de capas** con el **nº de neuronas** indicando su respectiva **función de activación** por capa (RELU suele ser la más usada).

    ```python
        keras.layers.Dense(64, activation="relu", kernel_initializer = "he_normal", kernel_regularizer= keras.regularizers.l2(0.01))
        #activation="relu" ; "elu" ; "selu"
        # kernel_initializer="he_normal" #Evita que se desvanesca el gradiente por la gran cantidad de capas.
        # kernel_regularizer= keras.regularizers.l2(0.01) : aplica regularizacion que indiquemos. l1, l2, l1_l2

        #para la funcion de activacion de PRelu
        keras.layers.Dense(64, kernel_initializer = "he_normal"),
        keras.layers.LeakyRelu(alpha=0.2) #Es el mejor valor para alpha

    ```
    Para Simplificar el codigo y usar varias capas con su regularizador usamos `partial`. Lo guardamos en una variable para luego armar el modelo. Podemos acceder a modificar los parametros de la variable RegularizedDense simplemente declarandolas con el nuevo valor.

    ```python
      from functools import partial
      RegularizedDense = partial(keras.layers.Dense(activation="relu", kernel_initializer = "he_normal", kernel_regularizer= keras.regularizers.l2(0.01))

      #,kernel_regularizer= keras.regularizers.l1(0.01) : Regulacion de Ridge Disminuye la importancia de las no significativas
      #,kernel_regularizer= keras.regularizers.l2(0.01) : Regulacion de Lasso Elimina variables no significativas
      #,kernel_regularizer= keras.regularizers.l1_l2(0.01) : Regulacion de Ridge + Lasso

      #Armamos el modelo incluyendo nuestra variable que hemos armado:

      model = keras.models.Sequential([
              keras.layers.Flatten(input_shape=[28,28]),
              RegularizedDense(300),
              RegularizedDense(300),
              RegularizedDense(30, activation ="softmax", kernel_initializer="glorot_uniform") #En la capa de salida modificamos de "relu" a "softmax"
              #Modificamos los parametros de la ultima capa solo declarandola con el nuevo valor.
            ])

    ```

 * Con este mismo método indicaremos nuestra capa de salida: Referenciamos nuestro target, 1 neurona por columna a predecir. Por ejemplo si nuestro objetivo era predecir el valor de una columna tendrá 1 sola neurona como resultado y si ese resultado es binario (1=si, 0=no) eligiremos nuestra **función de activación SIGMOIDE **

    ```python
        keras.layers.Dense(1, activation="sigmoid") #En caso de una salida binaria.

    ```

* **`keras.layers.BatchNormalization()`** : Toma los resultados de cada neurona y los estandariza (alinea a una distribucion normal) adcional añade una funcion lineal. Logra esto agregandose una capa intermedia por lo que acarrea costo computacional que es justibficable. Arregla el no traer lo datos normalizados, es muy util y aumenta mucho la precicion y aprendizaje del modelo.<br>
Puede reducir el tiempo de convergencia logrando asi que demore más el entrnamiento por epoca, pero requiriendo menos epocas para llegar a un Accuracy aceptable. Tambien ayuda a evitar el **Overfitting**

    ```python
        model= keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28,28]),
        keras.layers.BatchNormalization(),
        ])
        for _ in range(20):
          model.add(keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"))
          model.add(keras.layers.BatchNormalization())
          model.add(keras.layers.Activation("elu")) #Podemos especificar aquí la activación en caso de no poner el parámetro previamente.
        model.add(keras.layers.Dense(10, activation="softmax"))

        # kernel_initializer="he_normal" #Evita que se desvanesca el gradiente por la gran cantidad de capas.
    ```
##### Metodos de Regularizacion

Son metodos que tienen como finalidad de evitar el **Overfittin**. A medida que la red abarca una cantidad grande de parametros es necesario usar estos metodos.

* **Ridge & Lasso** : Podemos usar Ridge para disminuir la influencia de ciertas variables que no tengas mucha significancia pero no las elimina en comparacion de Lasso y que si las elimina o usar una combinacion de ambas. Esta regularizacion se puede combinar con el uso de `Dropout()`.<br>
Para Simplificar el codigo y usar varias capas con su regularizador usamos `partial`. Lo guardamos en una variable para luego armar el modelo. Podemos acceder a modificar los parametros de la variable RegularizedDense simplemente declarandolas con el nuevo valor.

    ```python
      from functools import partial
      RegularizedDense = partial(keras.layers.Dense(activation="relu", kernel_initializer = "he_normal", kernel_regularizer= keras.regularizers.l2(0.01))

      #,kernel_regularizer= keras.regularizers.l1(0.01) : Regulacion de Ridge Disminuye la importancia de las no significativas
      #,kernel_regularizer= keras.regularizers.l2(0.01) : Regulacion de Lasso Elimina variables no significativas
      #,kernel_regularizer= keras.regularizers.l1_l2(0.01) : Regulacion de Ridge + Lasso

      #Armamos el modelo incluyendo nuestra variable que hemos armado:

      model = keras.models.Sequential([
              keras.layers.Flatten(input_shape=[28,28]),
              RegularizedDense(300),
              RegularizedDense(300),
              RegularizedDense(30, activation ="softmax", kernel_initializer="glorot_uniform") #En la capa de salida modificamos de "relu" a "softmax"
              #Modificamos los parametros de la ultima capa solo declarandola con el nuevo valor.
            ])

    ```

* **`keras.layers.Dropout()`** : **Elimina Neuronas**. Indica el % de neuronas que se eliminaran al saltar de una capa oculta a otra. Mantendrá los valores del % de las neuronas con el fin de restar la probabilidad de Overfitting. Lo logra haciendo que cada epoca el modelo apague ciertas neuronas forzando a que no cree dependiencias de ciertas neuronas para resolver el problema, esto hace que baje su accuracy a costa de ganar un aprendizaje mas solido evitando el **Overfitting**.

    ```python
        keras.layers.Dense(64, activation="relu")
        keras.layers.Dropout(0.2) #20% de eliminación de neuronas.
        keras.layers.Dense(128, activation="relu") #Por ello aumentamos el nº de neuronas

    ```

#### Losses:

Hace referencia a la Función de Error a elegir. El cual indicará el comportamiento del modelo, como aprendió el modelo. Sirve para ver si hubo overfeeding o algún otro problema a la hora del aprendizaje.<br>

Esta es la etapa de compilación del modelo por eso usamos el método `.compile`:

```python
model.compile(optimizer="adam", #Es el más balanceado de los Optimizadores
              loss=,
              metrics=["accuracy"])
```

* **``"binary_crossentropy"``** : Para modelos de **Clasificacion Binaria** "0", "1". Dará como resultado 2 posibles valores para una columna ejemplo, sobrevive o no sobrevive.

```python
    model.compile(optimizer="adam",
                  loss="binary_crossentropy",
                  metrics=["accuracy"])
```
* **``"categorical_crossentropy"``** : (entropía cruzada categórica)  Para modelo de **clasificacion Multiclase** "Entropía cruzada categórica con objetivos enteros". Normalmente se usa cuando tenemos más de 2 categorias ejemplo para 1 de 10 posibles Categorias. Dara como resultado un % asignado a cada una de las 10 categorias señalando con la mayor probabilidad la respuesta.
```python
        model.compile(optimizer="adam",
                      loss="sparse_categorical_crossentropy",
                      metrics=["sparse_categorical_accuracy"])
```
* **``"sparse_categorical_crossentropy"``** : Para modelo de **clasificacion Multiclase**. Esta función se utiliza comúnmente en problemas de clasificación multiclase, donde cada ejemplo puede pertenecer a una única clase entre varias opciones. Ejemplos de problemas en los que se puede aplicar incluyen reconocimiento de objetos en imágenes, clasificación de documentos en categorías y muchas otras aplicaciones de clasificación. Penaliza las predicciones erróneas de manera más intensa. Si la probabilidad predicha para la clase correcta es alta, la pérdida será baja. Sin embargo, si la probabilidad predicha para la clase correcta es baja, la pérdida será alta, lo que incentiva al modelo a ajustarse de manera que aumente la probabilidad de la clase correcta.
```python
        model.compile(optimizer="adam",
                      loss="categorical_crossentropy",
                      metrics=["sparse_categorical_accuracy"])
```

* **``"mean_square_error"``** : Para modelo de **Regresion Escalar**. Este modelo busca predecir un valor continuo como un precio, temperatura de un dia en particular, etc. La arquitectura son capas Relu con una salida Relu o sin F.Activacion.
```python
        model.compile(optimizer="adam",
                      loss="mean_square_error",
                      metrics=["accuracy"])
```
* **``"SGD"``** : De manera manual podemos elegir un optimizador de `keras` configurando el **Learning Rate**.
```python
        model.compile(optimizer=keras.optimizers.SGD(learning_rate=1e-3), #lr: 0.001
                      loss="mean_square_error",
                      metrics=["accuracy"])
```

#### Entrenamiento del Modelo:

Guaramos en una variable el entrenamiento del modelo, para ejecutarlo usaremos el método `.fit()`. El objetivo es que aprenda el modelo para que con una tabla de features nueva pueda predecir los valores de la columna target.<br>
Guardamos este entrenamiento del modelo en una variable **history** para luego usar sus métodos `.history.keys()`.
**model.fit(Tabla de las columnas del df a entrenar, Tabla con la columna target, validation_split=0.2, ephocs=100)**

```python
    entrenamiento = model.fit(train_features, train_labels, validation_split=0.2, ephocs=100)
    #Entrenaremos un modelo con 100 ciclos.
    #batch_size : Indica el numero de muestras a procesar durante un "epochs"
    #validation_split : Indica que deje el 20% de los datos para validación
```
##### CallBacks

* **``ModelCheckpoint``** : Este metodo no permite ir guardando el modelo en un formato .h5 cada epoca que termine. Esto es util cuando el entrenamiento es largo o por precaucion en caso se puede cortar el entrenamiento como por un corte de luz.
```python
        #Aplicar el checkpoint
        #compilar el modelo
      model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=1e-3))

      checkpoint_cb = keras.callbacks.ModelCheckpoint("model_safe.h5", save_best_only=True)
      #,save_best_only=True : Permite guardar solo aquellas epocas en las que encontramos mejoras de presicion.

      #correr modelo
      history = model.fit(X_train,y_train,epochs=10,validation_data=(X_valid,y_valid),callbacks=[checkpoint_cb])
      #,callbacks=[variable] : Aplicamos el callback que hemos configurado y guardado en la variable. Podemos meter varias variables.
```

* **``EarlyStopping``** : Permite indicarle al modelo que deje de entrenar cuando no encuentre mejoras despues de cierta cantidad de epocas que le indiquemos. Se detendra cuando baje el accuracy o se estanque y no mejore.
```python
        early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10,restore_best_weights = True)

        #,patience=10 : Si en 10 epocas despues de no mejorar el accuracy no mejora se detendra el entrenamiento.
        #,restore_best_weights=True : Hace que el modelo se quede con el mejor score antes de bajar o estancarse y se detenga el modelo.

        #correr modelo
        history = model.fit(X_train,y_train,epochs=10,validation_data=(X_valid,y_valid),callbacks=[checkpoint_cb, early_stopping_cb])

        #,callbacks=[checkpoint_cb, early_stopping_cb] : Arrancara a entrenar con el Checkpoint anterior (en donde se quedo) y aplicara el early_stopping que hemos configurado.
```

* **``TensorBoard``** : Arroja una visualizacion interactiva de como se entrena el modelo mediante su plataforma en la web. Por ello creara 2 carpetas de **Train** y **Validation** con el que podremos visualizar en la web de Tensorborad. Es recomendable usar en el CallBack un ``ModelCheckpoint`` para guardar el modelo en un archivo `.h5` ya que los archivos que crea son solo para `TensorBoard`.
```python
        #Creamos una ruta incluyendo una nueva carpeta en nuestro directorio actual para que se cree las carpetas de TensorBoard

        import os
        import datetime

        root_logdir = os.path.join(os.curdir,"my_logs") #Creamos la ruta de la carpeta "my_logs"

        #Creando función para establecer nomencltarura
        def get_run_logdir():
            import time
            run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S") #Creamos el nombre de la carpeta que contendra el Train y Validation
            return os.path.join(root_logdir, run_id)

        #Guardamos el path en una variable
        run_logdir = get_run_logdir()
        run_logdir #Tenemos el path de la ruta my_logs con nombre con fecha y hora para guardar este nombre como un .h5

        #Creamos los archivos para TensorBoard con la ruta donde se alojaran.
        tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)

        #Corrermos el modelo con su checkpoint (.h5) y tensorboard. Ambos Callbacks
        history = model.fit(X_train, y_train, epochs=30,
                            validation_data=(X_valid, y_valid),
                            callbacks=[checkpoint_cb, tensorboard_cb])
```
Una vez se haya terminado de entrar el modelo necesitamos ver la grafica en la web de tensorflow mediante el puerto 6006 (ejemplo).
Por lo que necesitamos primero abrir el terminal situarnos en la carpeta y correr el siguiente comando:
```
cd "C:\Users\Foster-PC\Documents\Visual Studio Code\Learning\Python\A2 Capacitaciones Curso\DEEP LEARNING"

tensorboard --logdir=Nombre_Carpeta --port=6006

#Para visualizar en el buscador hacemos click en el vinculo que nos indica en el terminal
http://localhost:6006/

#Para Cerrar la coneccion al puerto tecleamos:
CTRL+C
```

___
### Evaluación del Modelo:

Nos arrojara la presición ("accuracy") del modelo entrenado.

```python
    loss, accuracy = model.evaluate(train_features, train_labels, verborse=2)
    # verbose = es la elección de cómo desea ver la salida de su Red Neuronal mientras se está entrenando. valores [0,1,2]

    #Otra manera de ver los resultados: arroja [loss, accuracy]
    model.evaluate(x_test,y_test)
```

### Gráfica del aprendizaje:

Evaluaremos gráficamente como aprendió nuestro modelo. Verificaremos que el "model accuracy" este lo mas cercano a 1 y en contra parte el "model loss" tiene que estar lo mas cercano a 0. Teniendo como referencia el accuracy que nos arrojo el método ``model.evaluate()``.

El método "fit" nos devuelve un diccionario que guardamos en nuestra variable **"history"** con los resultados de la entrenamiento. El **``"val_loss"``** y el **``"val_accuracy"``** son para los datos de **validación** que hemos indicado con el parámetro de **``validation_split=0.2``**, el cual usa el 20% de los mismos datos para evaluar y arrojarnos el **``loss``** y **``accuracy``**.

Con esto se busca evitar el **overfitting** (Sobre-Entrenamiento) para que el modelo no memorize los resultado, sino aprenda. Esto suele suceder cuando cargamos con demasiados **Ephocs** al entrenamiento. En caso contrario de poner muy poco **Ephocs** tendríamos lo que se conoce como **underfitting**


* **``"entrenamiento.history"``** : Las ventajas de `.fit()` es que nos trae el historico de las 4 metricas (loss', 'accuracy', 'val_loss', 'val_accuracy') del entrenamiento en el parametro `.history` con el cual podemos graficar.

```python
    entrenamiento.history.keys() #arroja el nombre de las columnas de la evaluación: (["loss","accuracy","val_loss","val_accuracy"])
    import matplotlib.pyplot as plt

    #Model Accuracy
    plt.plot(entrenamiento.history["accuracy"])
    plt.plot(entrenamiento.history["val_accuracy"])
    plt.title("Model Accuracy")
    plt.ylabel("Accuracy")
    plt.xlabel("Epoch")
    plt.legend(["Train","test"], loc="upper left")
    plt.show()

     #Model Loss
    plt.plot(entrenamiento.history["loss"])
    plt.plot(entrenamiento.history["val_loss"])
    plt.title("Model Loss")
    plt.ylabel("Loss")
    plt.xlabel("Epoch")
    plt.legend(["Train","test"], loc="upper left")
    plt.show()

    #Otra manera de graficar usando PANDAS.

    pd.DataFrame(entrenamiento.history).plot() #Graficara las 4 variables del parametro .history
    plt.grid()
    plt.gca().set_ylim(0,0.12)
    plt.show()

```
### Predicción

Tras tener nuestro modelo entrenado podemos cargarle nuevos datos al modelo según los campos de entrenamiento para que nos prediga nuestra columna target.

```python
    predictions = model.predict(test_features) #variable con nuevos valores de los campos de entrenamiento.

```

### Exportar Modelo

Guarda la topologia (arquitectura) del modelo de la red neuronal.

```python
    model_json = model.to_json()  #Guardamos en una variable el modelo en formato "json"
    with open ("fashion_model.json", "w") as json_file:  #Nombramos el archivo json y elegimos modo "w".
      json_file.write(model_json)

```

Exportara los pesos de la red neuronal (Red Weights) entrenado en un archivo `.h5` con el fin de no volver a gastar recursos para volver a entrenar.

```python
    model.save("modelo_1_binario.h5") #El formato es ".h5"
```
Para cargar un modelo:

```python
    from tensorflow.keras.models import load_model

    model = load_model("modelo_1_binario.h5")

```
### Conectar con Tensorflow

Nuestros **Hiperparametros** en **Deep Learning**  pueden ser cantidad de capas, neuronas y demas parametros a la hora de construir el modelo vamos a tener que englobarlo dentro de una funcion para poder pasarlo a `sklearn`. Pasaremos nuestro modelo en un regresor para que **Sklearn** lo pueda leer y aplicar un **Gridsearch()** o **RandomizedSearchCV()** y probar diferentes combinaciones de neuroas, capas, etc en el modelo.

Para poder pasar el modelo de `keras` a un modelo regressor o classifier en ``sklearn`` instalamos la libreria `scikeras`

```python

pip install scikeras

```

```python

  from scikeras.wrappers import KerasClassifier, KerasRegressor

  keras.backend.clear_session() #Limpiamos la sesion.

  #definir función que construirá nuestro modelo. Segun estas variables iran en el GridSearch
  def build_model(n_hidden=1,n_neurons=30,learning_rate=3e-3,shape=[8]): #Tomara solo 8 columnas o features de la tabla.
      model = keras.models.Sequential()
      model.add(keras.layers.Input(shape=shape)) #Establece la capa de entrada con 8 features de input.
      for layer in range(n_hidden):   #del 0 hasta n-1 debido a que cuenta el 0. Iteramos entre la cantidad de capas
          model.add(keras.layers.Dense(n_neurons,activation="relu")) # 30 neuronas por capa con función "relu"
      model.add(keras.layers.Dense(1)) #Agregamos la neurona final.

      model.compile(loss="mse", optimizer = keras.optimizers.SGD(learning_rate=learning_rate))
      return model

  #Convertir a un objeto de Scikit
  keras_reg = KerasRegressor(build_model, n_hidden=1,n_neurons=30,learning_rate=3e-3,shape=[8])
  #Es necesario volver a poner los parametros iniciales de la funcion build_model para que lo tenga como default.

  #Entrenar el modelo
  keras_reg.fit(X_train, y_train, epochs=100,validation_data=(X_valid, y_valid),callbacks=[keras.callbacks.EarlyStopping(patience=10)])

  #Usar con GridSearch para correr el modelo con diferentes neuronas
  from scipy.stats import reciprocal
  from sklearn.model_selection import RandomizedSearchCV

  # GridSearch
  param_distribs={
      "n_hidden":[0,1,2,3],
      "n_neurons": np.arange(1, 100),
      "learning_rate": reciprocal(3e-4, 3e-2) #Genera aumentos aleatorios de numeros pequeños para este intervalo.
    }

  #Usamos RandomizedSearch para búscar de manera más eficiente y aleatoria en el espacio de hiperparámetros utilizando.
  rnd_search_cv = RandomizedSearchCV(keras_reg, param_distributions=param_distribs, n_iter=10,cv=3,verbose=2)

  #Entrenamos el modelo dentro de la rejilla (RandomizedSearchCV).
  rnd_search_cv.fit(X_train, y_train, epochs=100,
                  validation_data=(X_valid, y_valid),
                  callbacks=[keras.callbacks.EarlyStopping(patience=10)]) #Que salte a los siguientes parámetros de la rejilla si no mejora

  #ver cuáles son los mejores parámetros
  rnd_search_cv.best_params_

  #mejor puntaje
  rnd_search_cv.best_score_

  #Ver el mejor estimador
  rnd_search_cv.best_estimator_  #En caso de haber probamo con varios estimadores.

  # Guardamos el modelo con la mejor configuracion de la rejilla en una variable.
  # El metodo ".model" Trae una funcion de sklearn y ".model_" trae para keras.

  model = rnd_search_cv.best_estimator_.model_    #Pasamos a Keras.

  #Vemos la estructura con los mejores parametros.
  model.summary()

  #Evaluación
  model.evaluate(X_test,y_test)


```

## [22. camelot](#indice)

Es una libreria para extraer cuadros de los PDF.

**pip install camelot**

pip install PyPDF2==2.0.0
pip install 'PyPDF2<3.0'

En caso falte alguna libreria podemos instalarlar como en el siguiente ejemplo.

**pip install opencv-python**

Es importante tener instalado el Ghostscript 10.0 (ultima version) en la pagina: [Ghostscript](https://ghostscript.com/releases/gsdnld.html)

```python
import camelot
```

### Metodos

* **``".read_pdf"``** : Carga un archivo PDF a la variable que le asignemos.
  ```python
    table = camelot.read_pdf("archivo.pdf", pages="1", flavor="lattice")
      #flavor= Indica el metodos que usara para extraer las tablas, por defecto usa "lattice"
  ```

* **``".export"``** : Exporta la tabla en la variable que guardamos la tabla.

  ```python
      table.export("archivo.csv", f="csv", compress=True) #Guarda el CSV en la variable
      table[0].to_csv("archivo.csv") #Exportamos a un Archivo csv en nuestro directorio.
  ```

## [23. tabula](#indice)

Es una libreria para extraer cuadros de los PDF el cual es mas rapida que "camelot". Pero es necesario tener instalado [java](https://www.java.com/en/download/) y agregarlo al PATH de las variables de entorno.<br><br>

Para Instalarlo usamos:

```
  pip install tabula-py
```
<br><br>
Para importarlo usamos:

```python
  from tabula import read_pdf
  from tabulate import tabulate

```
<br><br>
Para extraer las tablas de un archivo pdf:

```python
  #Extraer las tablas que contiene la pagina en una lista de dfs. Por lo que lista[0] es un DataFrame.
  tabla = read_pdf(f"{Ruta_Actual}/{i}.pdf", pages="all",stream=False, lattice=False)

  #stream=False, lattice=False : Nos ayudara a precisar la lectura de la tabla.

  try:
        Ultima_col = tabla[0].tail(3)[tabla[0].columns[-1]] #Seleccionamos la ultima columna de la primera tabla.
        lista_df = [float(i.translate(str.maketrans("","",",PENUSDIGVTOTALSUBCargo")).replace("18%","")) for i in Ultima_col]
    except Exception:
        None
```

## [24. geopandas](#indice)

Es una libreria para hacer graficas georeferenciales segun la coordenada que tengamos en el df.

**pip install geopandas**

Instala o comprueba 5 dependencias : fiona, packaging, pandas, pyproj, shapely<br><br>

### Elementos:

- Todos los archivos geopandas (.shp) contienen una columna **geometry**, el cual guarda la geometria del mapa que estamos analizando. Con esta geometria podemos sacar perimetro, distancia y más datos con los metodos de gpd.<br>
Considerar que podemos cambiar nuestra geometria de referencia.
- El **Codigo_Dane** son números ("05001000") de geolocalizacion o espacial que nos ayuda a ubicarnos en el mapa. En caso no tengamos la columna **geometry**, esta nos ayuda a poder realizar la grafica de geolocalizacion. Nos ayuda a poder hacer un **merge** con el `.shp` donde esta la columna **geometry** usando la columna **Codigo_Dane**.

### Clase, Funciones:

La mayoria de las funciones y metodos no hacen necesario referenciar la columna con la que vamos a georeferenciar debido a que por defecto operan sobre la columna **geometry**. Podemos cambiar la columna referencial con informacion espacial, en caso de tener varias, con `.set_geometry()`.

* **``".overlay()"``** : Intersecta 2 graficas o archivos `.shp` separados los cuales no se les ha hecho ningun `.sjoin()` o `merge()` haciendo el cruce de las graficas por medio de su columna **geometry**. Considerar que las graficas a cruzar tienen el mismo `.shape`.

    ```python
      antioquia = mapa_departamentos.query("Dep_nombres == 'Antioquia'") #Filtramos el departamento que queremos de todo el mapa que trae la grafica.

      rios_antioquia = gpd.overlay(antioquia, rios, how="intersection") # intersectamos grafica A con B.
      rios_antioquia.plot(figsize=(20,12))

    ```

### Metodos:

* **``".read_file()"``** : Carga un archivo ya sea ``.shp``. El cual es una tabla de pandas, pero con el metodo `.plot` podremos ver la grafica del boceto del mapa.

    ```python
      #Leer archivos directamente del token: abrir el raw del archivo en Github y copiar el URL.
      londres_map = gpd.read_file("https://github.com/a2Proyectos/MachineLearning_Data/raw/cf64ed6f29a15105cdf60bdca95352d056fe13af/Capitulo_2/map/London_Borough_Excluding_MHW.shp")

      londres_map.head()
      #Este token con el tiempo vence y cambia por lo que tendremos que actualizar el URL
    ```
    La mejor Manera es usando 2 variables de path y extension de ubicacion del archivo. Es importante que este modo solo funciona para **Repositorios Publicos**.<br>
    Transformamos el original URL al siguiente :<br>
    ``https://github.com/a2Proyectos/MachineLearning_Data/tree/main/Capitulo_2``<br><br>

    **``https://raw.githubusercontent.com/UsuarioGit/Carpeta/main/``**

    ```python

    DOWNLOAD_ROOT = "https://raw.githubusercontent.com/a2Proyectos/MachineLearning_Data/main/"
    #Complementos con la dirección especifica de la base de datos que queremos.
    MEDICAMENTOS = "Capitulo_3/drug200.csv"

    def extraer_datos(root,database):
        csv_path = root + database
        return pd.read_csv(csv_path) #dtype_backend='pyarrow'
    #Complementos con la dirección especifica de la base de datos que queremos.

    df = extraer_datos(DOWNLOAD_ROOT,MEDICAMENTOS)
    df.head()

    ```

* **``".plot()"``** : Muestra la grafica del mapa. Este metodo es propio de **pandas** y **geopandas**, a diferencia de usar **matplotlib**.

    ```python
      #Podemos combinar graficas de la siguiente manera:
      axis = colegios_antioquia.plot(figzise=(20,12)) #Marcamos como lienzo base (axis)
      colegios_afectados.plot(ax= axis, color="red") #muestra solo colegios con el overlay() de rios (con buffer) y colegios.
      rios_antioquia.plot(ax=axis, color="black") #citamos `,ax=axis` para graficar en el mismo lienzo.

    ```

* **``".crs()"``** : (Sistema de Referencia) Muestra detalle de la grafica y tambien sus medidas (metros, grados, etc)

* **``".buffer()"``** : Extiende el grosor y no longitud de la grafica. Es usado para referenciar un elemento en el mapa y extender su tamaño. Para ellos validamos con el metodo `.crs` las unidades de medida de la grafica.

    ```python

      # Aumentamos en 500 metros el grosor de los rios en la grafica ya que el crs esta en metros.
      rios_antioquia["buffer"] = rios_antioquia.buffer(500) #creamos un columna que aloja el calculo por fila de la geometria.
      rios_antioquia["buffer"].plot(figsize=(20,12))

    ```

* **``".set_geometry"``** : Indicamos la columna con informacion espacial que usara como **geometry** para mostrar la grafica. Esto se suele usar en el caso que en el df tengamos varias columnas con informacion espacial (**geometry**).<br>
Esto tambien es importante antes de hacer alguna operacion entre graficas como `.overlay()` ya que tomara la columna con informacion espacial que hayamos seteado.

    ```python

      rios_antioquia = rios_antioquia.set_geometry("buffer") #Indicamos el nombre de la columna "geometry".
      rios_antioquia.plot(figsize=(20,12))

    ```
## [25. polars](#indice)

Es una Libreria de analisis de datos basado en RUST con optimizacion en paralelo, basado en una sintaxis de SQL. Este no usa indices como `pandas`.<br>
Posee 2 modos de ejecucion de DataFrames (Eager) y LazyFrames (Lazy), el cual el primero ejecuta al instante el codigo mientras el segundo guarda en memoria optimiza la consulta y la ejecuta con el metodo `.collect()` el cual lo hace mas rapido.

```python
  !pip install polars

  import polars as pl
  pl.Config.set_tbl_width_chars(200) #Hace que no sean tan estrechos los df.
  pl.Config.set_fmt_str_lengths(50) #Cantidad de caracteres a mostrar de los valores
  pl.Config(thousands_separator=True,float_precision=2)
```

Para cargar y exportar archivos:

  * archivos ´.csv´
    ```python
    # Modo Eager:
    df = pl.read_csv(path, separator=";", encoding="utf8", ignore_errors=True, try_parse_dates=True) #Se puede comvertir a LazyFrame usando .lazy()

    # Modo Lazy o Diferido:
    df = pl.scan_csv(path, separator=";", encoding="utf8", ignore_errors=True, try_parse_dates=True) #LazyFrame

    ```
  * archivos ´.xlsx´
    ```python
    # Modo Eager:
    df_pl = pl.read_excel(path,sheet_name="DATA",engine="openpyxl",schema_overrides={"dt": pl.Date, "OC": pl.Utf8})

    #,engine= : El modo "calamine" es el mejor optimizado. (pip install fastexcel)
    #,schema_overrides={"dt": pl.Date} : Podemos setear el tipo de columnas al cargar.

    ```
  * sobrescribir ´.xlsx´
    ```python
    # Modo Eager:
    df_pl = pl.write_excel(path, worksheet="Resumen",table_name="DB", table_style="Table Style Medium 4", autofit=True)

    #,worksheet="Resumen" : Nombra la pestaña del Excel
    #,table_name="DB" : Nombra y convierte la data en formato trabla.
    #,table_style="Table Style Medium 4" : Elegimos un estilo de tabla.
    #,autofit=True : Autoregula el tamaño de las celdas.

    #LazyFrame usando .lazy()
    ```

### Clases:

* **``pl.Series()``** : Crea una Serie o columna. No se puede hacer `.filter()`, para ello tenemos que pasar a Dataframe (2 dimensiones) con `.to_frame()`
  ```python
    pl.Series("New Column", [1, 2, 3], dtype=pl.Categorical)

    #Añadiendo en caliente una nueva columna a la vista
    df.select(pl.col("Pedido").head(2), #Para que tengan el mismo tamaño de filas.
              new_col = pl.Series(lista_1, dtype=pl.int16))

    # Añadir una columna que se repirta por el tamaño de un df
    pl.Series("ACTIVIDAD", ["REFORMAS"] * len(df_pl_R))

    df_pl_R.insert_column(0, pl.Series("ACTIVIDAD", ["REFORMAS"] * len(df_pl_R)));

  ```

* **``pl.DataFrame({})``** : Crea una DataFrame de un diccionario dado.
* **``pl.LazyFrame({})``** : Crea una LazyFrame de un diccionario dado. Para poder visualizarlo usamos `.collect()`
  ```python
    df = pl.DataFrame({
                          "Tamaño": [10, 2,5,1,9],
                          "Resultado": [1, 0, 1, 0, 1]
      },schema={"Tamaño": pl.Float32, "Resultado": pl.Int64}) #Con su respectivo formato

    df = df.collect(streaming=True) #Con collect() ejecutamos la consulta guardada en la variable
    #streaming=True : Procesa y almacena solo el calculo en ram. Lee y procesa en batches la información (es mas eficiente para grandes db)

  ```
* **``pl.from_pandas()``** : Convierte un Df de pandas a DF de polars.
  ```python
    tabla_pl = pl.from_pandas(tabla_prueba[0],nan_to_null=True, include_index=False)

    # nan_to_null=True : Converite los valores Nan en Null de polars
    # include_index=False : Por defecto viene en false para no incluir el clasico index de PANDAS

  ```


### Metodos:

* **``df.with_columns()``** : Referencia el df actual con todas sus columnas arrojandolo como output.
  ```python
    df_pl.with_columns(pl.Series("New Column", [1, 2, 3])) #Añade una columna nueva al df actual.

  ```
* **``df.select()``** : Crea una tabla aparte en el que referencia al dataframe en la variable "df"
  ```python
    df_pl.select(pl.col("Nombre").alias("New_Name")) #Crea una nueva columna con datos de otra columna. Este modo es mas eficiente
    df_pl.select(pl.col("^deca.*$")) #con "^ $" apertura el modo regex. El punto "." tomara todo lo restante.
    df_pl.select("before", pl.struct(pl.col("^t_.$")).alias("t_struct"), "after") # Si no operamos ponemos el nombre de la col.
    df_pl.select(pl.col("Valor"))[0,0] #Obtenemos solo el valor de la primera columna similar a `.value` en pandas.
    df_pl.select(pl.col("Valor"))[:,0].to_list() #convertimos a un array y pasamos los valores a una lista.

    df_pl.select( New_Name =   pl.col("^deca.*$")) #con "^ $" apertura el modo regex. El punto "." tomara todo lo restante.
    df.select(pl.col(pl.List(pl.UInt32))) #Mostramos solo las columnas Tipo Lista que contengan enteros 32.
    df_pl.select(pl.all(ignore_nulls=True).exclude(pl.Utf8,pl.Date))#.describe()

    #Convertimos la lista a 1D enlistamos e iteramos. Eliminando las listas vacías y duplicados.
    [i for i in big_table.select(pl.col("Guías").str.split(","))[:,0].to_list()] #Lista con valores vacios y duplicados
    print(i) #La comprehension e iteracion elimina listas vacias y duplicados
  ```
* **``df.filter()``** : Permite la visualizacion de una tabla filtrada teniendo el contexto de toda la tabla en df. Debido a que interpreta los valores booleanos podemos usar "~" para negar y obtener el resultado contrario a la preposicion.<br>
Solo filtra dataframes 2 dimensiones, no funciona con Series.<br>
Para negacion podemos usar el simbolo `!=` o usar el metodo `not_()` al final de la expresion que arroja booleano.
  ```python
    df.filter(pl.col("city")=="New York" & pl.col("A")) #arroja Booleanos, para visulizarlo usamos .filter()

    df.filter(pl.col("a") % 2 == 1, pl.col("b") % 3 == 0) #Otra manera de enlazar 2 filtros.

    #Arroja los valores no duplicados de la columna indicada
    df.filter(~pl.col(['Tipo de Orden']).is_duplicated())
    df.filter(pl.col(['Tipo de Orden']).is_duplicated().not_()) #Negacion nativa de polars

    #Arrojara una tabla con 3 columnas manteniendo el primer filtro "New York" (Filtra, selecciona y añade columna)
    df.filter(pl.col("city")=="New York").select(pl.col(["Latitud","Longitud"])).with_columns(pl.col("Price").log1p().suffix("_log1p"))

    #Podemos pasarle una lista para filtrar: ".is_in()
    df.filter(pl.col(['Tipo de Orden']).is_in(["52","53","55"]))
    df.filter(pl.col(['Tipo de Orden']).is_in(["52","53","55"]).not_()) #invierte, arroja la negacion.

    #Otra manera de filtrar usando una lista de numeros para que nos arroje varias filas.
    lista_codigos = [5114190,5114192,5114193,5021165,6512215,5642245] #Valores numericos

    lista_codigos_str = list(map(str, lista_codigos))  #Convierte una lista a string ya que la columna es str.
    lista_codigos_str

    tabla_pl.filter(pl.col("CÓDIGO").is_in(lista_codigos_str)) #Filtramos segun los valores contemplados en la lista
    df_date.filter(pl.col("time").is_in(pl.Series("fechas",lista_fechas,pl.Utf8).str.to_date("%Y-%m-%d")))

    # .is_between() Ejm: Filtro entre 2 fechas
    df.filter(pl.col("Date").is_between(datetime(1995,7,1),datetime(1995,11,1))) #Tomas las fechas mas proximas a la data.

    #Fechas: Usamos el metodo ".dt" de las columnas tipo fecha para filtrar por año.
    df.filter(pl.col("Date").dt.year() < 1300)

    #Filtrar fechas en especifico:
    df_date.filter(pl.col("time") == pl.date(year = 2021, month= 1, day=31))

    df_date.filter(pl.col("time").is_in(pl.Series("fechas",lista_fechas,pl.Utf8).str.to_date("%Y-%m-%d")))

  ```
* **``df.rename()``** : Cambia el nombre de las columnas indicadas.
  ```python
    df.rename({"COL_1": "Nombres"}) #Ya lo sobreescribe

    #Renombramos las columnas con los valores de la primera fila (Ordenando Data)
    col_name = df_prueba.head(1).transpose().to_series().to_list() #convertimos la primera fila en una lista
    name_dict = dict(zip(df_prueba.columns,col_name)) #Creamos un diccionario clave-valor de las 2 listas.
    df_prueba.rename(name_dict) #renombramos

  ```
* **``df.insert_column(nº_index, pl.Series("nombre",Lista))``** : Inserta una columna con la data (pl.Series o Lista) en la posicion que le indiquemos desplazando hacia la derecha la columna con la posicion a sustituir.
  ```python
    column = pl.Series("nombre_col", [97, 98, 99])
    df.insert_column(-1,column) #(posicion, Serie o Lista), "-1" : index

    #Creamos un col parte del de la tabla total para que se contemple las filas repetidas.
    for i in Listas:
      main_table = main_table.insert_column(0,pl.Series("Factura",
                main_table.with_columns(Factura = pl.lit(os.path.splitext(os.path.basename(i))[0]))["Factura"].to_list()))

    # Insertamos una nueva Columna aplicando previamente una operacion.
    Tipo_concreto = {263:"1175N57B"}
    big_table.insert_column(8,pl.Series("Tipo_Concreto",
                    big_table.select(Tipo_Concreto = pl.col("V. Unitario").replace(Tipo_concreto, default="")).select("Tipo_Concreto")[:,0].to_list())

    #Insertando col de fecha:
    big_table.insert_column(6,pl.Series("F_Emision",
                            big_table.select(pl.col("Fecha de Vencimiento") - pl.duration(days=pl.col("Forma de Pago")))[:,0].to_list()))

    #Insertar una columna con una cadena string que se repita por la cantidad de filas del df.
    df_pl_R.insert_column(0, pl.Series("ACTIVIDAD", ["REFORMAS"] * len(df_pl_R)))

  ```
* **``df.with_row_index()``** : Genera automaticamente una columna index al inicio del df de tipo U32 empezando la numeracion desde el 0.
  ```python
    df = df.with_row_index()
  ```
* **``df.slice()``** : Hace la seleccion de intervalo de filas que le indiquemos: (index de inicio, cantidad de filas)
  ```python
    #Mostrara el df a partir de la segunda fila hasta 5 filas hacia delante.
    df.slice(1,5)

    #Otra manera de lograr lo mismo:
    df_prueba[1:5,:] #[filas, columnas]

  ```
* **``df.cast()``** : Cambia el formato de las columnas. `pl.Utf8` , `pl.Categorical` `pl.UInt64` , `pl.Float64`, `pl.Date`, `pl.Time`, `pl.Datetime`, `pl.Duration`.<br>
  Las columnas `pl.Categorical` el unico beneficio que dan es **Ahorro de memoria, mejor rendimiento y representacion clara**.
  Tambien hay columnas tipo lista que agrupa adentro a pl.Utf8, pl.UInt64, etc. Estos son: ``pl.List(pl.UInt32)``
  Para pasar de una columna que ya leyo como "str" a tiempo usamos el metodo `.str.to_date()` y viceverza con `dt.to_string()`
  ```python
    #Podemos tomar una lista de columnas y todas asignarlas a un tipo de columna.
    df.with_columns(pl.col("Posición").cast(pl.UInt16, strict= False)
                    pl.col(["Documuento compras","Posicion","Almacén","Grupo de artículos"].cast(pl.Utf8),
                    pl.col("dt").cast(pl.Datetime)) #Solo lo logra si viene en el formato "%Y-%m-%d"

    )
    # strict= False : Convertira a los valores que se desbordan (int8), los que arrojen error o no cumplan los convierten en nulos. Encambio en modo "True" arrojara error de haberlo.

    #Convertirmos los valores que arroja error para convertir en nulos, para luego convertirlos en 0 con fill_null(0)
    pl.col("A").str.replace_all(r"[ ,]","").cast(pl.Int64,strict=False).fill_null(0) #Quita "," y \s
  ```
* **``.str()``** : Afecta solo a columnas de tipo "str" (Utf8). Activa todos los metodos para manipular strings.
  ```python
    df = df.with_columns(pl.col("Fecha documento").str.to_date("%d/%m/%Y"), #Convertimos a fecha corta segun como este el texto.
       pl.col("dt").str.strptime(pl.Datetime) #Detecte en automatico, tambien se logra con `.cast()`
       pl.col("Por entregar").str.replace(",","").cast(pl.Float32) #De string a float32 quitamos la ","
       pl.col("Por entregar").str.replace(r"\0.$+","") # Quita la primera coincidencia con el texto o fila.
       pl.col("Por entregar").str.replace_all("a","-") # Quitas todas las coincidencias de un mismo texto
       pl.col("Por entregar").str.replace_all(r'[,$PENUSDIGVTOTALSUBCargos]|18%', '') # Quitas todas las coincidencias en cada fila
       pl.col("A").str.replace_all(r"[ ,]","").cast(pl.Int64,strict=False).fill_null(0) #Los valores que arroja error los convierte en nulos para luego convertirlos en 0 con fill_null(0)
       pl.col("Por entregar").str.replace_many([r'[,$PENUSDIGVTOTALSUB]',"Cargos" ,"18%"], '') # Quitas todas las coincidencias de lo indicado en la lista.
       pl.col("Por entregar").str.extract(r'(\d+)').alias("F_Entrega") #Extraera las primeras coincidencia de cada fila
       big_table.select(pl.col("Guías").str.split(","))[:,0].explode().str.extract(r'(\d{3}-\d{7})').drop_nulls().sort().n_unique()
    )

  ```
  * **``str.split()``** : Hace cortes  en cada elemento mediante el caracter que indiquemos, arrojando una lista como output.
    ```python
    # Crear un DataFrame de ejemplo
    data = {'Nombres': ['John Doe', 'Jane Smith', 'Bob Johnson']}
    df = pl.DataFrame(data)

    # Dividir la columna 'Nombres' en dos columnas usando el espacio como delimitador
    df_split = df.with_column(
    pl.col("Nombres").split(" ", n=1).alias(["Nombre", "Apellido"]) #n=1 : hara solo un corte por fila.

    weather.with_columns(pl.col("temperatures").str.split(" ") #Crea una lista por cada fila.
    .list.eval(pl.element().cast(pl.Int64, strict=False).is_null()) #Con ".list.eval(pl.element())" iteramos en cada valor
    .list.sum().alias("Suma de errores"), # Sumamos todos los valores nulos e indicamos como columna "Suma de errores".

    # Ejemplo 2:
    pl.col("temperatures").str.split(" ") # Segunda Columna
    .list.eval(pl.element().str.contains("(?i)[a-z]"))
    .list.sum().alias("errors")
    )
    ```
* **``.dt()``** : Afecta solo a columnas de tipo "date ; time y datetime". Activa todos los metodos para manipular fechas.<br>
  Considerar el siguiente esquema para los formatos:
  * **``%A``** : (dia) Nombre del dia: "Monday", "Tuesday", etc.
  * **``%d``** : (dia) Día del mes como un número decimal relleno con ceros. Ejemplo: 01, 02, ..., 31.
  * **``%m``** : (mes) Numero del mes abreviado del 1 al 12. (int)
  * **``%b``** : (mes) Nombre del mes abreviado. Ejemplo: Jan, Feb, ..., Dec.
  * **``%B``** : (mes) Nombre completo del mes. Ejemplo: January, February, ..., December.
  * **``%Y``** : (Año) con siglo como un número decimal. Ejemplo: 0001, 0002, ..., 2013, 2014, ..., 9998, 9999.
  * **``%y``** : (Año) sin siglo como un número decimal relleno con ceros. Ejemplo: 00, 01, ..., 99.
  * **``%H``** : (Hora) (reloj de 24 horas) como un número decimal relleno con ceros. Del 01 al 24. (int)
  * **``%I``** : (Hora) (reloj de 12 horas) como un número decimal relleno con ceros. Del 01 al 12
  * **``%p``** : (AM/PM) En conjunto con `%I` permite indicar entre am y pm: ``%I:%M %p`` -> "01:45 PM"
  * **``%M``** : (Minuto) como un número decimal relleno con ceros. Ejemplo: 00, 01, ..., 59.
  * **``%S``** : (Segundo) como un número decimal relleno con ceros. Ejemplo: 00, 01, ..., 59.
<br><br>
  ```python
    #Convertimos a str con el formato que queramos. dt.to_string() == dt.strftime()
    df = df.with_columns(pl.col("Fecha documento").dt.to_string("%m-%d"))

    #Podemos mostrar solo los años o cualquier periodo que seleccionemos de una columna en formato dt. Arroja en numerico
    # .year(), .month(), .week(), .day(), weekday({0-6}), .hour(), .minute(), .second(), etc.
    df = df.with_columns(pl.col("Fecha documento").dt.year())

    #Convertir de datetime ("%y-%m-%d %H:%M:%S") a solo date ("%y-%m-%d")
    df = df.with_columns(pl.col("Creado El").dt.date())
    df.select(pl.col("Creado El").dt.strftime("%y-%m-%d").str.to_date("%y-%m-%d")) #de str a date.

    # Mostrar nombres del mes de manera ordenada
    df = df.with_columns(pl.col("Creado El").sort().dt.to_string('%B').alias('Nombre_Mes'))

  ```
  Operaciones con Fechas:
  ```python
  #Usando timedelta
  from datetime import timedelta

  big_table.select( pl.col("Fecha de Vencimiento") - timedelta(days=5)) #arroja la resta en formato pl.Date

  #Tambien podemos usar "pl.duration()" en vez de "timedelta".
  big_table.select(pl.col("Fecha de Vencimiento") - pl.duration(days=5))

  ```
* **``pl.duration()``** : Se utiliza para operaciones con **tiempo**, ya sea sumas, restas ,etc. Tambien podemos indicarle el nombre de una columna para que opere con otra de tipo **Tiempo**.
  ```python
    #Restamos un dia en especifico como una columna int|str
    big_table.select(pl.col("Fecha de Vencimiento") - pl.duration(days=5))
    big_table.select(pl.col("Fecha de Vencimiento") - pl.duration(days="Forma de Pago"))

    #Parametros:
    #,weeks= ,days= ,hours= ,minutes= ,seconds= ,milliseconds= ('us') ,microseconds= ('ms') ,nanoseconds= ('ns')

  ```
  * **``dt.add_business_days()``** : Hace operaciones considerando quitar los sabados, domingos o considerando vacaciones en medio de 1 periodo de fecha.
  Podemos configurar esto de la siguiente manera:
  ```python
  #Crea una Columna Resulta añadiendole 5 dias considerando solo lunes a viernes a cada fila del df creado.
  from datetime import date
  df = pl.DataFrame({"start": [date(2020, 1, 1), date(2020, 1, 2)]}) #Creamos df con 2 fechas.
  df.with_columns(result=pl.col("start").dt.add_business_days(5))

  # De esta manera configuramos para que considere tambien los sabados, y haga un skip en el periodo de vacaciones al momento de contar los dias.
  week_mask = (True, True, True, True, True, True, False)
  holidays = [date(2020, 1, 3), date(2020, 1, 6)]
  df.with_columns(result=pl.col("start").dt.add_business_days(5, week_mask, holidays=holidays))

  ```

* **``pl.read_clipboard()``** : carga un df del portapapeles.
  ```python
    pl.write_clipboard(separator:",")
  ```
* **``pl.write_clipboard()``** : Sobrescribe en el portapeles.
  ```python
    pl.write_clipboard(separator:",")
  ```
* **``.describe()``** : Arroja una resumen de valores estadisticos de las columnas. No arroja error si hay str o dates en las columnas.
  ```python
    #Arrojamos un Describre de solo los valores numericos.
    df.select(pl.all().exclude(pl.Utf8,pl.Date)).describe()

  ```
* **``.sort()``** : Ordena una columna por modo ascendente por defecto. Para ciertas operaciones es necesario tener el df ordenado.
  ```python
    df.sort("missing",descending=True)

  ```
* **``df.value_counts()``** : Arroja una cuenta de cada valor unico en la columna. Arroja el conteo de cuantas veces se repite cada clase unica encontrada en la columna.
  ```python
    import cufflinks as cf
    #Modo largo y optimizado. Pasamos a pandas para graficar en cufflinks
    df.select(pl.col("Creado Por").value_counts(sort=True)).unnest("Creado Por").head().to_pandas().set_index("Creado Por").iplot("bar")
    #De este modo arroja valores de diccionario, usamos unnest() para divir en 2 columnas separadas.

    #Modo corto
    df["Creado Por"].value_counts(sort=True).head().to_pandas().set_index("Creado Por").iplot("bar")

  ```
* **``df.replace()``** : Reemplaza valores que le indiquemos ya sea en lista o diccionario (maaping). **((Depreciado))**
  ```python
    # Insertamos una nueva Columna aplicando previamente una operacion.
        Tipo_concreto = {263:"1175N57B"}   #diccionario para mapear

        big_table.insert_column(8,
          pl.Series("Tipo_Concreto",big_table.select(Tipo_Concreto = pl.col("V. Unitario").replace(Tipo_concreto, default="")).select("Tipo_Concreto")[:,0].to_list()))

    #,default= int|str : Podemos indicar el valor que tomara para los valores que este fuera del mapping indicado.
  ```
* **``df.replace_stric()``** : A diferencia de Replace, este es más "completo", lo que hace es que en todas las coincidencias que indiquemos hara el reemplazo y en todas las demas que no coincida pondra un valor por defecto que indiquemos con: `,default=[valor]` y tambien puede cambiar el tipo de dato con `,return_dtype=pl.Float64`
  ```python
    # Insertamos una nueva Columna aplicando previamente una operacion.
        Tipo_concreto = {263:"1175N57B"}   #diccionario para mapear

        pl.col("a").replace_strict(Tipo_concreto ,default=0, return_dtype=pl.Float64)

        pl.col("a").replace_strict(1,10, ,default=0, return_dtype=pl.Float64)

    #,default= int|str : Podemos indicar el valor que tomara para los valores que este fuera del mapping indicado.
  ```
* **``df.is_null()``** : Polars asigna a los valores nulos "null" y manteniendo el tipo de columna sin afectar.
  ```python
    # Revisar datos faltantes de las columnas
    missing_data = df.select(pl.all().is_null().sum()).melt(value_name="missing").filter(pl.col("missing")>0).sort("missing",descending=True).head()
    display(missing_data)

  ```
* **``df.is_not_null()``** : Arroja los valores excluyendo los nulos en un solo paso.
  ```python
    # Revisar datos faltantes de las columnas
    missing_data = df.filter(pl.col("missing").is_not_null())

  ```
* **``df.is_nan()``** : Los valores Nan son originarios de numpy, y PANDAS al encontrar valores NAN cambia el tipo de columna a flotante. Polar puede encontrar estos valores Nan sin afectar el tipo de la columna y reemplazar los valores.
  ```python
    # Revisar datos faltantes tipo NaN de las columnas
    missing_data = df.select(pl.col(pl.Float64).is_nan().sum()).melt(value_name="NaN_missing").filter(pl.col("NaN_missing")>0).sort("NaN_missing",descending=True).head() #Especificamos que es columna float sino arroja error con las columnas str.
    display(missing_data)_columns(a = (pl.col("Pedido"))-(pl.col("Pedido").shift(1).fill_null(0))).limit(3)

  ```
* **``df.fill_null()``** : Rellena los valores faltantes encontrados ya sean NULL o NAN con el valor que le indiquemos o la estrategia con operacion a indicar, Puede ser un numero o texto.
  ```python
    #Rellenamos todos los datos faltantes de todas las columnas con 0.
    df.fill_null(0)

    #Usando una operacion de la API:
    df.fill_null(strategy="forward") #Copia los valores nulos con el predecesor, de arriba hacia abajo. ‘backward’ lo hace de abajo hacia arriba.
    #,strategy="forward" : {None, ‘forward’, ‘backward’, ‘min’, ‘max’, ‘mean’, ‘zero’, ‘one’}

  ```
* **``df.is_not_nan()``** : Arroja los valores excluyendo los nulos de numpy en un solo paso.
  ```python
    # Revisar datos faltantes de las columnas
    missing_data = df.filter(pl.col("missing").is_not_nan())

  ```
* **``df.n_unique()``** : Numero de unicos. Arroja el conteo total de valores unicos encontrados en la columna. Si hay 3 clases diferentes arroja 3 independientemente de cuantas veces se repita.
  ```python
    # Revisar cantidad de datos unicos de las columnas
    missing_data = df.select(pl.all().n_unique()).melt(value_name="n_unique").filter(pl.col("n_unique")>0).sort("n_unique",descending=True).head()
    display(missing_data)
  ```
* **``.is_unique()``** : Arroja la tabla con los valores unicos excluyendo aquellos que se repiten(no los trae). Arroja un booleano por lo que debemos pasarlo por un ``filter``
  ```python
    df.filter(pl.col("Creado Por").is_unique()).select(pl.col("Creado Por"))
  ```
* **``.unique_counts()``** : Arroja una lista de conteo de valores con las veces que se repite, junto con `.count()` podemos ejecutar el "Recuento Distinto". De igualmanera que lo hace `.n_unique()`
  ```python
    # Cuenta la cantidad de OC unicas para cada grupo de compras.
    df_pl.group_by("Grupo de compras").agg(pl.col("Documento compras").unique_counts().count())
  ```
* **``.is_duplicated()``** : Arroja la tabla con las filas donde se encuentran los valores duplicados, excluye aquellos que aparecen una sola vez. Arroja un booleano por lo que debemos pasarlo por un ``filter``
  ```python
   df.filter(pl.col("Creado Por").is_duplicated()).select(pl.col("Creado Por"))
  ```
* **``.is_first_distinct()``** : Arroja todos los primeros valores como true y todos los demas repetidos como false. Arroja aquellos valores unicos y duplicados mostrandolos una sola vez. Arroja un booleano por lo que debemos pasarlo por un ``filter``
  ```python
   df.filter(pl.col("Creado Por").is_first_distinct()).select(pl.col("Creado Por"))
  ```
* **``.is_between()``** : Arroja valores entre el rango **minimo** y maximo que le indiquemos incluyendo los mencionados. Para valores de texto es necesario usar: ´pl.lit()´ Arroja un booleano por lo que debemos pasarlo por un ``filter``
  ```python
   big_table.filter(pl.col("Factura").is_between(pl.lit("F747 - 00069270"),pl.lit("F747 - 00069605"))).select(pl.col("Factura"))
  ```
* **``.apply()``** : Permite aplicar una funcion predefinida con la que interactuara con cada elemento de las columnas que indiquemos. (como un ciclo for)
  ```python
   # Crear un DataFrame de ejemplo
    data = {'Nombres': ['John Doe', 'Jane Smith', 'Bob Johnson']}
    df = pl.DataFrame(data)

    # Definir una función que toma un nombre y devuelve su longitud
    def obtener_longitud(nombre):
        return len(nombre)

    # Aplicar la función a la columna 'Nombres' arrojando una nueva columna con la longitud.
    df = df.with_column(
        pl.col("Nombres").apply(obtener_longitud).alias("Longitud_Nombre")
    )
  ```
* **``pl.when()``** : Se usa para abrir la logica de condicionales "if else".<br>
  **pl.when()** : Logica ; **.then** : proposicion verdadera ; **.otherwise** : proposicion falsa
  ```python
    df_conditional = df.select(
    pl.col("age"), pl.when(pl.col("age") > 2).then(pl.lit(True)).otherwise(pl.lit(False)) #Arroja el valor "True" o "False"
    .alias("conditional"))

    df.select( pago = (pl.col("Piezas") * pl.col("Costo") - pl.when( #abrimos condicional en el apartado de la resta.
                      pl.col("Oferta")=="Si")
                      .then(pl.col("Piezas")*pl.col("Costo")*pl.col("Descuento")/100)
                      .otherwise(0))
      )

    # Reemaplazar los valores donde se cumple la condicion con los valores de una columna u otra.
    df.with_columns(
    pl.when(pl.col("c") == 2)
    .then(pl.col("b"))  #Reemplaza con los valores de la columna "b"
    .otherwise(pl.col("a")).alias("New Col")
    )

    # Ingresar un texto en la columna que se cumpla.
    df.with_columns(
    pl.when(pl.col("customer") == 100)
    .then(pl.lit("Cumple"))  #Escribira el valor cumple
    .otherwise(pl.lit("False")).alias("New Col")
    )

    # Condicional referenciando 2 columnas
    big_table.select(Tipo_Concreto = pl.when(Col_A=4, Col_B=0).then(99).otherwise(-1))

    # Nueva columna con valor 0 o extraccion de valores de otra col.
    df.with_columns(
      pl.when(pl.col("Forma de Pago") == "Contado").then(0)
      .otherwise(pl.col("Forma de Pago").str.extract(r'(\d+)'))
      .alias("Dias_credito")
      )

  ```
* **``pl.over()``** : Funcion como un "by", o tambien puede ser entendido como un `SUMAR.SI` de excel ya que no reduce las filas del df al hacer la agrupacion y podemos hacer una operacion al filtrar con `over`. Arroja logica propia de las agrupaciones pero repitiendo el valor, manteniendo asi el numero de filas del DataFrame original.
  ```python
    #Arrojara la media de una columna segmentando por "Tipo de Orden". La media sera por cada "Tipo de Orden".
    df.with_columns(pl.col("Cantidad Pedida").mean().over("Tipo de Orden"))

    df.select(pl.col("Tipo de Orden"),pl.col("Cantidad Pedida").mean().over("Tipo de Orden")).unique()

    #Se puede tener más de un concepto para el "by".
    df.select(pl.col("Tipo de Orden"),pl.col("Creado Por"),
              pl.col("Cantidad Pedida").mean().over(["Tipo de Orden","Creado Por"])).sort("Creado Por"),
              pl.col("A").filter(pl.col("B") > 2).sum().over("cars").alias("sum_A_by_cars")

    # Agrupacion con filtro condicional: Valor maximo de C donde coincidan los valores unicos despues de la operacion con "b". En otras palabras transforma "b" y luego agrupa en base a ese resultado.
    df.with_columns(pl.col("c").max().over(pl.col("b") // 2).name.suffix("_max")

    #Division entera "//" arroja cociente (multiplicador); "%" arroja el restante.



  ```
* **``df.group_by()``** : Agrupamos por columnas y aplicamos una funcion a los valores:<br>
 **df.group_by("Filas_index").agg(pl.col("Valores").operacion())**<br>
 "Operaciones" : {'first()', 'last()', 'sum()', 'max()', 'min()', 'mean()', 'median()', 'count()','n_unique()', 'diff()','cumsum()','cummin()','cummax()','cumcount()','cumprod()','cumulative_eval(pl.element())'}.<br><br>

  * "diff(n=1, null_behavior ={'ignore', 'drop'})" : Toma un elemento de la columna y lo resta con el anterior. Sirve para ver variacion contra el dato anterior. El primer valor sera nulo ya que no tiene contra que valor restar.
  * "cumprod()" : es el producto acumulado de multiplicaciones.<br>
  * "cumulative_eval(pl.element())" : Podemos aplicar una operacion personalizada. No necesita ser una lista para iterar y funcionar.

  ```python
  #Agrupamos y contamos los valores unicos graficandolo con `Cufflinks`

  df.group_by(by="Creado Por", maintain_order=True).agg(
    Cantidad_OC = pl.col("Pedido").n_unique()
  ).sort("Cantidad_OC",descending=True).head(5).to_pandas().plot(x="Creado Por",y="Cantidad_OC",figsize=(6,4))

  # maintain_order=True : Mantiene el mismo orden que la tabla actual al mostrar los datos

  # Tambien podemos a usar filtros dentro de las columnas de agregaciones:

  df.group_by("Creado Por", maintain_order=True).agg(
    pl.col("random").filter(pl.col("names").is_not_null()).sum().name.suffix("_sum"),
    (pl.col("party") == "Anti-Administration").sum().alias("anti"),
    (pl.col("party") == "Pro-Administration").sum().alias("pro")
  )

  # Agrupamos una columna de fechas por meses y comprador sumando los numeros de OC (str) unicos.
  df_pl.group_by(pl.col("Fecha documento").dt.month(),"Grupo de compras").agg(
                  pl.col("Documento compras").n_unique()).sort("Fecha documento","Grupo de compras")

  # Compra promedio mensual
  df_pl.group_by("Grupo de compras").agg(
    pl.col("Documento compras").n_unique()/12).with_columns(pl.col("Documento compras").round(2))

  ```
* **``df.group_by_dynamic()``** : Sirve solo para **agrupar fechas**. Columnas tipo "DATE".
 **df.group_by_dynamic("Filas_index").agg(pl.col("Valores").operacion())**<br>
 "Operaciones" : {'first()', 'last()', 'sum()', 'max()', 'min()', 'mean()', 'median()', 'count()','n_unique()', 'diff()','cumsum()','cummin()','cummax()','cumcount()','cumprod()','cumulative_eval(pl.element())'}.<br><br>

 "diff(n=1, null_behavior ={'ignore', 'drop'})" : Toma un elemento de la columna y lo resta con el anterior. Sirve para ver variacion contra el dato anterior. El primer valor sera nulo ya que no tiene contra que valor restar.
 "cumprod()" : es el producto acumulado de multiplicaciones.<br>
 "cumulative_eval(pl.element())" : Podemos aplicar una operacion personalizada. No necesita ser una lista para iterar y funcionar.

  ```python
  df.group_by_dynamic("Date", every="1y", period="1mo", closed="left")
      .agg(Media =  pl.col("Close").mean(),
            Año = pl.col("Date").dt.year() # Columna que muestra solo los años del Index.
          )

    #every="1y" : Agrupara por cada año. El resultado de la fila sera por año. Afecta el index de agrupacion
    #period="1mo" : Indica subdivision del periodo con el que se afecta los calculos de la agrupacion (.agg)
    #closed={"left","right","both","neither"}: En "left" Tomara la fecha menor más proxima que hay en la columna Date.

  ```
* **``df.pivot()``** : Permite poder agrupar por indice y por columnas. Con el Metodo `.pivot()` solo permite hacer una función de agregación a diferencia de PANDAS.<br>
Esta funcion solo funciona en modo "eager" `.collect()` por lo que tendremos que volver a pasar a `.lazy()` tras usar esta funcion.
"aggregate_function" : {'first', 'sum', 'max', 'min', 'mean', 'median', 'last', 'count'}
  ```python

    df.pivot(index=["Creado Por","Moneda"], columns="Tipo de Orden", values="Cantidad Ingresada",aggregate_function="count", maintain_order=True).sort("Creado Por","Moneda").head(5)

  ```
* **``df.melt()``** : Despivotea las tablas teniendo como referencia una columna como index y las demas columnas las junta en una unica columna repitiendose y los valores de ellas en otra columna.
  ```python
    df.melt(id_vars=["A", "B"], value_vars=["Miami", "Lima"], variable_name="MES_2024",value_name="P_MENSUAL")

    # id_vars=["A", "B"]  : Toma las Columnas como Index segun el orden de niveles: Primera columna: "A" segunda col: "B"
    # value_vars=["Miami", "Lima"] : Estas 2 columnas seran contenidas en una unica columnas repitiendose correspondientemente.
    # variable_name="MES_2024" : # Asignamos el nombre de la columna que esta conteniendo el nombre de las columnas a despivotear. (opcional)
    # value_name="City" : # Asignamos el nombre de la columna que esta conteniendo el valor numerico de las columnas a despivotear. (opcional)

  ```
* **``pl.concat()``** : Conatena DataFrames siempre y cuando tengas el mismo nombre los encabezados.
  ```python
    new_DataFrame = pl.concat(df_1, df_2, how="vertical") # how = {vertical, horizontal, diagonal, align}

    #Cuando una tabla tiene mas columnas que otra y se deja en nulos los datos inexistentes:
    pl.concat([df for df in tablas],how="align") #"align": no obliga mismo tamaño de cols.

  ```
* **``df.join()``** : Une 2 o mas tablas indicando el tipo de conexion, muy similar a Pandas. "how" : {'inner', 'left', 'outer', 'semi', 'anti', 'cross'}
  ```python

    df.join(df_b, on="Tipo de Orden", how="inner")

  ```
* **``pl.SQLContext()``** : Es una Clase que nos permite leer archivos de base de datos SQL o arrow.
  ```python

    context = pl.SQLContext(register_globals=False, eager_execution=False)  # Inicializamos el Contexto SQL
    table = pl.scan_ipc("file.arrow")   # Cargamos el archivo DB en modo perezoso.
    context.register("my_table", table)  # Indicamos el nombre real de la tabla y la variable con el archivo cargado.

    # Escribimos la query en una variable con la cual podemos ejecutar la tabla sql o correrlo sobre polars.
    query = """
      SELECT sum(v1) as sum_v1, min(v2) as min_v2 FROM my_table
      WHERE id1 = 'id016'
      ORDER BY population
      LIMIT 10
          """
  ## OPTION 1 Materializamos la Query SQL : ´.query()´

    context.query(query)

  ## OPTION 2 No materializamos la query y en vez lo corremos como LazyFrame : ´.execute()´
   lf = context.execute(query, eager=False)   # Pasa a tipo polars.DataFrame o LazyFrame

   (lf.join(other_table)
        .group_by("foo")
        .agg(
       pl.col("sum_v1").count()
   ).collect())

  ```
* **``pl.read_database()``** : Permite la conexion con una base de Datos, para ello pasamos la cadena de conexion. Se apoya usando un **motor de conexion externa** para interactuar con la base de datos, por ello lo importamos como `sqlalchemy`.
  ```python
    from sqlalchemy import create_engine

    conn = create_engine(f"sqlite:///test.db")

    query = "SELECT * FROM foo"

    pl.read_database(query=query, connection=conn.connect())

  ```
* **``pl.read_database_uri()``** : Permite la conexion con una base de Datos para ellos pasamos la cadena de conexion. Este metodo es más rápido que ``pl.read_database()`` si está utilizando una conexión **SQLAlchemy** o **DBAPI2**, ya que estas conexiones pueden cargar los datos por filas en Python antes de copiar los datos nuevamente al formato Apache Arrow de columnas.<br>
Podemos elegir uno de los 2 motores disponibles para este metodo: ``ConectorX`` (default) o ``ADBC``. Ambos motores tienen soporte nativo para Apache Arrow y, por lo tanto, pueden leer datos directamente en Polars DataFrame sin copiar los datos:

   * **``ConectorX``** : ConnectorX es el motor predeterminado y admite numerosas bases de datos, incluidas Postgres, Mysql, SQL Server y Redshift. ConnectorX está escrito en Rust y almacena datos en formato Arrow para permitir la copia cero en Polars.<br>
   Para leer desde una de las bases de datos compatibles con ConnectorX necesitas activar la dependencia adicional ConnectorX al instalar Polars o instalarla manualmente con `pip install connectorx`.

   * **``ADBC``** : (Arrow Database Connectivity) es un motor compatible con el proyecto Apache Arrow. ADBC pretende ser un estándar API para conectarse a bases de datos y bibliotecas que implementen este estándar en una variedad de idiomas.<br>
   Aún es temprano para ADBC, por lo que el soporte para diferentes bases de datos aún es limitado. Actualmente, los controladores para ADBC ​​sólo están disponibles para **Postgres** y **SQLite**. Para instalar ADBC ​​necesita instalar el controlador de su base de datos. ``pip install adbc-driver-sqlite``

  ```python
    uri = "postgresql://username:password@server:port/database"
    query = "SELECT * FROM foo"

    pl.read_database_uri(query=query, uri=uri, engine="adbc")

  ```
* **``pl.write_database()``** : Permite escribir una tabla al database conectado. Para ello se apoya en la dependencia de `SQLAlchemy` por lo que es importante tenerlo instalado. `pip install SQLAlchemy`
  ```python
    uri = "postgresql://username:password@server:port/database"
    df = pl.DataFrame({"foo": [1, 2, 3]})

    df.write_database(table_name="records",  uri=uri)

  ```
* **``pl.sum_horizontal()``** : Arroja suma de manera horizonal (entre columnas). Arroja una Serie como output.
  ```python
    #convertimos una columna en negativo, sumamos y convertimos de Serie a 2 Dimensiones para poder filtrar.
    df_prueba.select(pl.col("Por Entregar")*-1,pl.col("Facturado")).sum_horizontal().alias("sum").to_frame().filter(pl.col("sum")<0)

  ```
* **``pl.dot()``** : Arroja la **Suma-Producto** de 2 columnas o listas.
  ```python
    df = pl.DataFrame({
        "a": [1, 3, 5],
        "b": [2, 4, 6],
      })

    df.select(pl.col("a").dot(pl.col("b")))
    #OUTPUT : pl.Series valor 44
  ```
* **``df.diff()``** : Arroja la diferencia de restar un fila de una misma columna con la anterior o saltos hacia atras que indiquemos.
  ```python
    df.with_columns(a = (pl.col("Pedido").diff()).fill_null(0)) #por defecto diff(n=1)

  ```
* **``pl.reverse()``** : Voltea de cabeza los valores del DF o Serie. Pone el primer valor como último volteando así todos los datos y viceversa.

* **``pl.shift()``** : Desplaza una columna posiciones hacia abajo (+) o hacia arriba (-) dejando espacios nulos en su lugar y cambiando los valores de los extremos a los predecesores manteniendo siempre el mismo numero de filas.
  ```python
    #restando 2 columnas y conviertiendo el valor nulo en 0. Hay Nan y Nulls
    df.with_columns(a = (pl.col("Pedido"))-(pl.col("Pedido").shift(1).fill_null(0))).limit(3)

    #Hay otra manera diseñada para lograr esto con `.diff()`
    df.with_columns(a = (pl.col("Pedido").diff()).fill_null(0))

  ```
* **``df.corr()``** : Arroja una tabla con todas las correlaciones de Pearson por defecto. Es necesario filtar solo valores numericos.
  ```python
    #Correlacionamos solo columnas que no sean texto.
    df.select(pl.col("*").exclude(pl.Utf8,pl.Date)).corr()

  ```
* **``df.to_dummies()``** : Convierte una columna en **OneHotEncoding**.
  ```python
    #Añadimos a la tabla actual las columnas de to_dummies
    data = data.with_columns(data.select(pl.col("Categoría")).to_dummies()).drop("Categoría")
    data

  ```
* **``pl.map_elements()``** : Nos permite aplicar una **Funcion Python** previamente definida para iterar sobre una columna que hemos señalado. En otras palabras permite aplicar una Funcion python a una columna de Polars.
  ```python
    #Hay 2 manera de lograr esto, aplicando la funcion o usando logica con polars. Contador
    counter = 0
    def add_counter(valor: int) -> int:
      global counter
      counter += 1
      return counter + valor

    df.select(solucion_1 = pl.col("values").map_elements(add_counter), #Contador usando la funcion previa
              solucion_2 = pl.col("values") + pl.int_range(1, pl.count()+1) #Contador usando metodos de polars. Usa 2 parametros.
    )
  ```
* **``pl.flatten()``** : Se utiliza para aplanar columnas de listas. Convierte una columna de listas de elementos, puedes usar **pl.flatten()** para convertir esa columna en una serie de elementos individuales
  ```python
    import polars as pl

    # Crear un DataFrame de ejemplo con una columna de listas
    df = pl.DataFrame({
        "columna1": [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
    })

    # Aplanar la columna de listas
    df = df.with_column(pl.flatten(pl.col("columna1")).alias("columna1"))

    print(df)

  ```
* **``pl.to_list()``** : Permite extraer una columna como lista de python, para ello tenemos que seleccionar o convertir el data frame a Serie para que se habilite el metodo `to_list()`
  ```python
    df.select(pl.col("Pedido"))[:,0].to_list()
  ```
* **``pl.to_frame()``** : Se suele usar para convertir una `pl.Serie` a **DataFrame** (2 dimensiones) con el fin de poder hacer `.filter()`.
  ```python
    df.select(pl.col("Pedido"))[:,0].to_list()
  ```
* **``pl.item()``** : Selecciona el valor del dataframe indicando **fila y columna**. Permite Extraer solo el valor en formato python saliendo de polars:
  ```python
    #restando 2 columnas y conviertiendo el valor nulo en 0. Hay Nan y Nulls
    df.select(pl.col("Pedido")).sum().item()  # 4500 (int)
    # df.select(pl.col("Pedido")).sum()[0,0]   Otra manera de lograrlo, aveces funciona mejor.

    # seleccionar un valor de un DataFrame
    df.item(1,1)  #No usamos pl.col ya que interactura con todo el df.
    df.select(pl.col("Pedido")).item(1,"Pedido") #A menos que lo indiquemos, pero no es necesario
    df.item(1,"Pedido")  #Podemos escribir directamente el nombre de la columna.

   ```
* **``pl.format()``** : Permite crear una cadena de text que interactue con los valores de otras columnas.
  ```python
    #Crear una nueva columna de texto con valores de "A" y "B"
    df.select(pl.format("Se compro {} manzanas a {} soles", pl.col("A"), pl.col("B")).alias("Text_Col")
  ```

### Parametro

* **``df.columns``** : Arroja en una lista el nombre de las columnas del df.
  ```python
    # Usamos "print()" para que muestre las columnas de forma horizontal
    print(df.columns)

    #Cambiar el nombre de las columnas:
    fd.columns =["OC","OS","Otro"]

  ```

* **``df.schema``** : Arroja los nombres de la colmunas con sus respectivos tipos de datos.

### POLARS CODES

```python

  tabla_pl = tabla_pl.with_columns(pl.col(["IT","CÓDIGO"]).cast(pl.Utf8),
                                  pl.col("SUBT.").str.replace(",","").cast(pl.Float64,strict=False) # strict=False debido a que los valores nulos arrojaban error se dejaron en nulos.
                                  ).with_columns(pl.col("CÓDIGO").str.replace(r'\.0$+', '')) #Quitamos los ".0" de los numeros pasados a string.

  #Filtrar usando una lista de numeros para que nos arroje varias filas.
  lista_codigos = [5114190,5114192,5114193,5021165,6512215,5642245] #Valores numericosv

  lista_codigos_str = list(map(str, lista_codigos))  #Aplica o convierte a str cada elemento de la lista.
  lista_codigos_str

  tabla_pl.filter(pl.col("CÓDIGO").is_in(lista_codigos_str)) #Filtramos segun los valores contemplados en la lista

# Usando polars en sklearn

  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler

  X, y = load_iris(return_X_y=True)
  X_train, X_test, y_train, y_test = train_test_split(X, y)

  scaler = {
    StandardScaler(),
    .set_output(transform="pandas")
  }

  X_test_scaled = scaler.fit_transform(X_train)
  X_test_scaled.head()

```
## [26. hvplot & panel](#indice)

Tanto `hvplot` como `panel` son libreiras de **Holoviz** el cual son una medida de comunicacion para usar varios tipos de graficos de otras librerias como ``matoplotlib``, ``plotly``, ``bokeh`` ,etc.<br>
Por otra parte `panel` esta orientada a adicionar `widgets` a las graficas de ``hvplot`` para que interactuen y sean dinamicas con otros graficos si asi se le configura. Tambien compila todos estas graficas y widgets para alojarse en un dashboard con los diferentes `templates` que ha desarrolado la comunidad.

```python
  !pip install hvplot panel holoviews
  !pip install jupyter_bokeh  #Para poder visualizar los widgets en VSCode.

  import hvplot.pandas #Confirgura como default a hvplot como grafico.
  import hvplot.polars
  hvplot.extension("plotly") #Configuramos "plotly" como default en hvplot

  #Usamos tema dark de plotly
  import plotly.io as pio
  pio.templates.default = "plotly_dark"

  #bokeh en modo dark
  import holoviews as hv
  #hv.renderer('bokeh').theme = "dark_minimal" #Preferible no usar para evitar conflicto al mostrar dashboard.

  import panel as pn
  pn.extension("tabulator") #importa in widget tipo tabla.
  pn.extension(sizing_mode="stretch_width") #ajusta el grafico al ancho de la ventana.
  #pn.config.theme = 'dark'
  #pn.extension(theme=‘dark’) #Probar si arroja oscuro.

```

### Tipos de Graficos

Usamos la funcion `hvplot` seguido del tipo de grafico que deseamos graficar con sus respectivos parametros.

* **``hvplot.help('line')``** : Arroja todos los metodos que soporta el tipo de grafica indicada.

* **``hvplot.line()``** : **Grafico de Lineas** es el que viene por defecto `hvplot()` (usado como metodo).
  ```python
    cant_oc_fecha = df.group_by("Creado El",maintain_order=True).agg(pl.col("Pedido").unique_counts().count()).sort("Creado El")

    cant_oc_fecha.hvplot.line(x="Creado El", y="Pedido",, by="Type", title="Cantidad de Pedidos", width=1100, height=500, rot=90, legend='top_left',color="green", hover=["Col_1","Col_2"],subplots =False, invert=False, flip_yaxis=False, ,line_dash= 'dashed', rasterize= False, grid=True)

    # ,by="Type" : Agrupara en la leyenda por una tercera columna  de tipo categorica. (Reemplaza el tener que hacer un groupby).
    # ,width=1100 : Indica el ancho del lienzo del grafico.
    # ,height=500 : Indica el alto del lienzo del grafico.
    # ,rot=90 : Rotara los nombres del Eje x en vertical para los casos en que no alcanzen al mostrarse en horizontal.
    # ,legend='top_left' : Mueve la leyenda donde le indiquemos: {top, bottom, left, right, top_left, top_right, bottom_left, bottom_right}.
    # ,subplots =False : En modo "True" creara graficas separadas cuando se esta usando "by=".
    # ,invert=False : Convierte un grafico horizontal en vertical iniciando el primer dato desde la parte bottom -> top.
    # ,flip_yaxis=False : Voltea de cabeza el grafico, en otras palabras inicia desde bottom con el valor mas alto.
    # ,hover=[col1,col2] : Permite añadir en la información adicional al pasar el mouse por los datos de columnas que no están en el plot.
    # ,line_dash= 'dashed' : Cambia la linea continua a linea punteada "---".
    # ,col= & ,row= : Crea multigraficas poniendo una variable categorica aparte de x y el cual generara las minigraficas.
    # ,rasterize= False : en modo True va a rasterizar la grafica cada vez que demos zoom, para una mayor definicion.
    # ,grid=True : Añade la rejilla de fondo en la grafica.
    # ,format='{value:.3f}' : Damos formatos a los numeros que muestra.
    # ,font_size='36pt' : Indicamos el tamaño del texto que arroja.

  ```
Podemos añadir una regresion lineal para demarcar la tendencia en una grafica de tipo`scatter`.

  ```python
    grafica_scatter * hv.Slope.from_scatter(grafica_scatter)

  ```
Podemos hacer que la seleccion de una grafica interactue con otra tipo power bi.

  ```python
    ls = hv.selection.link_selections.instance() #Creamos el objeto
    ls(histograma_1 + histograma_2) #linkeamos las graficas
  ```


### Adicion de Graficos

Podemos, en un mismo grafico, combinar varios tipos, los cuales seran interactivos entre ellos del siguiente modo:

```python
  plot_1 + plot_2 #pone un grafico a lado del otro en horizontal.
  (plot_1 + plot_2).cols(1) #pone un grafico debajo del otro en vertical. Importate agrupar mediante los parentesis
  plot_1 * plot_2 #Superpone los graficos en uno solo.

```

### Panel: Widgets and Dashboard

Panel crea widgets y template de dashboard. En el caso de los Widgets se trata como un objeto a parte que luego se une a la logica en el pipline que finalmente de plotea (hv.plot)<br><br>

Para Unir cualquier Widgets de Panel con una logica, necesitamos que esta logica esté en una funcion y las unimos con:`pn.bind()`


* **``pn.bind()``** : Conecta los Widgets de Panel con una Funcion. Cuando El Widget cambia de valor la funcion se ejecuta automaticamente. Cuando usas ``pn.bind()`` no necesitas convertir el **DataFrame** a **interactivo** con ``df_pandas.interactive()``. Esto es porque ``pn.bind()`` maneja la reactividad por sí mismo.

```python
  def Funcion(F_param_1, F_param_2):
    pass

  Integracion = pn.bind(Funcion, widget_1, widget_2) #Asi los parametros de la funcion tomaran el valor que arroja cada widget.

  Integracion = pn.bind(Funcion, F_param_1 = widget_1, F_param_2 = widget_2)

```
  Ejemplo con polars pasando a pandas:

```python
  import polars as pl
  import panel as pn

  # Crear una tabla interactiva que se filtre segun el valor de un slider.
  # Crear un DataFrame en Polars
  df_pl = pl.DataFrame({
      'A': [1, 2, 3],
      'B': ['a', 'b', 'c']
  })

  # Función para filtrar los datos en Polars, con dos argumentos: el valor del slider y el objeto que lo invoca
  def filtrar_datos_polars(valor):
      return df_pl.filter(pl.col('A') > valor).to_pandas()

  # Crear un slider para filtrar los datos
  slider = pn.widgets.FloatSlider(start=0, end=3, value=1)

  # Enlazar la función de filtrado con el slider
  tabla_interactiva = pn.bind(filtrar_datos_polars, valor=slider)

  # Crear el dashboard
  dashboard = pn.Column(
      slider,
      pn.widgets.Tabulator(tabla_interactiva)  # Mostrar la tabla interactiva, se filtrara segun el valor del slider.
  )

  # Mostrar el dashboard
  dashboard.servable()

```

#### Widgets

* **``pn.widgets.DateRangeSlider()``** : Cread un slider con 2 deslizadores (minimo y maximo). Importante selecionar Date o Datetime segun el tipo de columna.
    ```python
      widget_range = pn.widgets.DateRangeSlider(name="rango_fecha", start=(pandas_df["Creado El"].iloc[0]),
                                          end=(pandas_df["Creado El"].iloc[-1]), step=30, format="%B")
      #widget_range.value : arroja fecha inicial , fecha final
      #widget_range.value_start  :arroja fecha inicial
      #widget_range.value_end  : arroja fecha final
      #widget_range.param.value_start  :arroja el valor de fecha inicial. Util para filtrar las columnas.
      #,step=30 : nº de pasos en dias
      #,format="%B" : Podemos que nos arroje el widget en formato fehca que indiquemos.

    ```
* **``pn.widgets.DatetimeRangeSlider()``** : Cread un slider con 2 deslizadores (minimo y maximo). Importante selecionar Date o Datetime segun el tipo de columna.
    ```python
      widget_range = pn.widgets.DatetimeRangeSlider(name="rango_fecha", start=(pandas_df.index[0]),
                                                end=(pandas_df.index[-1]))
      widget_range

* **``pn.widgets.Button()``** : Cread un Boton que arrojara un unico valor que indiquemos.
    ```python
      pn.widgets.Button(name="Boton", button_type="primary"))

    ```
Tambien podemos hacer que un parametro de un widget dependa de otro:

  ```python
  imagen.param.height = slider.param.end
  layout = pn.row(slider, imagen)

  ```

#### Dashboard

```python

  import hvplot.pandas
  import hvplot.polars

  import holoviews as hv
  hvplot.extension("bokeh") #Configuramos "plotly" como default en hvplot

  import panel as pn
  pn.extension("tabulator")
  pn.extension(sizing_mode="stretch_width") #ajusta el grafico al ancho de la ventana.

  #Convertir cualquier grafico en un objeto panel y mostrar en el navegador.
  grafica_panel = pn.panel(grap_1)
  grafica_panel.show()    # Muestra la gráfica en el navegador

  #Agrupamos la Data
  df = df.with_columns(pl.col("Creado El").dt.date())
  cant_oc_fecha = df.group_by("Creado El",maintain_order=True).agg(pl.col("Pedido").unique_counts().count()).sort("Creado El")

  #Convertimos a pandas ya que solo con pandas podemos convertir a ".interactive()"
  pandas_df = cant_oc_fecha.to_pandas()
  ipandas = pandas_df.interactive()

  #Creamos el Header: Titulo e imagen png arriba de la grafica.
  header = pn.Row(
                   pn.panel("## Indicadores Compras"),
                   pn.panel(r"C:\Users\Foster-PC\Documents\TRABAJOS\EQUANS\INICIO\ARCHIVOS\LOGOEQUANS LOGO (png).png", height= 90)
    )
  #Creamos los Widgets
  widget_range = pn.widgets.DateRangeSlider(name="rango_fecha", start=(pandas_df["Creado El"].iloc[0]),
                                          end=(pandas_df["Creado El"].iloc[-1]))

  #Creamos el pipeline con el interactive dataframe. Logica para unir los Widgets
  pipline = ipandas[
     (ipandas["Creado El"] >= widget_range.param.value_start) &
     (ipandas["Creado El"] <= widget_range.param.value_end)] #Se puede filtrar usando el index tambien ".index <="

  #Graficamos el "pipline"
  graphic = ((pipline.hvplot.line(x="Creado El", y ="Pedido", width=1100, height=500, title="Cantidad de Pedidos")*\
  pipline.hvplot.scatter(x="Creado El", y ="Pedido", width=1100, height=500, size=100))+\
  (pipline.hvplot.table(columns=["Creado El","Pedido"],selectable=True))).cols(1)

  #Graficamos una tabla:

  table  = pipline.pipe(pn.widgets.Tabulator, pagination='remote', page_size = 10, sizing_mode='stretch_width')

  #Creamos el "template"
  template = pn.template.FastListTemplate(
                title = "Reporte Cantidad de OC por Fecha",
                sidebar= [pn.panel("# Date Range"),
                          widget_range],
                main = [pn.Column(header,graphic)],
                       #[pn.Row(pn.Column(Widget_grph_1,                            # 2 graficas por fila (Row)
                       #                  Graph_1.panel(width=700), margin=(0,25)),
                       #                  table.panel(width=500)),
                       # pn.Row(pn.Column(Graph_3.panel(width=600), margin=(0,25)),
                       #        pn.Column(Widget_grph_4, Graph_4.panel(width=600)))]
                accent_base_color="#88d8b0",
                header_background="#88d8b0" #Color de la barra del header del dashboard.
              )

  template.servable(); #Con la coma evitamos que el note arroje el resultado pero si lo ejecuta.
  #template.show(); #Es lento en este modo.
  # command: panel serve Romex_Ene-Nov_2019.ipynb






```