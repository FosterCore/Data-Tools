# *Librerias*

## Indice:

* [os](#os)
* [pathlib](#pathlib--path)
* [1. math](#1-math)
* [2. statistics](#2-statistics-st)
* [3. random](#3-random-rd)
* [4. numpy](#4-numpy-np)
* [5. re](#5-re-regex)
* [6. pandas](#6-pandas-pd)
* [7. openpyxl](#7-openpyxl)
* [8. matplotlib](#8-matplotlib-plt)
* [9. seaborn](#9-seaborn-sns)
* [10. plotly](#10-plotly-px)
* [11. collections](#11-collections-clt)
* [12. selenium](#12-selenium)
* [13. BeautifulSoup](#13-beautifulsoup-bs)
* [14. scrapy](#14-scrapy-scp)
* [15. request](#15-request)
* [16. scipy](#16-scipy-stats)
* [17. TextBlob](#17-textblob-tb)
* [18. nltk](#18-nltk)
* [19. spacy](#19-spacy)
* [20. sklearn](#20-sklearn)
* [21. TensorFlow](#21-tensorflow)
* [22. Camelot](#22-camelot)


## [os](#indice)
El módulo os de Python le permite a usted realizar operaciones dependiente del Sistema Operativo como crear una carpeta, listar contenidos de una carpeta, conocer acerca de un proceso, finalizar un proceso, etc. Esta librería trabaja retornando valores de **Strings** con el **shell** activado o en modo True (el cual puede ser un riesgo latente).

```python
import os
```
***``Windows limita en número de caracteres a 260, tanto OS y Pathlib no podrán leer archivos con nombres tan largos mayores a 260 caracteres``***

Es importante aclarar que python tiene como carácter reservado el " **\\** " por ello debemos reemplazar el símbolo " **/** " o digitar un doble " **\\** **\\** " para anularlo u lea como un string. Otra manera es también pegar la dirección tan cual antecediéndolo con un r-String. Aquí las siguientes soluciones:

 ``"C:/Users/Foster-PC/"``<br>
 ``"C:\\Users\\Foster-PC\\"``<br>
 ``r"C:\Users\Foster-PC\"`` #Esta es la solucionas más sencilla pegamos la dirección y le agregamos r'

Otra manera de convertir el carácter a " **/** ".

```python
(Variable_path).replace("\\","/")
```

Para mover carpetas y archivos a una ruta en especifica usamos:
```python
import shutil

#CarpetaExistente.
shutil.move('C:/Users/Foster-PC/Desktop/Archivo.py', 'C:/Users/Foster-PC/Desktop/Nueva Carpeta') 

# Para eliminar árboles de directorios completos con todos sus archivos dentro.
shutil.rmtree('C:/Users/Foster-PC/Desktop/Nueva_Carpeta'). 
#Si tenemos el directorio grabado en una variable no dejara borrarla.
```
* CMD: 
  * Ejecutar un script de python:
  ```cmd
          C:\Users\Foster-PC\Documents\Visual Studio Code\Learning>py nombre.py
  ```
  * Comandos del sistema:
    * **`cls`** : Limpia la consola. También puede usarse `clear`
    * **`cd`** : Entrar y movernos a un directorio. Para entrar a carpetas cuyo nombre tiene espacios en blancos se debe escribir el nombre entre comillas. ***>cd "Tutoriales Internet"***
      ```
                >cd ("Tutoriales Internet/Tutorial 0") #para ingresar por más directorios a la vez.
      
      ```
    * **`cd../`** : Salir de un directorio. Ejm: ***cd../../../*** (Regresara 3 directorios hacia atrás.); ***cd/*** (Saldrá hasta la raíz "C:")
      ```python
                ('./../practica') # De la ubicación actual ("./") Retrocede un directorio ("../")
      
      ```
    * **`pwd`** : Muestra el directorio donde nos encontramos actualmente.
    * **`dir`** : Muestra el contenido de un directorio archivos y carpetas (DIR).
    * **`tree`** : Muestra el contenido y sub carpetas en forma de árbol raíz.
    * **`mkdir`** : Crea una carpeta en el directorio actual donde nos ubicamos. También puede usarse `md`
    * **`rmdir`** : remueve una carpeta en el directorio actual donde nos ubicamos. También puede usarse `rd`
    * **`del`** : Elimina un archivo.abc o una carpeta en el directorio actual donde nos ubicamos.
  
___
### Clase, Función:

* **`os.system()`** : Ejecuta comandos al sistema (cmd). Ejm: ('dir'). A diferencia de la librería ``subprocess`` aquí si se puede ejecutar los comandos tal cual se digitan con sus espacios. Es necesario ejecutarlo en archivos `.py` para ver el resultado en la consola. Usa el path actual por defecto:
```
               os.system('py --version')           
```
* **`os.listdir()`** : Crea una lista con los archivos encontrados en un directorio. Podemos indicar dentro del paréntesis que carpeta queremos que haga la lista de sus archivos contenidos. Arranca buscando del directoria actual donde nos encontramos. Esta función es muy usada para enlistar archivos.
```python
          os.listdir('C:/Users/Foster-PC/Documents/Visual Studio Code/Learning/Beat Data - Tutorials/sales')
          #Creara una lista con todos los archivos .csv dentro de la ruta indicada.
```
Otra manera partiendo de la Ruta de Trabajo
```python
          files = os.listdir('../files') #Lista con el nombre de cada archivo.csv
          #los 2 puntos indican una carpeta antes
          df=pd.DataFrame() #vacío
          for x in files:
              file=pd.read_csv('files/'+x) #Directorio de cada archivo.
              df=pd.concat([file,df]) #añade cada Dataframe creado.
```

* **`os.getcwd()`** : Muestra la ruta del archivo python en el que estamos trabajando (Ruta de trabajo). Lo muestra en Formato **" \ "** por ello debemos utilizar un `.replace("\\","/")` para usarlo en una cadena lógica de comandos.
* **`os.chdir()`** : Este método cambia el directorio de trabajo actual a la ruta dada. 
  ```python
          path = "C:/GoogleDrive/06/archivos_unificar

          os.chdir(path)
          # %pwd o os.getcwd() para verificar que nuestra ruta se ha movido.
  ```
* **`os.path.join()`** : La función `.join()` es usada para que diferentes sistemas operativos puedan correr el código tanto como en windows ( \ ) como en Mac o Linux ( / ). En conclusion Esta función **arroja una ruta** como resultado.
```python
          os.path.join(os.getcwd(), 'Carpeta') #Del directorio añadirá '/Carpeta' a la ruta.
          #Esto no crea una carpeta con el nombre 'Carpeta'.
```
* **`os.mkdir()`** : Creará una nueva carpeta, utiliza el path o directorio actual por defecto. Necesitaremos indicarle la ruta adentro incluyendo la carpeta que queremos crear considerando usar " / "
```python
          os.mkdir(os.path.join('./../../','Nueva_Carpeta')) #leerá una ruta adentro donde creara la carpeta.
          os.mkdir('C:/Users/Foster-PC/Desktop/Nueva Carpeta') #Podemos poner la ruta entera también.
```
* **`os.makedirs()`** : Creará varias carpetas según la ruta que le especifiquemos. Utiliza el path o directorio actual por defecto. Necesitaremos indicarle la ruta adentro incluyendo la carpeta que queremos crear considerando usar " / ".
```python
          os.makedirs(os.path.join('C:/Users/Foster-PC/Desktop','Nueva Carpeta','Carpeta Hijo'))
          #creara "Carpeta Hijo" dentro de "Nueva Carpeta". Creara a ambas.
```
* **`os.removedirs() o rmdirs()`** : Elimina carpetas vacías, de uno en uno. No elimina conjunto de carpetas. Si tenemos el directorio grabado en una variable o nuestro proyecto se sitúa en esa carpeta no dejara borrarla.
```python
          os.rmdirs('C:/Users/Foster-PC/Desktop/Nueva_Carpeta')
         
```
* **`os.rename()`** : Renombrará a una carpeta. (nombre del archivo, nuevo nombre). Tanto **OS** como **Pathlib** no funcionaran correctamente con rutas o nombres mayores a 260 caracteres.
```python
          for file in os.listdir():
            if file.endswith('.csv'):
              os.rename(file, f'2021_{file}')

          #Otro Ejemplo Limpiando el nombre con R.E
          import re
          reg = re.sub("#","",i)
          os.rename(f"{path}/{i}", f"{path}/{reg}") #funciona con path y sin el.
```
* **`os.remove()`** : Solo eliminar archivos tras una ruta dada.
  ```python
    os.remove("C:/Users/Foster-PC/Downloads/Descargas Jdowloader 2/Automatizar SAP - PYTHON/0.mp4")
  
  ```
* **`os.path.exists()`** : Muestra con un booleano (True or False) si un archivo.py o una carpeta existe.
* **`os.path.abspath()`** : Muestra la ruta absoluta de la carpeta o archivo.py en la ruta de trabajo que le indiquemos.
* **`os.path.realpath()`** : Muestra la ruta real donde se encuentra la carpeta o archivo.py que le indiquemos. (resuelve los enlaces simbólicos) Por ello suelen usar:
```python
          os.path.abspath(os.path.realpath(__file__))
          #Muestra la ruta incluyendo las carpetas y el archivo.py

```
* **`os.path.splitext()`** : Dividida a un archivo por su nombre y extension.
  ```python
            archivo, ext = os.path.splitext(archivo)
  ```

___
### Métodos:
___
### Propiedad:

* **`os.extsep`** : Referencia al "." (lo ubica) que divide la extensión de un archivo.
  ```python
            print("hola.abc".split(os.extsep))
            #Cortara por la extension del archivo "."
  ```

## [pathlib > Path](#indice)
Esta Librería, al igual que con `os` permite trabajar con nuestro directorio y comandos del sistema, ya sea creando carpetas en nuestro sistema, etc. La diferencia con `os` es que `pathlib` los resultado que arroja no son de formato `str` sino que son de formato `path`.

Es importante aclarar que esta librería leerá las rutas de los directorios con el siguiente carácter " **/** " :

 ``C:/Users/Foster-PC/``

Python 3 incluye el módulo ``pathlib`` para manipular rutas de sistemas de archivos de forma agnóstica en cualquier sistema operativo. El módulo ``pathlib`` es similar al os.path, pero ``pathlib`` ofrece una interfaz de nivel más alto, y, a menudo, más conveniente, que os.path. [Tutorial pathlib](https://www.digitalocean.com/community/tutorials/how-to-use-the-pathlib-module-to-manipulate-filesystem-paths-in-python-3-es) <br><br>

```python
from pathlib import Path
```
La contraparte de la función ´os.path.join()´ es PurePath para tener compatibilidad de código con diferentes sistemas operativos.
La diferencia es que como `PurePath()` es una clase esta la podemos usar para encapsular todo los comandos bajo la clase `Path()`, haciendo de esta manera todo nuestro código con `Path()` compatible con cualquier sistema operativo.
```python
from pathlib import PurePath
```
___
### Clase, Función:

Normalmente suelen guardarse en una variable la clase `Path('ruta')` con la ruta de la carpeta o archivo.py a trabajar.

* **`PurePath(Path())`** : Convertirá el código `Path` compatible con cualquier sistema operativo.

* **`Path('C:Ruta/')`** : Esta clase representa **una ruta** del sistema de archivos. Indicaremos los directorios en donde queremos que se situé el `Path`. El **Path** se encontrara por defecto en nuestra Ruta de trabajo. Con respecto a las funciones y métodos no importa donde este ubicado el `Path()` siempre se ejecutaran los comando de la ruta que indiquemos.
* **`Path.cwd()`** : Muestra la ruta del archivo python en el que estamos trabajando. Ruta de trabajo

___
### Métodos:
Muchos de los métodos arrojaran como resultado un generador de objeto. Para poder ver el resultado necesitamos encerrar el código en una **clase** **`list()`**.

* **`.iterdir()`** : Crea una lista con los archivos encontrados en un directorio (Path() toma por defecto el path actual de nuestro trabajo). Podemos indicar dentro del paréntesis de la clase **`Path('Carpeta_Prueba')`** que carpeta queremos que haga la lista de sus archivos contenidos. Arranca buscando del directoria actual donde nos encontramos. Debido a que el resultado es formato ``Path`` necesitamos pasarlo a formato ``list``. Esta método devuelve en formato path una lista con los elementos a diferencia de `os.listdir` que devuelve ya los elementos dentro de una lista. 
```python
              list(Path('Carpeta_prueba').iterdir()) #list es solo para verlo por pantalla.
              list(Path.iterdir(Path('C:/Users/Foster-PC/Documents/Visual Studio Code/Learning/Tutoriales Internet/Tutorial 0')))
              #Esta es otra manera de indicar el Path. Ejecutara entorno a la ruta.
```
* **`.joinpath()`** : Esta función **arroja una ruta** como resultado. Puede usarse con la función ``Path`` o ``PurePath`` con este ultimo funcionará el código en varios sistemas operativos. En conclusion Esta función **arroja una ruta** como resultado uniendo (Path(), "nombre") *(Por mas que creamos una ruta con nombres con directorios que no existan no creara las carpetas hasta que usemos el método mkdir() o mkdir(parents=True) para mas de 1 carpeta.)*
```python
          print(Path.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # Path.joinpath(Path(),'Nueva Carpeta')
          print(PurePath.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # PurePath.joinpath(Path(),'Nueva Carpeta')
          print(PurePath.joinpath(Path.cwd(),'Carpeta'))
```

* **`.mkdir()`** : Crea una carpeta. Propiedades:
  *  ``(exist_ok= True or False)`` : Indicara si ya existe una carpeta con ese nombre, el código no se rompa y no arroje error sino lo omita.
    ```python
              print(Path('Nueva_carpeta').mkdir(exist_ok=True)) #usando el path actual.

              print(Path('C:/Users/Foster-PC/Desktop/Nueva Carpeta').mkdir(exist_ok=True))
               #Especificando el path con el nombre de la carpeta a crear.
              print(PurePath.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta').mkdir(exist_ok=True))
              # PurePath.joinpath(Path(),'Nueva Carpeta')
    ```

  *  ``(parents= True or False)`` : Creara mas de una carpeta con el parámetro en `True`. Para lo cual nuestra ruta debe contener una carpeta nueva con otra que sea hijo (dentro de carpeta nueva).
    ```python
              print(Path.joinpath(Path('C:/Users/Foster-PC/Desktop/'),'Nueva Carpeta','Carpeta Hijo').mkdir(exist_ok=True,parents=True))
    ```
* **`Path.rename()`** : Cambia el nombre de path1 a path2.
```python
          Path.rename(Path('C:/Users/Foster-PC/Desktop/Nueva Carpeta'),Path('C:/Users/Foster-PC/Desktop/New Directory'))
```
* **`Path.rmdir()`** : elimina la carpeta situado en el path indicado.
```python
          Path.rmdir(Path('C:/Users/Foster-PC/Desktop/New Directory'))
```
* **`Path.exists(Path())`** : Indica con un booleano si el path indicado existe.
* **`Path.resolve(Path())`** : Arroja la ruta absoluta de la carpeta o archivo.py indicado previamente en la ruta.
* **`Path.stem(Path())`** : Arroja solo el nombre, sin la extension, de la carpeta o archivo.py indicado previamente en la ruta.
* **`Path.suffix(Path())`** : Arroja solo la extension de los archivo.py indicado previamente en la ruta.
* **`Path.stat(Path()).st_size`** : Arroja el tamaño de los archivo.py indicado previamente en la ruta.

___
### Propiedad:

## [1. math](#indice)
Es una librería que brinda funciones para operaciones matemáticas clásicas como: **`.log`**

## [2. statistics ("*st*")](#indice)
Es una librería que brinda funciones para cálculos estadísticos como: **`.mean`**, **`.median`**, **`.mode`**, **`.stdev`**, **`.pstdev`**, **`.variance`**, **`.pvariance`**, etc. 

## [3. random ("*rd*")](#indice)
Es una librería que ofrece generadores de números pseudo-aleatorios para varias distribuciones:

* **`.randrange()`** : Selecciona un item de manera aleatoria indicado (1,7)
* **`.randint()`** : Crea números aleatorios en el rango especificado con una distribución "Uniforme Discreta". Ejemplo para un Array de numpy:
```python
    Array_1 = np.random.randint(1, 100, size=1000)
    #Creara un array con valores del 1-100 con 1000 datos dentro del array.
```
* **`.uniform()`** : Crea números aleatorios de números decimales en el rango especificado. Ej: (1, 100, size=1000)

## [4. numpy ("*np*")](#indice)
**Numeric Python**. Es una librería que brinda la función de trabajar con matrices, algebra lineal, estadística para análisis de datos.<br>
Esta librería solo permite trabajar con datos homogéneos bien sea con números o con letras. Y trabaja solo con indices "0" ahi es donde entra `pandas`.
Incluye la librería **random**.

* Cabe resaltar que para numpy cada cierre de **"`[]`"** es una fila.
* El punto de origen de las matrices es la coordenada **(0,0)** Empezando por fila y columna.
  
### Tipos de Data:
* **`np.int64`**
* **`np.float32`**
* **`np.complex`**
* **`np.bool`**
* **`np.object`**
* **`np.string`**
* **`np.unicode_`**
---     
### Indexado:
*  Con **[x,y]** traera de un dato entre filas y columnas (coordenada)
*  Con **[[x,y]]** traera las filas específicas señaladas.
*  Con **[x:y]** traera de un rango continuo de solo filas.
*  Con **[x:y,0]** traera de un rango de filas la primer columna.
*  Con **[x:y,[0,2]]** Delimitara por columnas (traera 2 primeras columnas contando con la columna "0") 
---
### Clase, Funciones:
Ciertas funciones pueden pasar a ser métodos depende a quien esten modificando.

* **`np.array()`** : Crea una matriz de una sola dimension o una fila **([ ])**. Es necesario que para Crear un Array todos los datos sean del mismo tipo ya sea **float** o **int**. La diferencia entre un Array y una lista, es que los arrays son vectores y por ello son muy eficientes y mas rápidos a la hora de hacer cálculos. También podemos operar vectores contra otros vectores cosa que con las listas tendríamos que abrir un ciclo **``for``** para iterar. 
* **`np.asarray()`** : Crea una **copia activa** de un Array, haciendo que si modificamos el Array_A, el Array_B también tendrá la modificación. Viene por defecto el parámetro **``,copy=False``** que conecta las modificaciones a en b.
```python
    Lista = [1,2,3,4,5,6]
    a = np.array(Lista) # Convertimos la Lista A a un Array

    b = np.asarray(a)
```
* **`np.mat()`** : Crea una matriz de 2 dimensiones *('1 2;3 4')*
* **`np.arange()`** : Funciona exactamente igual que la clase `range`. Genera una lista de valores dentro de un intervalo dado. Por defecto el intervalo mínimo es 0.
  ```python
      np.arange(3)
      # array([0, 1, 2])
      np.arange(3.0)
      # array([ 0.,  1.,  2.])
      np.arange(3,7)
      # array([3, 4, 5, 6])
      np.arange(3,7,2) #Intervalo de 2 en 2.
      # array([3, 5])
  
  ```
* **`np.linspace()`** : A diferencia de `arange` indicamos la cantidad de divisiones de nuestro array. Es un intervalo Especificado.
* **`np.unique()`** : Retorna valores unicos y de manera ordenada.
* **`np.where()`** : Es una manera mas sencilla de escribir condicional "**if else**". `(modificacion,si cumple,no cumple)`
* **`np.choice()`** : Elige aleatoriamente valores de una lista o set de datos, también podemos obtener muestras aleatorias de una matriz unidimensional y devolver las muestras aleatorias de una matriz numpy.
    ```python
    indice = np.random.choice(np.arange(len(X_train)),24 , replace=False)
    ```
* **`np.array_split()`** : Particiona las filas de una matriz o DataFrame en partes iguales creando más dfs con las divisiones.
    ```python
        #(df,nºpartes a divir filas)
        partes_df = np.array_split(df,10) #creara 10 df con igual nº de filas

    ```
* **`np.zeros(3) `** : Crea un array de "0"s: array([0., 0., 0.]).
* **`np.random.<método>(3) `** : Llama a la librería Random que esta incorporada en numpy para hacer uso de sus métodos.<br> 
Crea un array con números aleatorios siguiendo una distribución normal por defecto pero podemos indicarle otro tipo de distribución (.normal, .unirforme, .exponential)
  ```python
        datos = np.random.normal(size=10000) #Creara 10k números aleatorios con distribución normal.
        
         datos = np.random.randint(1, 100, size=1000) #Crea números decimales entre 2 rangos especificados con una distribución "Uniforme Discreta".
         #,size = cantidad de nº, da por defecto un array. También puede ser (nºfilas,nºcolumnas) arroja matriz.

        datos = np.random.choice(Lista, 24, replace=False, p=[0.1, 0, 0.3, 0.6, 0]) 
        #Elige 24 números al azar de la lista. 
  ```
* **`np.vstack((np1, np2)) `** : Apila de manera vertical un array debajo de otro array. Puede usarse para actualizar tablas con nuevos datos (nuevas filas de datos).

---
### Metodos de Numpy:
* **`.reshape()`** : Esto ordena a una matriz de 2 dimensiones que deseamos sin modificar la matriz original. *ejm: (4,5) 4 filas 5 columnas*. (Van al final de otro método ligado)
* **`.resize()`** : Al igual que `.reshape` ordena una matriz pero reemplazando a la matriz original
* **`.astype()`** : Cambia el tipo de "kind" de data. *(int o float)* 
* **`.argsort()`** : Ordena una columna en base al ordenar de menor a mayor la columna de referencia.<br>
**low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()[::-1]][:10,:]**
**Col_Ordendar[Col_Referencia[1D de ser necesario].argsort()[Mostrar de mayor a menor]][nº Valores a mostrar]**
  
  ```python
        sat_scores = np.array([1440,1256,1543,1043,989,1412,1343]) #Ordenara de menor a mayor esta columna
        students = np.array(["Jhon", "Bob", "Alice", "Joe", "Jane", "Frank", "Carl"]) #En base a eso acomodara los nombres.

        # Asume el valor para cada nombre en el orden dado ejemplo: "Alice" (columna 2) = 1546 (nº mayor de la lista)
        # [::-1] muestra los resultados de menor a mayor.

        z = students[sat_scores.argsort()[::-1]] #Ordenara students en base de ordenar de menor a mayor sat_scores

        #array(['Alice', 'Jhon', 'Frank', 'Carl', 'Bob', 'Joe', 'Jane'], dtype='<U5')

        z = students[sat_scores.argsort()[::-1]][:3,:] #Mostrara solo 3 primeros nombres.
        #array(['Alice', 'Jhon', 'Frank'], dtype='<U5')

        from sklearn.preprocessing import StandardScaler, RobustScaler
        saleprice_scaled = StandardScaler().fit_transform(df[['SalePrice']]) #Transformamos el valor de "SalePrice" a 2D.
        saleprice_scaled #Nos arroja la distribución de los datos ya ordenado de menor (-) a mayor (+).

        #En este caso convertimos de 2D a 1D seleccionando solo una columna de array "saleprice_scaled[:,0]"
        low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10,:]
  ```

#### Métodos Estadísticos:
* **`.mean()`**
* **`.sum()`**
* **`.cumsum()`**
* **`.min()`**
* **`.max()`**
* **`.std()`**
* **`.var()`**
* **`.corrcoef()`**
---
### Propiedades:
Estas no usan parentesis ni modifican el valor ni forma del original.

* **``np.newaxis``**:  es un alias de None, que se utiliza para indexar matrices en Python. El uso más sencillo de numpy.newaxis es agregar una nueva dimensión a un array NumPy en Python. Por ejemplo, convertir un array 1D en un array 2D, convertir un array 2D en un array 3D, etc. 
  ```python
        vector = [ 0.04965172  0.04979645  0.04994022  0.05008303] # esto es vector[0:4,] no posee columnas.
        vector[:, np.newaxis][0:4,]
        #output: [[ 0.04965172][ 0.04979645][ 0.04994022][ 0.05008303]]
  
  ```
* **`.ndim`** : Indica la dimension bien 1 o 2.
* **`.shape`** : Indica la cantidad de listas y datos dentro de ellas. *(filas [ ] , columnas)*
* **`.dtype`** : Indica el tipo de "kind" de dato *(int32 o float64)*
* **`.size`** : Indica la cantidad de datos que hay en la matriz.
* **`.itemsize`** : Indica la cantidad de bytes que se va a almacenar.
* **`.T`** : Transpone la forma de una matriz sin modificar el original. Ejm: matriz 2x4 --> 4x2

---
### Parámetro

Van ligados a una función o método por una **"`,`"**

* **`,end=', '`** : Indica que el resultado este separado por comas y un espacio.
* **`,num=`** : indica el numero de segmentaciones para el método `.linspace`
* **`,axis=`** : Dirige el procesamiento a filas o columnas de la matriz. **0** para **filas** y **1** para **columnas**.
* **`,dtype=`** : Indica el tipo de "kind" de dato *(int32 o float64)*
---
### Arreglos 
Son funciones únicas de `np` para lo cual las llamaremos con **`%`**.<br>
Estos son usadas en **DataScience** debido a que son mas rápidas en ejecución que una lista convencional a la hora de procesar grandes volúmenes de datos u operaciones.

* **`%timeit`** (variable) : asigna un contador a la operación de la variable que estamos creando.

###  Aplicaciones Estadísticas con Pandas:

* **`Emparejar Arrays con Nulos`** : En caso tengamos 2 listas que no tienen el mismo tamaño las emparejamos de nulos con la siguiente iteración.
```python
    p = [[1,2,3,4,5,6], [0,5]]

    n = len(max(p, key=len)) # Nos arroja el número de la lista màs grande.

    # Empareja a las listas mas pequeñas con la mas grande llenándolas de nulos.
    p2 = [x + [None]*(n-len(x)) for x in p] 
    p2
```

* **`np.log(df)`** : Este parámetro aplicamos una transformación logarítmica a los datos indicados. Debido a que los datos presentan asimetría positiva y no sigue la línea diagonal. Cabe resaltar que para aplicar la `transformación logarítmica` es necesario que no haya datos con valores "0".
  
    ```python
        # Aplicamos la transformación logarítmica a "SalePrice".
        df['SalePrice'] = np.log(df['SalePrice']) 

        #Verificamos la mejoría de los datos y su alineación. 
        from scipy import stats   #Para el gráfico .probplot(df)
        from scipy.stats import norm  #Para el gráfico de distplot

        sns.distplot(df['SalePrice'], fit=norm) #Fit=norm : Ajusta la distribución a una normal.
        fig = plt.figure()

        res = stats.probplot(df['SalePrice'],plot=plt)
    ```

## [5. re (regex)](#indice)

**Regular Expressions** Es una librería para trabajar con expresiones regulares. Estas son verificaciones por patrones o parametros de busqueda. *(cap 8.4)* [Regex101](https://regex101.com/)

### Funciones:
Las funciones **r-strings** tienen el siguiente formato:<br>
        `.funcion(comando a verificar,variable o string,parametro=x)`

* **`re.fullmatch()`** : Verifica si coincide el **string completo** en la variable alojada o parametros especificados ((variable o parametros digitados,"string")). Imprime el resultado en forma de Expresion.<br><br>

* **`re.search()`** : Similar a `.fullmatch()` este verifica la coincidencia de almenos una cadena del string, hace una búsqueda para un dato en especifico. Imprime el resultado en forma de Expresión. Opciones de impresión, es necesario un método `.group()` o `.groups()` para mostrar la cadena que coincidió.<br>  ***,flags=re.IGNORECASE** indica que ignore las mayúsculas en la verificación de los strings*. 

```python
        busca3 = re.search("GUIDO","Guido Van Rossum",flags = re.IGNORECASE)
        busca3.group() if busca3 else "no se encontro"

        # Listar en la carpeta solo archivos .csv
        data =  [x for x in os.listdir('files') if re.search(".csv$",x)]
        data
```
* **`re.findall()`** : Extrae todas las coincidencias del texto todos los datos con el formato que le indiquemos. A diferencia de `.search()` extrae datos para más de una búsqueda de manera simultanea. Imprime el resultado
```python
        usuario = 'Tel_casa:52-1234-1234, Celular:52 4321 4321'
        re.findall(r'\d{2}-\d{4}-\d{4}|\d{2} \d{4} \d{4}',usuario)
        # | : Indica "or"

```
* **`re.finditer()`** : Es exactamente lo mismo que `.findall()` solo que  Imprime el resultado en forma de expresión.

* **`re.sub()`** : Al igual que `.replace()` Remueve el texto que le indiquemos, la diferencia es que este lo hace diferenciando texto y expresiones regulares.<br> ***,count=(nº)** indica cantidad de reemplazos al string*.
```python
        re.sub(r"\n",",","Salto 1\nSalto 2\nSalto 3",count=1)
        # (r"buscar","reemplazar","texto o variable", count= primera que encuentre)
```
* **`re.split()`** : Al igual que `.split()` segmenta el texto que le indiquemos. Podemos indicarle mediante expresiones regulares los espacios, tabulaciones, etc, que hallemos impreso en el texto.<br> ***,maxsplit=(nº)** indica cantidad de segmentaciones al string*.<br>
```python
        quitar_espacios = re.sub(r"\n+",",","W\nX\n\nY\n\n\nZ") #'W,X,Y,Z'
        re.split(r',\s*',quitar_espacios)  #Añadirá una coma con espacio: ['W', 'X', 'Y', 'Z']
```

---
### Métodos:

* **`.group()`** : Imprime el dato sacado del string en forma de un grupo total.
* **`.groups()`** : Imprime el dato sacado del string de manera segmentada ya sea conjuntos de texto o caracteres.
---
### Expresiones:

Las expresiones regulares, en general contienen símbolos especiales llamados meta caracteres como
\, @, #, $ * y signos de agrupación como [ ], { }, ( )
En consecuencia, resulta necesario utilizar los r-strings.

#### - Metacaracteres:

Para generar un patron de búsqueda usamos los meta-caracteres, suelen usarse de la siguiente forma:

r"\w{1,3}\s\d{1,2}"

* "`r''`" : Abre una Expresión regular llamado **r-string**, seguido va '  '. Las "," separan los parámetros para cada argumento a digitar.
* "`\d`" : Dígitos (0-9). indica que verificara solo números enteros en el string, {(nºdigitos)}; 4 a mas digitos{4,}; entre 8 y 10 dígitos {8,10}
* "`\s`" : Espacio en blanco.
* "`[ ]`" : verificara si el parámetro escrito dentro se verifica con la variable. Los corchetes no necesitan usar condicional "|". Ejm: [A-Z][a-z]
* "` * `" : Indica verificar caracter por caracter del "string".
* "`+`" : Verifica cualquier tipo de "kind" de expresion sucesiva que cumpla almenos con un parametro indicado anteriormente. Ejm: con un "\n" que encuentre elimina en conjunto la cadena "\n\n\n"
* "` ^ `" : Por si solo indica mostrar el inicio de una cadena de caracteres (string), pero tambien indica encontrar todos los demás caracteres que no están dentro de corchetes.. Ejm: "[^a-z]"
* "` `" : Separa un conjunto ya sea palabras o numeros de otros tal cual se digita el string. Ejm (parametro para texto) (parametro para numero)  --> calle 3876<br><br>
  
* "``\d``"    : Digitos (0-9)
* "``\D``"    : No digitos (0-9)
* "``\w``"    : Caracter de palabra (a-z, A-Z, 0-9, _)
* "``\W``"    : No caracter de palabra
* "``\s``"    : Espacio en blanco (espacio, tab, nueva linea)
* "``\S``"    : No espacio en blanco (espacio '/s', tab '/t', nueva linea '/n')
* "``.``"     : Cualquier caracter excepto nueva linea (codicioso - greedy)
* "``\``"     : Cancela caracteres especiales
* "``^``"     : Inicio de una cadena de caracteres (string)
* "``$``"     : Fin de una cadena de caracteres, se indica al final del caracter. ejm Hola$  

#### - Cuantificadores:

* "``*``"     : 0 o más (codicioso - greedy)
* "``+``"     : 1 o más (codicioso - greedy)
* "``?``"     : 0 or 1 (perezoso - lazy)
* "``{3}``"   : Numero exacto
* "``{n,}``"  : Numero n+
* "``{3,4}``" : Rango de números (Minimo, Maximo), ignorara todo lo que no lo cumpla.
* "``( )``"   : Grupos. Usan condicional "|".
* "``[]``"    : Encuentra caracteres en corchetes. Los corchetes no necesitan usar condicional "|".
* "``[^ ]``"  : Encuentra todos los demás caracteres que no están dentro de corchetes.
* "``|``"     : Condicional O
* "``\b``"    : Se posiciona a los limite de palabras y caracteres
* "``\B``"    : Se posiciona omitiendo los limites de palabras y caracteres.
* "``\1``"    : Referencias. Simula no tener que repetir lo digitado muchas veces, para cadenas de más de un caracter es necesario agruparlos "( )"
---
### Parámetros:

* "**``,flag=re.M``**"  : Interpreta cada linea del texto como un string independiente a pesar que el texto este en su conjunto como un solo string.
* "**``,flags=re.I ó ,flags=re.IGNORECASE``**"  : Interpreta las mayusculas y las minusculas como parte del mismo texto. "HOLA" = "hola"
* "**``,count=(nº)``**"  : indica cantidad de reemplazos al string, para la funcion **`.sub()`**
* "**``,maxsplit=(nº)``**"  : indica cantidad de segmentaciones al string, para la funcion **`.split()`**

## [6. pandas ("*pd*")](#indice)
**Panel Data**. Es una librería que brinda la función de importar documentos `.csv` y usarlos como Dataframes. Es para manejar tablas.<br>
Pandas es una evolución de **numpy** por lo que muchos de sus métodos funcionan aquí:<br>
*Indexados, Métodos Estadísticos*<br>
También incorpora las librerías de: `re`, `datetime`

```python
  import pandas as pd

  pd.set_option('display.max_columns', None) # Muestra todas las columnas del df.

```

Para la carga y exportación de archivos según su tipo de "kind" de archivo o extension: Tener en cuenta que al exportar con el mismo nombre el archivo se sobrescribirá

  * Archivos `.csv`
    * Para cargar archivos **`.csv`** (tiene que estar guardado como .cvs utf-8 (delimitado por comas)))<br>
    ```python
        variable = pd.read_csv('./../carpeta/nombre de archivo.csv', sep=";", encoding="utf-8", usecols=[0,2,3], skiprows=[1,4], index_col=0, parse_dates=["Fecha","Creado El"])
    ```    
      *El ["Fecha","Creado El"] punto indica en la carpeta actual*<br>
      *Los 2 puntos seguidos indica saltar una carpeta hacia atrás*
      
      * **``,usecols=``** : Columnas que quiero que se muestren, las indicamos según su indice.
      * **``skiprows=[0,1,2]``** : Omitirá cargar filas que le indiquemos.
      * **``index_col=str,index``** : Podemos usar una columna[indice] como index para el df, sea ya nombrandolos o indicando el numero de index de la columan. Podemos poner mas de un index para generar **multindex**.
      * **``header=[0,1]``** : Indicamos que numero de fila usara como encabezado, pueden ser mas de 1.
      * **``parser_dates=True``** : Convertirá a formato fecha las columnas que lo sean. O indicando (parser_dates="Column1","Column2")<br><br>
    * Para exportar a formato **`.csv`**, obviando los índices, con encoding **`utf-8`**:
    ```python
        df_tarjetas.to_csv("terrernos2.csv", index=False, encoding="utf-8")

        #En caso queramos exportar en varios archivos un df dividido:
        path = r'C:\Users\foster\Desktop\files"
        partes_df = np.array_split(df,10) #dividimos las filas el df en 10 df con filas iguales

        for ix, df in enumerate(partes)
          df.to_csv(path + "\files" + str(ix+1).zfill(2) + "csv",index=false)
          #Creara archivos con el nombre empezando con nombre_s01.csv
    ```
  * Archivos `.xlsx`
    * Para cargar archivos Excel **`.xlsx`** Tiene que instalarse con un pip install **``openpyxl``** si queremos cambiarlo por otro usaremos ``(,engine="xlrd")``.<br>
    Podemos leer todo el archivo o solo una pestaña especifica con el atributo **`,sheet_name=`** en caso nuestro archivo contenga varias pestañas, también podemos asignarle cada pestaña a una variable en caso asi lo deseemos.
        ```python
        variable = pd.read_excel('./../carpeta/nombre de archivo.xlsx', sheet_name="Nombre_Pestaña_Excel",usecols=[0,2,3],
                                  skiprows=[0,1,2], startrow=4, index_col=0, parser_dates=["Fecha","Hora"])     
        ```
      * **``,sheet_name=``** : puede tener extension "nombre.csv". Igualando a "None" mostrara en un diccionario todos los "Sheet" del cuaderno Excel.
      * **``,usecols=``** : Columnas que quiero que se muestren, las indicamos según su indice.
      * **``skiprows=[0,1,2]``** : Omitirá cargar filas que le indiquemos.
      * **``startrow=4``** : Indica a partir de que fila arme el DataFrame.
      * **``index_col=str,index``** : Podemos usar una columna[indice] como index para el df, sea ya nombrandolos o indicando el numero de index de la columan. Podemos poner mas de un index para generar **multindex**.
      * **``parse_dates=True``** : Convertirá a formato fecha las columnas que lo sean. O indicando (parser_dates="Column1","Column2")<br><br>
      
    * Para Revisar rápidamente solo las pestañas de excel **`.xlsx`**.
    ```python
              df = pd.read_excel("C:/Users/Foster-PC/Documents/G&S Instalaciones/PLANTILLAS/BALDOSAS Y DRYWALL/PLANTILLA BYD 2021.xlsx", None)
              df.keys() #De esta manera arrojará solo el nombre de las pestañas.
    ```
    * Para exportar a formato excel **`.xlsx`**. No importa la extension del archivo lo convertirá a excel (.xlsx).
    ```python
        df_tarjetas.to_excel("terrernos2.xlsx", sheet_name="Hoja1")
        #Podemos exportar en una nueva hoja de nuestro mismo archivo Excel.
    ```
    * Para exportar en multiples pestañas:
    ```python
        df2 = df1.copy() #En caso hagamos una copia del df. 
        with pd.ExcelWriter('output.xlsx') as writer:  
          df1.to_excel(writer, sheet_name='Sheet_name_1')
          df2.to_excel(writer, sheet_name='Sheet_name_2')
    ```
    * Para leer y  Concatenar todos los `sheet_name` de un cuaderno Excel.
    ```python
        df = pd.read_excel("terrernos2.xlsx", sheet_name= None)
        df = pd.concat(df, ignore_index=True) #para tener 1 solo index de toda la tabla.
        df.head()
    ```

  * Archivos `.json`
    * Para Cargar archivos `.json`. index,name,value
    ```python
        pd.read_json("Nombre.json", orient="columns") #columns es la que viene por defecto.
        #no hay "indent". Pero si podemos indicarle el tipo de "kind" de orientación que queremos.
    ```
    * Para Exportar a `.json`. index,name,value. (indent =True or 3,4,5) Respetará la indentación (TAB).
    ```python
        pd.to_json("Nombre.json", indent=True)
         #{"index":{0:1,1:2,2:3,3:4,4:5},"name"{0:A,1:B,2:C,3:D,4:E},"value"{etc}}

        pd.to_json("Nombre.json", indent=True, orient="columns") 
        #Es el mismo que viene por defecto.

        pd.to_json("Nombre.json", indent=True, orient="index") 
        #{"0"{"index":1,"name":"A","value":3.03},"1"{"index":2,"name":"B",etc},

        pd.to_json("Nombre.json", indent=True, orient="split") 
        #{"columns"["index","name","value"],"index"[0,1,2,3,4],"value"[etc]

        pd.to_json("Nombre.json", indent=True, orient="records") 
        #[{"index":1,"name":A,"value":3.03},{"index":2,"name":B,"value":5.14}]

        pd.to_json("Nombre.json", indent=True, orient="values") 
        #[[1,"A",3.03],[2,"B",5.14],[etc]]

        pd.to_json("Nombre.json", indent=True, orient="table") 
        #"schema","Datos"...
    ```  
    * Importar Datasets (csv,etc) de Github:
      Es necesario entrar al archivo por medio de github y copiar y pegar el URL del archivo en formato **RAW**

      ```python
        url = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/archived/ecdc/total_cases.csv"
        datacov = pd.read_csv(url)
        datacov.shape
      ```


___
### Clase, funciones:
* **`pd.set_option()`** : Configura opciones de la librería de pandas.
  ```python
          pd.set_option("precision",4) 
          #Configuramos que muestre 1 decimal.

          pd.set_option("max_columns",9) 
          # Muestra 9 columnas, incluye indice.

          pd.set_option("display.width",None) 
          #None lo ajustara automáticamente.

          pd.set_option("precision",4,"max_columns",9,"display.width",None)
          #Puede Resumirse estos 3 cambios en una sola linea de código.
          
  ```
* **`pd.Series()`** : Crea una **columna** con los datos a asignarle, una Serie son columnas en pandas. Pueden contener forma de diccionarios "``{'a':b}``", Listas "``['a,b']``" o tuplas "``('a,b')``". Para convertir una Serie (1D) a dataframe (2D) usamos el parametro ``np.newaxis`` o tambien podemos encerrar en doble **[[ ]]**, en ambos casos convertira un array de 1D como podria ser el llamar a una columna df["SalePrice"].shape a 2D df[["SalePrice"]].shape<br<br>>

Cabe resaltar que al llamar o seleccionar una columna del dataframe estamos frente a un array de 1D.

  ```python
    In [24]: df['a'].shape
    Out[24]: (5,)      # <--- 1D array

    In [26]: df[['a']].shape
    Out[26]: (5, 1)    # <--- 2D array

    #Esto es igual a:
    In [25]: df['a'][:, np.newaxis].shape
    Out[25]: (5, 1)    # <--- 2D array

    #Convertir de 2D a 1D:
    In [25]: df[['a']][:,0].shape
    Out[25]: (5,)    # <--- 1D array
  
  ```
* **`pd.DataFrame()`** : Crea un DataFrame (tabla). Son Series de 2 dimensiones (Tablas). Se pueden armar DataFrames de diccionarios, listas y tuplas. La mejor manera de crear un `DataFrame` es usando un **diccionario** para poder referenciar sus encabezados y sus valores, ya sean listas, tuplas, etc. <br>
En Caso Tengamos una Listas de Listas `array([[          0],[          0]])` es necesario "Aplanarlas" usando el método `.flatten()` o simplemente indicando la indexacion de todas las filas, una columna = [:,0]
  ```python
          df_tarjetas = pd.DataFrame({"Ubicación":ubicacion, "Precio":precio, "Area":m2})
          #({"nombre_columna",variable_con_datos}), variables previamente definidas (Listas)

          Model_Accuracy = pd.DataFrame({"Accuracy":history.history["accuracy"],
                                "Val_Accuracy":history.history["val_accuracy"]})
          #Para devolver un Array de 1D de un Dataframe de una Lista de Listas:
          pd.DataFrame({"Survive":predictions_out.flatten()})
          pd.DataFrame({"Survive":predictions_out[:,0]}) #Ambos son validos nos dan 1 columna o 1D

          #Cuando indicamos un diccionario con un solo valor:
          files = {'A.txt':12, 'B.txt':34, 'C.txt':56, 'D.txt':78}
          filesFrame = pd.DataFrame(files.items(), columns=['filename','size'])

          #Otra manera de Crear un DataFrame:
          pd.DataFrame(predictions_out, columns=["Survive"])

          #,columns= : referencia a la columna, si ya la especificamos esta servirá para asignarle un nombre a la columna.

          pd.DataFrame(history.history["accuracy"], columns=["Accuracy"]), #de no indicar la columna no cambiara el nombre.

            
  ```
* **`pd.ExcelWriter()`** : Clase para escribir objetos "DataFrame" en hojas de Excel. (uso en exportar en multipestaña)
* **`pd.concat([datas])`** : Concatena tablas (df) o columnas de un df una debajo de otra por defecto `,axis=0` (por fila), pero también podemos concaternarlo una tabla a lado de otra usando `,axis=1` (por columna) retornando la compilación de ellas en una nueva tabla. Va entre "**[ ]**" porque sera una lista de DataFrames ([df_1,df_2,df_3])
 
  ```python
        df = pd.concat([df_1, df_2, df_3], axis=1 ,sort=True, ignore_index=True) #Juntará todos los dataframe en uno solo. 
        #,axis=1 Concatena por columna     
        #,sort=True1 Que lo ordene.
        #,ignore_index= True Junta los indices de la concatenacion en uno solo, Creara un index enumerado eliminando los demás index previos.
        #,join= default {"outer"}
        #,key= ['Total', 'Percent'] #Cambia el orden de presentación de las columnas.


        df = pd.concat([df_train["SalePrice"],df_train[YearBuilt]], axis=1)
        #Concatenamos la Columna de un "df_train" con otra del mismo "df_train", guardandose esta nueva tabla en "df".

        #Creamos 2 columnas y las unimos con concat manteniendo el mismo index:
          total = df_train.isnull().sum().sort_values(ascending=False)
          percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
          missing_data = pd.concat([total, percent], keys=['Total', 'Percent'], axis=1)
          missing_data.head(20)

  ```
  * Un ejemplo para concatenar varios archivos en un ciclo ``for``:

  ```python
      #Concatenando archivos que solo tengan terminación ".csv"
      import re, os 

      data =  [x for x in os.listdir('files') if re.search(".csv$",x)] #carpeta esta en el mismo directorio de nuestro archivo .py

      df_Prueba = pd.DataFrame() #Creamos un df vacío.
      for i in data:  #variable data contiene en lista los archivos [achr1.csv,achr2.csv,achr3.csv]
          df_temp = pd.read_csv("files/" + i)   #creara un dataframe por diccionario
          df_Prueba = pd.concat([df_Prueba, df_temp])   #Juntará todos los dataframe en uno solo.      
  ```
  * Para leer y  Concatenar todos los `sheet_name` de un cuaderno Excel.
    ```python
        df = pd.read_excel("terrernos2.xlsx", sheet_name= None)
        df = pd.concat(df, ignore_index=True) #resetea los index contando de 0 para las tablas concatenadas.
        df.head()
    ```
* **`pd.to_numeric()`** : Formatea una columna a formato **numerico**. 
  ```python
    df["Price"] = pd.to_numeric(df["Price"], errors="coerce")
    #,errors="coerce" : Indica "forzado", el análisis no válido se establecerá como NaN.
  ```
* **`pd.to_datetime()`** : Formatea una columna a formato **date**. Arrojando el formato americano año,mes,dia,hora<br> 
Al tener un dato en formato fecha tenemos la ventaja de convertir esta columna fecha en nuestro index mediante `.set_index("Col", inplace=True)` y hacer la búsqueda por fecha sea poniendo solo el año con `df.loc["2020"]` Ejm:

  ```python
      df['Order Date'] = pd.to_datetime(df['Order Date'])
      #Convierte una columna en formato Fecha.
  ```
  * Especificando el formato de fecha que tenemos en la columna, lo convertirá siempre a formato americano *año mes dia hora*:
  ```python
        df['Order Date'] = pd.to_datetime(df['Order Date'],format= "%Y-%m-%d %H:%M:%S") # "2022-01-04 11:35:50" Solo este formato con años.
        
        data["Fe, de picking"] = pd.to_datetime(data["Fe, de picking"], format= "%d/%m/%Y") #En la columna esta como dia/mes/año     
  ```

  * Para cambiar el formato americano a un formato personalizado usamos `dt.strftime()`, considerar que estamos cambiando el formato a `String`:
  ```python
      df['Order Date'] = df['Order Date'].dt.strftime("%d/%m/%y")  #dia/mes/año

      #Para indicar los meses por sus nombres en Español
      df['Order Date'] = df['Order Date'].dt.month_name(locale='es_ES.utf8') # en_US (ingles)

  ```

  * Aumentar fechas a una fecha base:
  ```python
        pd.to_datetime(160, unit='D', origin='2020-02-01')
        #Aumente 160 días a una fecha Base.
        #origin= puedes ser también "unix"

  ```

* **`pd.PeriodIndex()`** : Permite hacer agrupaciones de fechas ya sea por dia, mes ,trimestre,año , etc. Se suele usar en los parámetros de `df.pivot_table()` y  `groupby()`.
  
  ```python
        #pd.PeriodIndex(Columna, freq='M')
        pd.PeriodIndex(df_new["Fe, de picking"], freq='M')

        #En pivot_table
        df_2 = df_new.pivot_table(index=pd.PeriodIndex(df_new["Fe, de picking"], freq='M'), values=["Cantidad ","PT"],
                                  aggfunc="sum").sort_index(ascending=True)

        #En groupby
        grupo = df_new.groupby(pd.PeriodIndex(df_new["Fe, de picking"], freq='M')).sum().sort_index(ascending=True).reset_index()
        #reset_index() creara un nuevo index enumerando el numero de filas {0,1,2,3}
  ```
  
* **`pd.get_dummies()`** : **One-Hot-Encoding** ("Variables Nominales"). Transforma los datos de una columna categórica (columna que tiene clasificaciones) a una forma en la que python pueda entender de que no se esta aportando valor sino una clasificación. Para ello creara varias columnas según el tipo de "kind" de valores únicos que encuentre en la columna que vamos a aplicar esta función llenando de valores binarios estas nuevas columnas para que python entienda en determinada fila cual de las categorías encontradas se activo.<br>
Esta clasificación se suele usar en la preparación de datos para posteriormente procesarlo con ML o Redes Neuronales.<br>
Un punto negativo de esta Categorizacion es que crea una nueva columna por cada categoria unica que encuetra, haciendo que en ciertos casos nos genere demasiadas columnas nuevas el cual acarrea el Problema de la Dimensionalidad. Este problema nos fuerza a que tengamos mucha cantidad de datos a mayor dimensiones de nuestra tabla para reducir el sesgo por falta de datos.<br>.
Un punto Positivo es que debido que categoriza nuestros datos en 0 y 1 hace que no sea necesario el escalamiento de estos datos a diferencia del **Ordinal-Encoder** (se usa para categorizar variables que guardar una relacion de proximidad "Variables Ordinales").

  ```python
    df = pd.get_dummies(df["Pclass"], prefix="Pclass"), axis=1 #Tabla con los nuevos campos en binario.

    #Otro ejemplo:
    df = pd.concat([df, pd.get_dummies(df["Pclass"], prefix="Pclass")], axis=1)
    df.drop(["Pclass"], axis=1, inplace=True) #Una vez clasificado borramos la columna origen. Concatenamos por columna.

    #,prefix="Nombra_Columna" = Podemos cambiar o conservar el nombre para el cual usara como base para enumerar según la cantidad de valores únicos o categorías encuentre en esta columna. "Nombra_Columna_1", "Nombra_Columna_2", "Nombra_Columna_3 .. etc"
  ```

* **`pd.cut()`** : A diferencia de las categorizaciones de valores categoricos, aqui categorizamos valores numericos, indicando el numero de categorias a segmentar o especificando un rango para cada segmento de valores numericos. 

  ```python
    edades = np.array([1,7,8,15,28,35,50,55,70,75,100])
    # Para que segmente de forma Automatica:
    segmentacion = pd.cut(edades, bins=3, labels=["baja", "media", "alta"], include_lowest=True, retbins=True)

    #bins= : indica el numero de segmentos automaticos que queremos que calcule y separe. Tambien Podemos especifiar un rango.
    #labels= : Etiquetas de Categorizacion.
    #include_lowest=True : Indicamos que incluya el valor màs bajo de los datos numericos. 
    #retbins=True : Indica que muestre o retorne los rangos de los contenedores que uso de manera automatica o que especificamos.

    # Para que segmente de forma Especifica segun los intervalos que indiquemos (Hay parametros adicionales al indicar los rangos a segmentar):

    segmentacion = pd.cut(edades, bins=[0, 11, 17, 59, np.inf]
                      , labels=["infante", "joven", "adulto", "mayor"], include_lowest=True, retbins=True
                      ,right=True)

    #right=True : Tomara los saltos entre intervalos como <= ej: 0-11 ; 11-17 ; 17-59 ; 59-np.inf
    
    # Para verificar la categorizacion que hemos realizado:

      print(segmentacion[1], "\n") #Nos Arroja los valores de los intervalos que uso para la segmentacion.
      print(segmentacion[0].categories, "\n") #Nos Muestra nuestras etiquetas que definimos.
      print(segmentacion[0].codes, "\n") #Nos arroja la categorizacion de forma numerica. [0,0,1,1,2]
      print(np.array(segmentacion[0])) #Nos arroja la categorizacion con las Etiquetas que indicamos.

  ```

Para Pandas la manera común de filtrado es:
 * Filtrar por columnas: df[["Col1", "Col2"]]
 * Filtrar por filas o nº de index: df[df.index == 38]
 * Filtrar por un Atributo de la columna: df[df["Sexo"] == "Masculino"] 
 * Filtrar por Varios Atributo de la columna: df[df["Clase"].isin([1,2])]
 * Borrar o Excluir valores de un Filtro: df = df[~df["Col"].isin(["Valor_1","valor_2"])]  #Solo funciona con .isin([])

**df[<Filtro> df[] operador][[Lista de columnas a mostrar en la tabla filtrada]].where(<filtro_2>).(quitar nulos, ordenar, +métodos)**

```python
  df[df["Creado Por"] == "JARCAN"][["Descrición Material","Tipo de Orden"]].where(df["Tipo de Orden"] == "OC").dropna()

```
* Filtrar por multiples Criterios usando "&": 
  ```
  df = pd.DataFrame(
    {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]})

  Crit1 = df[AAA] <= 5.5
  Crit2 = df[BBB] == 10.0
  Crit3 = df[CCC] > -40.0
  AllCrit = Crit1 & Crit2 & Crit3

  df[AllCrit]
  ```
___

### Métodos para DataFrames:
Estos métodos son para una variable donde este alojada el DataFrame.

Se puede llamar a una columna digitandolo entre **`['Columna']`** o digitando su indice, pero también digitandolo como atributo **`.nombre_columna`** Digitandolo por su nombre pero este ultimo no es para columnas con palabras separas por un espacio en cambio el otro por corchetes se puede llamar a más de una columna con un doble corchete: **`[['Columna_A'], ['Columna_B']]`**

* **`df.describe()`** : Trae todas las estadísticas relevantes: `count`, `mean`, `std`, `min`, `25%`, `50%`, `75%`, `max`, `dtype`.
  ```python
    df.describe(include= "O") # Trae las estadisticas de datos tipo "Object".
  
  ```

* **`df.astype()`** : Convierte el resultado bien a ``("int","float","object","string","datetime64[ns]","bool")``
```python
          df_distribution.astype({"Cantidad ": "float"}).dtypes
          #Cambia de int a float en una columna en especifico

          df["col1"] = df["col1"].astype(str)
```
* **`df.tolist()`** : Engloba los datos del df o una columna en una lista de datos. "[a,b,c,d]"
* **`df.sort_index()`** : Ordena el indice. Por defecto viene en ascendente. *(,ascending=True)*, para columnas *(,axis=1)*.
Siempre en importante despues de haber improtado data, ordenar el index.
  ```python
    df.sort_index(inplace=True)
  ```
* **`df.reindex()`** : Ordena el index según el orden de las columnas que le pasemos.
  ```python
        df.columns = ["a,b,c,d,e,f"]
        df = df.reindex(columns=['a','f','d','b','c','e']) #Cambiara el orden de las columnas al que indicamos. 
  ```
* **`df.set_index()`** : Indicamos que columna queremos como indice base. Tambien podemos hacer multi-index para tener varios indices (relacionados como categorias) eligiendo las columnas que queremos como indices.
```python
          df.set_index("Fechas", inplace=True)
          #Ponemos como index una columna de tipo fecha.

          #Multi-Index:
          df.set_index(["Region","Provincia","Distrito"], inplace=True) #Multi-Index de 3 niveles.

          #Para hacer un filtro sensillo por Multi-Index usamos la propiedad loc.
          df.loc["Lima"] #Nombramos la Region que queremos, ya sea Region, Provincia o Distrito.

          #Para hacer un filtro avanzado contemplando los 3 niveles de Multi-Index usamos la propiedad loc + pd.IndexSlice
          #df.loc[pd.IndexSlice[primer nivel, segundo nivel, [tercer nivel_1, tercer nivel_2]], "Col_A", "Col_B ]
          df.loc[pd.IndexSlice[:, :, ["Los Olivos", "Comas"]], :] 


```
* **`df.reset_index()`** : Corrige para que el df tenga un index aparte de las columnas.
* **`df.insert()`** : Añade una columna o lista de valores a lado derecho de la posición de columna que le indiquemos.
```python
          df.insert(loc=2, column="Número de pedido", value=df["Pedido"])
          #loc= nº columna, lo insertara a lado derecho
          #column= Nombramos la columna 
```
* **`df.replace()`** : Reemplaza valores de los valores de las columnas que le indiquemos.
  ```python
          #Reemplaza los valores True por 1 y False por 0
          df["paid"] = df["paid"].replace({True:1, False:0})  

          df["col_1].replace(to_replace= "texto regex a buscar", value= "valor de reemplazo", regex=True)
  ```

* **`df.sort_values()`** : Ordena los valores. Por defecto viene en ascendente. *(ascending=True)*, para columnas *(,axis=1)*
  ```python
          df.sort_values(by=["Price","Score","Link"], ascending=[True,False,False])
          # Otra manera de ordenar de menor a mayor a columna "Price". by= funciona para listas o no.

  ```
  En caso los datos o filas de la Columna estén nombrados con mayúsculas y minúsculas ("grupo A") se tiene primero que homogeneizar antes de ordenar para ello usamos el parámetro `,key= lambda x : x.str.lower()`
  ```python
          df.sort_values("Race/Ethnicity", ascending=True, key= lambda x:x.str.lower())

  ```
* **`df.shape`** : Muestra la cantidad de filas y columnas de la data. Para obtener el total de datos usamos. `df.shape[0]*df.shape[1]` *(filas * columnas)*
* **`df.size()`** : Muestra la cantidad de valores en el DataFrame. *(filas * columnas)*
* **`df.head()`** : Muestra las primeras n filas del df.
* **`df.info()`** : Muestra la información general con un `head()` incluido del df. Este método se suele usar para ver si hay **valores nulos** en las **columnas**.
* **`df.tail()`** : Muestra las ultimas n filas del df.
* **`df.rename()`** : Renombrar. Ejm: 
```python
          #Renombrar una columna
          restdf = restdf.rename(columns={"Atendió":"Mesero"})

          #Renombrar el Index
          restdf = restdf.rename(index = {0:"Not Survived", 1: "Survived"})

          #Renombrar todas las primeras columnas de dfs:
          for i in range (0,10,2): #Va de 2 en 2 
          df = tablas_html[i]
          df.rename(columns={df.columns[1]: "Team"}, inplace=True) #renombra todas las primeras columnas a "Team"
          df.pop("Qualification")
```
* **`~`** : Indica "No". En vez de arrojarnos el filtro que le indiquemos hace lo contrario, excluye el filtro que le indicamos de la tabla. Ejemplo Borra todos valores arrojados de un filtro. df[~<Filtro>.metodos()]<br>
Solo funcion con: `isin.([""])` 
  ```python
    #Borra o Excluir todos valores arrojados de un filtro.
    df = df[~df["Nombre Proveedor"].isin(["HAMANN DISEÑO Y CONSTRUCCION S.A.C.","BANCO DE CREDITO DEL PERU"])]
    df.reset_index(inplace=True) #Reinicia el índice del df para que cuente desde 0.
  ```
* **`df.pop()`** : Borra una columna por su nombre.
  ```python
    df.pop("Qualification")
  ```
* **`df.drop()`** : Borra un dato al indicar su nombre o index para borrar columnas y solo nº de index para borrar filas.`,axis=0` para borrar fila o `,axis=1` para borrar columna.
  ```python
    #Para borrar una fila del df indicando un valor de la columna.
    df.drop(df.index[df["Cabin"]=="T"], axis=0, inplace=True)
  ```
* **`df.duplicated()`** : Muestra las filas donde encuentre datos duplicados. Para ello podemos indicarle las columnas de la tabla que queremos que haga la busqueda con `,subset=[]`.
  ```python
    df[df.duplicated(subset=["Date","Rank","Title","Category"], keep= False)]
  ```
* **`df.drop_duplicates()`** : Elimina elementos duplicados.
  ```pyhthon
    df.drop_duplicates(inplace=True)
  ```

* **`df.dropna()`** : Borra filas o columnas enteras donde encuentre 1 datos en blanco o un total de fila o columna en blanco *(NaT,NaN)*. Por defecto borrara la filas en donde encuentra 1 dato null, es necesario usar *(inplace=True)* para aplicar el cambio.

  ```python
    df_population_raw.dropna(inplace=True, axis=0, how="any")
    #modo por defecto ,axis=0, how="any".

    df_population_raw.dropna(subset=["Col_1", "Col_2"] inplace=True, axis=0, how="any")
    # ,subset= : Indicamos que columnas con nulos son relevantes para eliminar filas de la tabla.
  ```
  * axis = 0 : fila, 1 : columna
  * how = "all" : toda la fila o columna debe estar llenas de null, "any" : basta 1 solo null que encuentre.<br><br>

* **`df.isnull() o df.isna()`** : Arroja elementos nulos, vacíos (NaN). En combinacion son un operador como `.sum()` arrojará la cantidad de valores nulos de cada columna.
  ```python
    df.isna().sum()
    df.isnull().sum()
    #Ambas formas dan el mismo resultado.

    df['Código'].isnull().unique()
    #Busca elementos nulos únicos en la columna "Código"    
    
    tabla_despv[tabla_despv["Creado Por"].isnull()]
    #Mostrara la tabla conteniendo todas las filas donde la columna tenga valor nulo.

    # Arroja la cantidad total de datos incluidos valores nulos de cada columna
    # Esto es debido a que count contara los valores true y false.
    df.isnull().count().sort_values(ascending=False)  
       
  ```
* **`df.fillna()`** : Reemplaza los valores **NULL** con un valor **específico** a indicar. El método fillna() devuelve un nuevo objeto DataFrame a menos que el parámetro inplace se establezca en True , en ese caso, el método fillna() hace el reemplazo en el DataFrame original. <br>
Se suele utilizar este metodo para no borrar los valores nulo y se suelen sustituir por la **moda** de la columna.<br>
El parámetro `method=` es utilizado para rellenar huecos en serie reindexada relleno/relleno: propagar la última observación válida hacia adelante hasta el siguiente relleno válido/bfill: utilizar la siguiente observación válida para rellenar el hueco. {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}

```python
          df.fillna(0,inplace=True)
          #Reemplazara los valores nulos con un "0"

          df.fillna(method="ffill",inplace=True)
         #Este método propaga los Nan hacia adelante

          values = {"Col_A": 0, "Col_B": 1, "Col_C": 2, "Col_D": 3}
          df.fillna(value=values,inplace=True)
          #Asignara los valores que hemos definido en cada valor nulo de cada columna del df que encuentre.
          
```
* **`df.iterrows()`** : Separa el Index de los datos por columna concatenandolos por cada nuevo valor de la columna.
```python
          for row, datos in restdf.iterrows(): #rows= index , datos=df concatenado con todos sus valores de las columnas.
              Orden  = datos["Orden"]
              Tipo  = datos["Tipo"]
              Producto = datos["Productos"]
```
* **`df.unique()`** : Arroja el nombre de los elementos únicos.
* **`df.nunique()`** : Arroja la cantidad de los elementos únicos.
```python
          df["Tipos"].nunique()
          # 5 (resultado forma numérica)
```
* **`df['Columna'].value_counts()`** : Arroja una lista indicando el recuento de datos (valores) únicos en la columna. Este es muy usado para explorar el contenido de las columnas de un df.
```python
          
          df[['Código','Colm_2']].value_counts()
          #arrojara un recuento de los valores únicos de os pares de datos únicos por las 2 columnas.
          
          df['Código'].isnull().value_counts()
          #Arroja buleano con conteo de elementos nulos en la columna "Código"          
```
  * `(normalize=True)` : Obtiene la "Frecuencia Relativa". Indica su proporción con respecto al total en decimales.
    ```python
          df['Código'].value_counts(normalize=True, dropna=False).round(2)
          #Ejem: Female = 0.55 ; males = 0.45; NaN = 1 (redondeado mostrando 2 decimales)
          #Con dropna establecido en False , nos dará el recuento de elementos NaN.
    ```

* **`df.apply()`** : Crea una columna calculada con la función que le indiquemos. Por lo general va compuesta por `lambda`, pero también podemos indicar una función de python o una propia que hayamos creado.
```python
          df["Dígitos"] = df["Cuenta"].apply(lambda x : len(str(x))) #cuenta nº de caracteres.
          df["Atrittion"] = df["Atrittion"].apply(lambda x : 1 if x == "Yes" else 0 ) #cuenta nº de caracteres.

          #Creamos una columna a partir de separar un texto de una columna por un caracter:
          df["Ciudad"] = df["Provincia"].apply(lambda x: x.split("-")[0]) #cortamos y nos quedamos con el primer segmento.
```
  * Limpiar columna numerica con indicador de miles o millones ("100M").
```python
          #Definimos una funcion a reemplazar datos. 
          def unids(x):
            if x[-1] == "M": #Indica ultimo caracter.
              return(float(x[:-2])*1000000) #Multiplique el numero excluyendo la letra "M" y un caracter de espacio.
            else: 
              return(float(x[:-2])*1000)

          # Usamos .apply para aplicar la funcion que hemos creado.
          df["Pagos"] = df["Pagos"].apply(unids)
          
          # Dar formato de dolar a un numero:
          f"${valor:.2f}"  => $ 50.00


```
* **`df.applymap()`** : Aplica cambios o reemplaza datos a las columnas que le indiquemos, segun el patron que designemos.
  Aplicamos un diccionario con comandos que indiquemos.
  ```python
    df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})

      df
      Out[54]: 
        AAA  BBB  CCC
      0    1    1    2
      1    2    1    1
      2    1    2    3
      3    3    2    1

      # Asignamos los nombres de las columnas del df
      source_cols = df.columns  
      new_cols = [str(x) + "_cat" for x in source_cols] #Creamos 3 columnas con _cat al final.

      categories = {1: "Alpha", 2: "Beta", 3: "Charlie"} #Configuramos nuestra categorizacion.
      df[new_cols] = df[source_cols].applymap(categories.get) # Aplicamos la Categorizacion a nuestras 3 columnas creadas.

      df
      Out[59]: 
        AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat
      0    1    1    2    Alpha   Alpha     Beta
      1    2    1    1     Beta   Alpha    Alpha
      2    1    2    3    Alpha    Beta  Charlie
      3    3    2    1  Charlie    Beta    Alpha
  ```

* **`df.isin()`** : Sirve para filtrar por los valores de una columna, Retorna un booleano por lo que es recomendable para una mejor visualización aplicar una función de agregación al final o encerrarlo en un df.
```python
          df[df["Tipo de Orden"].isin(["OC", "OS"])] #Muestra toda la tabla con el filtro.
          df[df["Tipo de Orden"].isin(["OC", "OS"])]["Tipo de Orden"] #Mostrara solo esta columna en la tabla
          
```

* **`df.where() o np.where()`** : Es una condicional de formato **(Filtro,Valor_if,Valor_else)** que viene de numpy. Ejm: Para crear una columna con condicionales:
```python
    preciosdf["Margen"] = np.where(preciosdf["Ganancias"]>100,"Alto","Bajo")
    #Creamos una columna rellenando en funcion si cumple tendrá un valor de "Alto" else "Bajo".

    # Buscamos el precio donde GlivArea es igual a 4676 (Area de vivienda), también arroja su index.
    df["SalePrice"].where(df["GrLivArea"] == 4676).dropna()

    #Crea una tabla en el que se mostrarán las columnas "Descrición Material","Tipo de Orden"
    df[df["Creado Por"] == "JARCAN"][["Descrición Material","Tipo de Orden"]].where(df["Tipo de Orden"] == "OC").dropna()

    #Reemplaza valores a 0 donde df["A"] es nulo.
    df["A"].where(df["A"].isnull(),0)

```
* **`df.groupby()`** : Creara una tabla dinámica con las filas de la columna indicada (puede ser más de una Columna).El método se lo agregamos al final. Se puede usar los `groupby()` como input para armar una tabla pivote ``pd.pivot_table()``.
Este metodo es Iterable para extraer en un ciclo for.<br>
*No usa parámetro ,",aggfunc=" pero si .agg({"Columna":"Opercion"})*<br>
*Parametro: dfgroupby().filter() y dfgroupby().transform()*
  ```python
            grupo_Tipo = restdf.groupby(["Tipo","Producto"]).mean()
            grupo_Tipo.loc["Bebida"] #Mostrará las filas de "Tipo" -> "Bebida"

            grupo_Tipo = restdf.groupby(["Tipo","Producto"], as_index=False).agg({"InvoiceNo":"unique"}) #valor unicos
            grupo_Tipo.loc["Bebida"]

  ```
    * Para agrupar según una columna tipo de "kind" `datetime64[ns]` asignando una operación a las columnas afectadas
      ```python
            grupo = df_new.groupby(pd.PeriodIndex(df_new["Fe, de picking"], freq='M')).sum().sort_index(ascending=True).reset_index()
            #Le ponemos reset_index para que no me tome las fechas como index, pero las ordenamos antes de resetear.
            
      ```
    * Para agrupar valores que contengan todas estas combinacion de columnas
      ```python
          df = df.groupby(["Date","Rank","Title","Rating"])["3 stars","2 stars","1 stars"].mean()
          #Tomara todas las columnas como valor unico y agrupara en caso de haber duplicados promdeiando los valores en
          #las columnas ["3 stars","2 stars","1 stars"].
      ```
    * **``df.get_group("cat")``** : Tras tener un df agrupado por **`gorupby`** podemos filtrar con `df.get_group("Elemento_a_Filtrar")`.
  
    *Podemos poner mas de una funcion en agg():
      ```python
        df.groupby("Nombre").agg([np.mean, np.std])["Valor_1","Valor_2"] 
      ```
    * Paramtro dfgroupby().filter(): Aplicamos una funcion por la que queremos que cada valor de la se filtre. Acortando la tabla por el filtro.
      ```python
        dfgroupby().filter(<funcion>).["Valor_1","Valor_2"] #Recortamos columnas procesadas a mostrar.
      ```
    * Paramtro dfgroupby().transform(): Aplicamos una funcion por la que queremos que cada valor de la agrupacion se procesara. Arrojando la tabla entera con los valores procesados.

      ```python
        #Mostrar como porcentaje de la suma total de la columna por la cual agrupemos.
        def porcentaje(x):
          return 100*x/x.sum()

        dfgroupby("Catergoria").transform(porcentaje)["Valor_1","Valor_2"] #Recortamos procesadas columnas a mostrar.
      ```

* **`df.pivot()`** : Devuelve el DataFrame remodelado (reshape) organizado por valores, sin agrupar, de índice/columna dados.<br>
Solo tiene 3 parámetros: `,index=` `,columns=` `,values=`.

  ```python
    data3 = df.pivot(columns="OverallQual", values="SalePrice")
    
  ```
* **`df.crosstab()`** : Cruza 2 campos de un df poniendo una en index y otra como columna mostrando sus valores unicos de ambos campos y poniendo el conteo donde se cruzan los datos unicos ambos campos.<br>
Para vizualizacion de conteos cruzando 2 campos es mejor `.crosstab()` pero podemos modificar sus valores y su aggfunc. Es muy usado como tabla resumen de entre 2 columnas o campos pudiendo ver su % con respecto al total usando `,normlazing=` 
Parámetros: `,index=` `,columns=`. De usar ``,values=`` tenemos que indicarle su `,aggfunc=`.<br>

  ```python
    pd.crosstab(index = train_data['Survived'],columns = train_data['Pclass'],
                 margins= True, margins_name= "Total")
          #,Normlazing= : True, "index", "columns". En modo True indicara los valores en porcentaje del total general.
          #En modo index indicara los valores en porcentaje del total de cada indice, etc.

    #Se puede combinar con mas metodos ejemplo .apply()
    pd.crosstab(index=train_data['Survived'], values=train_data['Pclass'], aggfunc="count",
            columns=train_data['Pclass'], margins=True, margins_name="Total", normalize=True).apply(
            lambda r: r*100, axis=0).round(2) #Redondea los datos a un 100% 2 decimales
  ```
* **`df.pivot_table()`** : También puede usar (`pd.pivot_table()`). Crea una Nueva tabla dinámica a partir de un dataframe agrupando sus valores por la operación que indiquemos por `,aggfunc=`.<br>
El "`,aggfunc=`" que traen por defecto es promedio. Puede hacer una tabla pivote de una `groupby()` <br>
***(DataFrame,Valores,Filas target,(jerarquía)columnas mostrada en valor precio,aggfunc= función a aplicar,margins = subtotales)***

  * `,fill_value = 0` : Reemplaza todos los valores nulos de df encontrado por el numero o string que le indiquemos.
  * `,aggfunc =` : Indicamos el método que operara con los valores. *aggfunc=np.sum o aggfunc="sum"; aggfunc="mean"; aggfunc= pd.Series.nunique #Devuelve un valor distinct count; aggfunc=np.count_nonzero #Cuenta valores no nulos*

  ```python
    vent_tipo_mesero = df.pivot_table(values="Precio",index=["Tipo_x","Categoria"],
                        columns="Mesero",aggfunc=np.sum,margins=True, fill_value=0).round(0)
    
    # Tabla agrupada por columnas en Meses del conteo distinto (no repetidos) de los ID de compras.
    df.pivot_table(index=["Creado Por",df["Tipo de Orden"]], columns=[pd.PeriodIndex(df["Creado El"], freq="m")],
                values=["Pedido"], aggfunc=pd.Series.nunique) #Devuelve un valor distinct count
  ```
* **`df.melt()`** : Despivotea una tabla pivot, retornandola a la forma de tabla de datos convencional (quitando agrupaciones). Al momento que despivotea va a retornados una columna para los valores de columna y otra para los valores valores. <br>
En una tabla pivote tenemos los siguientes elementos: filas (id_vars), columnas (values), values (value_name).

  * `,id_vars = ` : (filas) Nombre de la columna por la que fue agrupada la tabla, esta es la que va en index o filas.
  * `,values = ` : (Columnas) Nombres de las columnas de la tabla pivote, para no digitar en una lista nombre por nombre de todas las columnas es recomendable sacar sus nombres con una compresión de listas y guardarla en un variable para luego asignarla a este campo de values.
  * `,value_name = ` : (Encabezado Valores) Dado que separara los valores en una columna aparte es necesario asignarle un nombre como "Ventas".
  * `,var_name = ` : (Encabezado Columnas) Asigna un nombre o encabezado al campo de las columnas de la ex tabla pivote. De no asignarle un nombre pondrá un nombre por defecto.
  ```python
    #Despivoteamos una tabla agrupada por productos con columnas por año y ventas en valores.
    data3 = df.pivot(id_vars=["Producto"], values=["2010","2011","2021"], value_name="Ventas", var_name="Año")

    #Primero para listar todos los nombres de la columna values y no estar escribiendo año por año el nombre.
    años = [col for col in df if col!="Producto"] #devuelve todos los nombres de la columna menos la primera "Producto"
    
    #Tras hacer esto podemos igualar ,values= años
    
  ```
* **`df.merge()`** : Es la union de tablas como se efectúan en ``SQL``.<br> 
Se puede precisar la columna de union con `,on=["Columna"]`.<br>
s`,how=` indica el tipo de "kind" de union, por defecto esta en ``INNER JOIN`` (intersección) donde en ambas tablas existan datos.<br>
No posee parametro Axis dado que nos traera las columnas del otro df a unir.

  * **`,on=`** : (col_ID) Por defecto unira las columnas con igual nombre. Indicamos la Columna por la cual uniremos las tablas. 
  * **`,left_index=True, right_index=True`** : Por defecto junta en una columna los valores de la columna por union (col_ID), con este parametro nos separara la columna con respecto col_ID_tbA y col_ID_tbB, manteniendo la suma de filas de por ambas tablas y dejando nulos en donde cada tabla no tenga datos.
```python
  * **`suffixes=('_high','_low')`** : Añade sufijo a las columnas generadas por la separacion de left y right index: "Ranges_high	Ranges_low"

          #LEFT JOIN (columna izquierda completa)
          df = df_A.merge(df_B,on=["Producto"],how="left")

          #OUTER JOIN Preserva todos los valores Tabla A y B.
          df = df_A.merge(df_B,on=["Producto"],how="outer")

          #Unir Filas Por Index: Esto nos dara una columna para los index A y otra para B; combinando sus index. Rellenara con nulls
          df = df_A.merge(df_B, left_index=True, right_index=True)

          #Unira los valores de ColA con cada valor similar encontrado en ColB. Revisar en .merge()
          df1.merge(df2, left_on='lkey', right_on='rkey')

```
* **`df.sample()`** : Coje una parte de la data del DataFrame de manera aleatoria o de la semilla que nosotros indiquemos.
```python
          muestra_df = california_df.sample(frac=0.1, random_state=17)

          # Creamos una nueva columna en df_2 con los datos de df donde solo tomaremos 100 valores de manera aleatoria 
          # del total.
          df_2["network"] = df["network"].sample(n=100).tolist()
```
* **`df.resample()`** : Afecta a datos de tipo fecha, para ellos es necesario tener la fecha en el index. Este metodo es parecido a la funcion ``pd.PeriodIndex()``, el cual agrupa el indice de fechas sea a "d", "m", ""
```python
          df.resample("m").mean()
          #Podria conjugar con funcion min(), max() (minimos o maximo mensuales, etc)
```

* **`df.corr()`** : Devuelve una tabla matriz de correlaciones de todos los campos de la tabla de Columna vs columnas, excluyendo NA/valores nulos y textos.
  ```python
          Correlacion = restdf.corr(method='pearson', min_periods=1)

          rest.corr()["AMZ"].sort_values(ascending=False)
          #Muestra la correlación de una sola columna con los demás campos de manera descendente
  ```
  * method= pearson : standard correlation coefficient; kendall : Kendall Tau correlation coefficient; spearman : Spearman rank correlation
  * min_periods=1 : (Opcional, solo para "pearson" y "spearman") Número mínimo de observaciones requeridas por vs.

* **`df.corrwith()`** : Correlacion de una columna con respecto a las demás. La correlación por pares se calcula entre filas o columnas con otro df de la misma dimension. `,method=` "pearson", "kendall", "spearman". Ejm: Correlación de 1 campo con respecto al resto de un mismo df:
  ```python
          df.corrwith(df["SalePrice"], method="pearson", axis=0).nlargest(10).sort_values(ascending=False)
          #Va a correlacionar una columna del df con todas las demás. Entregando los 10 campos más correlacionados.
  ```
  * method= pearson : standard correlation coefficient; kendall : Kendall Tau correlation coefficient; spearman : Spearman rank correlation
  * min_periods=1 : (Opcional, solo para "pearson" y "spearman") Número mínimo de observaciones requeridas por vs.

* **`df.plot()`** : Muestra una gráfica lineal. Se puede llamar el tipo de "kind" de gráfica con ``df.plot.GRÁFICA()`` o ``df.plot(kind=GRÁFICA)`` Tiene la siguiente configuración:
    ```python
          axes = temps_df.plot(x="Fahrenheit", y="Celsius",style=".-",figsize=(10,8))
          y_label = axes.set_ylabel("Celsius") #Mostrar etiqueta "Y"
    ``` 
  * **`df.plot.scatter()`** :Gráfico de Dispersion: "Scatter Plot"
    ```python
          axes = restdf.plot.scatter(x="Producto",y="Propina")
  
    ```
  * **`df.plot.bar()`** : Muestra una gráfica de barras. Tiene la siguiente configuración:
    ```python
          axes=datos.plot.bar(x='palabra',y='frecuencia',legend=-False)  #Invierte la legenda del eje "x" La pone en Vertical.         
    ```

---
### Propiedades:
El indexado es igual que numpy trabaja con corchetes **[ : : ]** para indicar los rangos de datos que queremos que nos arrojen. No trabaja con paréntesis. *Ejm: pd.DataFrame.loc*

* **`.index[]`** : A diferencia de `loc` y `iloc` Devolverá el nº de index del valor indicado. Podemos tambien manipular el index ya que esta siendo referenciado, ejemplo cambiar el nombre ``.names``, sirve mucho para multi-index.
  ```python
    #Borra el valor "T" de la columna.
    train_features.drop(train_features.index[train_features["Cabin"]=="T"],axis=0) #Elmina toda las filas.
      
    #Se puede usar en combinación con loc y iloc.
    train_features.drop(train_features.loc[train_features["Cabin"]=="T",:].index,axis=0)

    #Renombrar el index (Multi-index)
    df.index.names =["Region", "Comuna", "Organismo"]

  ```
* **`.columns[]`** : Muestra el nombre de todas las columnas del ``df`` . Podemos indicar el numero de columna del df que queremos que nos muestre.
Podemos también cambiar el nombre de las columnas en el orden que estas están con esta propiedad. EJM: 
  ```python
          kchouse.columns=["AreaFt","P.V"] #Cambia el nombre en el orden que están las columnas de df.

          #Funciona igual con la propiedad .names
          kchouse.columns.names =["AreaFt","P.V"] #Cambia el nombre en el orden que están las columnas de df.

          df.columns[1] #Mostrara la segunda columna del df.

  ```

* **`.loc[]`** : Se usa para visualizaciones. Imprime una columna por su 'nombre'. Para ello es necesario indicar primero el indice luego el nombre de la columna. Imprime también intersecciones con columnas. **['fila','columna']**. También puede cambiar datos del DF. Para seleccionar más de un index a la vez usamos doble **[[]]**

  ```python
            df.loc[:,"SalePrice"] #Siempre indicamos primero el indice y luego el nombre de la columna.
            df.loc[nºIndice,'columna'] or [1:2,"Producto"] #Rango de filas de una columna o sin ella.
            df.loc['fila']
            df.loc['fila','Fila dentro de fila'] #Encaso este en un groupby o alguna tabla dinámica.
            df.loc['fila':'fila']

            #para poder seleccionar más de un index a la vez usamos doble [[]]
            df_population_sample = df_population.loc[["1980","1990","2000","2010","2020"]]

            #También puede usarse el siguiente código para indicar columnas.
            df = df[df.index.isin(["2020","2021"])] #usa corchetes porque usa lista.

            #Podemos usar el indice de otra columna para filtrar otra:
            df.loc[df['TotalBsmtSF']>0,'HasBsmt'] = 1 #Escribe 1 en todos los indices donde "TotalBsmtSF" sea mayor a 0 en "HasBsmt".

            #Otra manera de hacer lo mismo sin "iloc". Reemplazamos los valores 0 de "HasBsmt" por "1" donde "TotalBsmtSF" sea mayor a 0.
            df[df['TotalBsmtSF']>0]['HasBsmt'] = 1
            df.loc[df['TotalBsmtSF']>0,'HasBsmt'] = 1

            #Un filtrado en 2 dimensiones fila,columnas[Columna a mostrar].
            df.loc[df["Tipo de Orden"].isin(["OC", "OS"]),["Tipo de Orden"]]["Tipo de Orden"]] #Añadimos esta columna a mostrar al final

            #Del ejercicio anterior otra manera de hacer el filtrado sin loc:
            df[df["Tipo de Orden"].isin(["OC", "OS"])]["Tipo de Orden"]]

            #Añade una columna "Total" en el df
            tabla_total["Total"] = tabla_total.sum(axis=1)

            # Otra forma con .loc Añade una fila "Total" en el df
            tabla_total.loc["Total"] = tabla_total.sum(axis=0)

            # Filtrado con cadena de condiciones "y" (&).
            df.loc[df["Series"].str.endswith("Slam")&(df["Surface"]=="Clay")&(df["Winner"]=="Federer")]
            
  ```
    También se puede usar para filtrar:
  ```python
            restdf.loc[restdf["Propina"]>0.05]
  ``` 
  Para filtrar Multi-Index, usamos Tupla para referenciar el nombre del index: df.loc[("Primer nivel", "Segundo nivel"):, ("Index_col1", "Index_col2")]
  ```python
  df.loc[("Primer nivel"):,:] # Este caso hay 3 niveles de index.

  df.loc[("Primer nivel", "Segundo nivel"):, ("Col1")] #Los niveles escritos van dentro de la tupla. 

  # Para Filtrar en rango de años es necesario cambiar a intero los años de las fechas:

  df.columns.set_levels(levels= df.columns.levels[0].astype(int), #referencia a convertir a int la primera fila de columas
                        level= 0, inplace=True) #Aplica lo mismo en caso los años esten como filas usariamos ".index".
  df.loc[("Primer nivel", "Segundo nivel"):, (2010):(2013)] #Los rangos deben estar en ruplas independientes.
  ```

* **`.iloc[]`** : Se usa para visualizaciones por indice. Imprime una fila por su indice con todas las columnas correspondientes.

```python
          df.iloc['nºfila']
          df.iloc['nº.fila':'nº.fila2'] #hasta fila 2
```


* **`.at[]`** : imprimirá solo el valor específico en la intersección de fila y columna señaladas por su 'nombre'. ['fila','columna'].También podemos asignarle **"="** un nuevo valor a la intersección.

* **`.str[]`** : Referencia a la librería ``String``, convierte todos los resultados de sus métodos a formato String. Nos permite utilizar todos los metedos que usa una lista en python. Habilita métodos regex. *(Expresiones Regulares)*
  ```python
    #Mostrar la primera letra de la columna Cabin.  
    train_features["Cabin"] =  train_features["Cabin"].astype(str).str[0] #Pasamos a formato string antes.

  ```
  * **`.str.upper()`** : Convertira todos los textos en mayusculas.
  ```python
      df["Catergory"] = df["Catergory"].str.upper()
  ```

  * **`.str.strip()`** : Elimina Todos los Espacios en blanco tanto a la izquierda como a la derecha de la lista, no elimina espacios intermedios entre palabras. Elimina tambien todos los caracteres que le indiquemos:
  ```python
      txt = ",,,,,rrttgg.....banana....rrr"
      x = txt.strip(",.grt")
      out: "banana"     

  ```
  * **`.str.split()`** : Separa una cadena de texto segun el caracter que le indiquemos, por defecto entrega una lista con los 2 valores separados. Para que nos entrege 2 listas separadas mediante el caracter que hemos indicado usamos el parametro: ``,expand=True``
    ```python
      df[["ColumA","ColumB"]] = df.str.split("-", expand=True) #Creamos 2 culmnas en el df donde se alojara las 2 listas con los textos separados.

      #,expand=True : Nos entrega en 2 listas los textos separados.
    ```

  * **`.str.contains()`** : Este método Habilita el poder hacer un búsqueda por Expresiones Regulares en los valores del df o columna. Se puede lograr activando el parametro `regex=True` o invocandolo antes como `.str.contains(r'')` en donde escribimos la Expresion Regular dentro del **r''**.<br>
  Buscara en cada string el patron de búsqueda indicado (r''). No se ve afectado si hay mas datos después del patron que queremos hallar. Vendria a ser un **``re.findall()``**
    ```python
      df[df["Pedido"].str.contains("^451", regex=True)]   

      #Para hacer uso de flags=re.IGNORECASE necesitamos importar la libreria ``re``
      import re 
      df[df["Pedido"].str.contains("OC|OS", regex=True, flags=re.IGNORECASE, case=False, na=False)]  

      #regex=True : Activa lectura en modo regex.
      #flags=re.IGNORECASE : Ignora mayusculas y minusculas
      #case=False : Buscara la coincidencia de lo indicado en modo "True" (default). En modo "False" lo omitara arrojando un resultado obiando lo indicado. Seria su paralelo al simbolo "~".
      #na=False : Rellena los valores nulos con False haciendo que no arroje error si se topa con un NaN.
      # "nombre\.|nombre2\." : Despues del "\" indica que lo que sigue son simbolos de REGEX.
    ```
  * **`.str.match(r'')`** : Busca que el elemento contenga unicamente lo especificado en la búsqueda. Si hay un dato mas a lado botara como falso. Vendria a ser un **``re.fullmatch()``**
    
  * **`.str.replace("$","")`** : Reemplazara el valor existente por uno indicado después de la coma, en este caso por nada (indica borrar símbolo "$".). Vendria a ser un **``re.sub()``**
  * **`.str.extract()`** : Este método Habilita el poder hacer un búsqueda por Expresiones Regulares y Extraer texto segun lo indicado sin remover de la fuente original los valores.
  ```python
          #Extraemos nombres y apellidos en 3 columnas nuevas manteniendo la fuente ["fullname"] con sus datos.
          df["First Name"] = df["Full Name"].str.extract("(\w+)")
          df["Middle Name"] = df["Full Name"].str.extract("(?:[\w]+\s)(\w[.]?)(?:\s\w+)")
          df["Last Name"] = df["Full Name"].str.extract("([\w-]+$)")

  ```

* **`.dtype[]`** : Mostrará el tipo de "kind" de dato de cada columna. ***(int, float, object,datetime64[ns])***.
  
* **`.values[]`** : Arrojara los valores de la columna seleccionada en forma de array ([1,2,3]).

  ```python
              kc_house["Areaft"].values
  ```
* **`.dt[]`** : Referencia a la librería ``datetime``. Se aplicará a todos las columnas de tipo de "kind" ``datetime64`` y ``timedelta64`` (Diferencia entre 2 datetime64). Este tipo de "kind" de dato son el resultado de operar columnas de tipo de "kind" fecha como restar 2 columnas de tipo de "kind" `datetime64`.

  ```python
    df['Order Date'] = df['Order Date'].dt.strftime("%d/%m/%y")
    #Arrojara valores de formato string.

    df['Order Date'] = df['Order Date'].dt.strftime("%b")
    #Arrojara los meses en abreviados.

    df['Order Date'] = df['Order Date'].dt.month_name(locale='es_ES.utf8') # en_US (ingles)
    #Arrojara nombre de los meses en Español

    df['Order Date'] = df['Order Date'].dt.month_name(locale='es_ES.utf8').str.slice(stop=3)
    #Arrojara nombre de los meses en Español abreviados.

    df["Lead Time Entrega"] = (df["Creado El"] - df["Fecha entrega SAP"]).dt.days
    # Devolverá la diferencia en valor de formato entero (int32), sin el dt devolvera un timedelta64.
  ```
---
### Parámetro:

Van ligados a un método por una **"`,`"**

* **`,index=`** : Añade o reemplaza indices en forma de key a los datos de una serie, son encabezados para una fila. Este parámetro hace lo mismo que un diccionario pero de una forma más rápida a la hora de digitar.
* **`,columns=`** : Asigna nombre a las columnas del DF.
* **`,axis=`** : Dirige el procesamiento a filas o columnas de la matriz. **0** para **filas** y **1** para **columnas**.
  ```python
          #Creamos una nueva columna que sea el promedio con todas las columnas:
          df["Valor_prom"] = df.mean(axis=1)    
            
  ```
* **`,ascending=`** : Ordena a ascendente, por defecto viene en True.
* **`,inplace=`** : Viene por defecto en `False`, en modo `True` toma un método que no reemplaza al archivo o valor raíz, como un filtro, para reemplazarlo por la modificación hecha o método aplicado.
* **`,key=`** : Podemos añadir una función `lambda` que procesará previamente nuestro método.
* **`,na=`** : En modo ``false`` ignorará los datos vacíos.
* **`,case=`** : En modo ``false`` ignorará mayúsculas y minúsculas.
* **`,regex=`** : En modo ``true`` entenderá los operadores de `re` como el "or" ("``|``").
* **`,legend=`** : En modo ``-false`` Pondrá la leyenda del eje "x" en Vertical en el `.plot.bar()`.
  
## [7. openpyxl](#indice)
Es una librería que nos permite abrir los archivos excel y operar como si estuviéramos dentro del software. Cabe resaltar que pandas corre archivos excel bajo esta librería en su interior.<br>
*pip install openpyxl*

Se importa de la siguiente manera:
```python
import openpyxl
```
Otras librerías dentro de openpyxl
```python
from openpyxl.chart import BarChart, Reference #Gráfico de cuadros
from openpyxl.styles import Font #Fuentes de excel
``` 
Para el manejo de las celdas de excel es necesario la librería `string` el cual nos traerá el abecedario `.ascii_uppercase`.
```python
import string

  abecedario = list(string.ascii_uppercase)
```

### clase, Función:

* **`load_workbook()`** : Carga un archivo de excel `.xlsx`
  
  ```python
          wb = load_workbook('sales_2021.xlsx')#abrimos resultado de pandas
          pestaña = wb['Report'] #asignamos a la variable la pestaña existente
  ```
### Librerías, Métodos:

* **`Reference()`** : Librería para referenciar los datos que se graficará. Primero se referencia los datos luego se gráfica. Previamente debemos indicar con las propiedades de `.active` donde están los datos con los que se referenciará la gráfica.
  ```python
          data = Reference(pestaña, min_col=min_col+1, max_col=max_col, min_row=min_fila, max_row=max_fila)
          #data hace referencia a Columnas a tomar.
          categorias = Reference(pestaña, min_col=min_col, max_col=min_col, min_row=min_fila+1, max_row=max_fila)
          #categorias hace referencia a Filas a tomar.

  ```
* **`BarChart()`** : Librería de Gráfico de barras de Excel. Es necesario asignar esta librería a una **variable** para usar sus siguientes **Métodos**:
  ```python
        barchart = BarChart()
  ```
  * **`.add_data()`** : Añade el `Reference` con respecto a columnas (`data`) al gráfico.
    ```python
          barchart.add_data(data, titles_from_data=True)

    ```
  * **`.set_categories()`** : Añade el `Reference` con respecto a Filas (`categorias`) al gráfico.
    ```python
          barchart.set_categories(categorias)
          
    ```
  * **`.title=`** : Asigna un titulo al gráfico. Ejm: "Nombre"
  * **`.style=`** : Asigna un estilo al gráfico. Ejm: 5
  <br><br>
* **`.add_chart()`** : Este método añadira el grafico previamente creado a la pestaña de nuestro workbook cargado (`pestaña = wb["Nombre_Pestaña"]`)
```python
          pestaña.add_chart(barchart, 'B12')
```

### Propiedad:

* **`.active`** : Entra a la pestaña activa de la variable donde previamente hemos cargado el archivo excel, tiene las siguientes propiedades: Cabe resaltar que el conteo de las columnas y filas activas las arroja en forma de numero empezando con el valor "1" y no "0" como lo es en python.
  ```python
          min_col = wb.active.min_column # Cuenta empezando por "1"
          max_col = wb.active.max_column
          min_fila = wb.active.min_row
          max_fila = wb.active.max_row
  ```
  * **`.min_column`** : Columna minima de nuestro cuadro o tabla. 
  * **`.max_column`** : Columna maxima de nuestro cuadro o tabla.
  * **`.min_row`** : Fila minima de nuestro cuadro o tabla.
  * **`.max_row`** : fila maxima de nuestro cuadro o tabla.
  <br><br>
* **`.style`** : De la librería ``openpyxl.styles`` da formato a los gráficos.
  ```python
            pestaña[f'{i}{max_fila+1}'].style = 'Currency'
            #Convierte una celda a tipo de "kind" moneda en excel.   
  ```
* **`.font`** : De la librería ``openpyxl.styles`` da formato a la celda.
  ```python
            pestaña['A1'].font = Font('Arial', bold=True, size=20)
  ```

## [8. matplotlib ("*plt*")](#indice)
Es una librería que da una representación gráfica de nuestros datos en cuadros estadísticos en 2D.<br>
*pip install matplotlib*

Se importa de la siguiente manera:
```python
    import matplotlib.pyplot as plt 
```
### clase, Función:

* **`plt.figure()`** : Crea un lienzo para insertar un gráfico dentro. Podemos indicar nºfilas (nrows=), nªcolumnas (ncols=), tamaño del lienzo (figsize=): La variable `axes` o `ax` es la que dibujará en el lienzo para ello indicamos el tipo de "kind" de gráfico que queremos e indicaremos con que datos graficará:

  ```python
      plt.figure(figsize=(16,9))
      
  ```
* **`plt.plot()`** : Es usado para graficar valores dentro del lienzo.

  ```python
      re_x = np.arange(100,14000,1) #rango eje "x"
      re_y = re_x*reglin.slope + reglin.intercept #regresion para cada valor "y"
      ax.plot(x,y,"r--")
      #Previamente definimos "x" , "y" con tipo de "kind" de linea y color

  ```

* **`plt.subplot()`** : Es usado para graficar listas ya que estas no poseen el atributo `plot`. Añade `axes` a la figura existente (lienzo) para insertar un gráfico dentro. Podemos indicar nºfilas (nrows=), nªcolumnas (ncols=), tamaño del lienzo (figsize=): La variable `axes` o `ax` es la que dibujará en el lienzo para ello indicamos el tipo de "kind" de gráfico que queremos e indicaremos con que datos graficará:
 
    ```python
    figure, axes = plt.subplot(nrows=4, ncols=6, figsize=(6,4)) #para multicuadros.
    ```
  * Para regresión lineal:

    ```python
      fig, ax = plt.subplots(figsize=(6,5))
      ax.scatter(x=kchouse.AreaFt, y=kchouse["P.V"]) #scatter de datos
      re_x = np.arange(100,14000,1) #rango eje "x" (necesario numpy)
      re_y = re_x*reglin.slope + reglin.intercept #regresión para cada valor "y"
      ax.plot(re_x,re_y,"r--") #variable ax es la que dibuja el gráfico.
      plt.show()
      
    ```
* **`plt.title()`** : Inserta titulo al gráfico.
* **`plt.xlabel()`** : Inserta titulo al eje x.
* **`plt.ylabel()`** : Inserta titulo al eje y.
* **`plt.legend()`** : Insertara la leyenda a nuestra gráfica.

    * **``,loc=``** : Indica la posición donde se ubicara la leyenda ejemplo ("upper left","upper right", etc)

## [9. seaborn ("*sns*")](#indice)

Es una librería para visualización de datos, esta esta desarrollada sobre **matplotlib**. Además está integrada con la librería de **pandas** por lo cual puede leer **DataFrame** y campos directamente como argumentos de las funciones de visualización:
___
### Clase, Función:

* **`sns.set()`** : Cambia el tamaño de las fuentes en las gráficas.
```python
        sns.set(font_scale=2)
```
* **`sns.set_style()`** : Cambia los ajustes a la configuración de la librería Seaborn. 
```python
        sns.set_style("whitegrid")
```
### Graficos:

* **`sns.barplot()`** : Diagrama de barras. 
```python  
        axes =sns.barplot(x=valores,y=frecuencias, palette="bright")
```
* **`sns.regplot()`**: Muestra un diagrama de regresión lineal de 2 variables.
```python
        axes = sns.regplot(x=kchouse.AreaFt, y=kchouse["P.V"], scatter=True,order=1,color="blue", label="order 1", line_kws={"label":"y={0:.1f}x+ {1:.1f}".format(reglin.slope,reglin.intercept)})
        #order= nº de polinomio de la regresión.
```
* **`sns.heatmap`** : Diagrama o mapa de calor.
```python
        axes = sns.heatmap(confusion_df, annot=True, cmap='nipy_spectral_r')
```
* **`sns.displot()`** : Histograma con curva normal. Muestra la distribucion en barras de la grafica seguida de una curva lineal de como deberian de distribuirse los datos.
  ```python
        sns.displot(df["Col"])
        #,hist=False : Quita las barras del histograma para solo vizualizar la linea.
  ```
   
* **`sns.pairplot()`** : (Gráfico_Matriz) "Parcela" Un diagrama de pares traza una relación por pares en un conjunto de datos. La función ``pairplot`` crea una cuadrícula de ejes de modo que cada variable en los datos se compartirá en el eje y en una sola fila y en el eje x en una sola columna. Eso crea parcelas
```python
        cuadricula = sns.pairplot(data= iris_df, vars=iris_df.columns[:4],hue="Especie")
#vars = (lista de nombres de variables)Son en total 4 columnas del df. (data + target)
#huw = Agrupa la gráfica por la columna que le indiquemos.
```
___
### Métodos:
___
### Propiedad:
___
### Parámetro:
* **`,hue=`** : "matiz" (opcional), Este parámetro toma el nombre de la columna para la codificación de colores.
* **`,palette=`** : Cambia los colores del gráfico. Ejm: "cool"

## [10 plotly ("*px*")](#indice)

### CUFFLINKS
[Fuente y guia de uso Git](https://github.com/santosjorge/cufflinks)

Es una librería que da una representación gráfica de nuestros datos de manera interactiva y dinámica <br>
*pip install plotly*

También necesitaremos un librería llamada `cufflinks` que va a hacer de intermediario entre `pandas` y `plotly` el cual va a hacer mas fácil de escribir y leer el código cuando grafiquemos nuestras visualizaciones.
*pip install cufflinks*

Se importa de la siguiente manera para hacer gráficas con pandas:
```python
    import pandas as pd
    import cufflinks as cf
    from IPython.display import display,HTML

    cf.set_config_file(sharing="public", theme="space", offline="True")
    
    #para ver todos los temas disponibles para las gráficas usamos: cf.getThemes()
    #space da color oscuro.

    setattr(plotly.offline, "__PLOTLY_OFFLINE_INITIALIZED", True)
    #Esto soluciona el error de attributeerror: module 'plotly.offline' has no attribute '__plotly_offline_initialized'

```

### Funciones:

* **`help(df.iplot)`** : Nos arrojara todos los metodos y parametros que podemos usar con la version actual.
* **`cf.go_offline()`** : Gráficos offline. Para que no redireccione a la pagina de ``plotly``.
* **`cf.datagen. ... ()`** : Genera DataFrames preparados para el tipo de "kind" de gráfica con valores aleatorios que querramos, dentro del ``( )`` indicamos cuantas DataFrames queremos generar. Para graficarlar usamos ``.iplot()``. Examinando como están armados los DF podemos guiarnos para las gráficas.
  ```python
    cf.datagen.lines(1)
    cf.datagen.histogram(4)
    cf.datagen.bubble() #Para esta grafica se toma minimo 4 variables. (x=,y=,size=,categoria=)
  ```

* **`cf.subplot()`** : Grafica un conjunto de graficas previamente enlistadas y concatenadas entre **``[ ]``** (Suma de iplots). Para imprimir la grafica usamos: cf.iplot()<br>
Una comprension de listas no es mas que una concatenacion de listas: [""]+[""]
  
  
    ```python
    #Ejemplo de Boxplot
      m = [df[df['X']=="A"][['Col1','Col2']].iplot(kind='box',asFigure=True)] +
          [df[df['X']=="B"][['Col1','Col2']].iplot(kind='box',asFigure=True)]
      cf.iplot(cf.subplots(m))
      #Con Asfigure comparten el Eje Y las graficas


      # Con figure (Multigraficas independientes)
      #Ponemos cada una de las gráficas -> df.figure(kind=,) dentro de un cf.subplots().
      figs = cf.subplots([df1.figure(kind='scatter'),df2.figure(kind='hist'),df3.figure(kind='box'), 
                          df4.figure(kind='barh')], subplot_titles=['Bar 1','Bar 2','Bar 3','Bar 4'],
                          vertical_spacing=.20,horizontal_spacing=.05)

      #Formato y Titulo
      figs['layout'].update(showlegend=True, title='Subplots Graphics Comparasion', font=dict(color='#ffffff'))

      #Imprimimos la gráfica.
      cf.iplot(figs)
    ```


* **`cufflinks.to_df(Figure)`** : Extraerá de un gráfico ``plotly`` alojado en una variable, el dataframe que contiene el gráfico.

### Métodos:

* **`df.figure()`** : Sirve para hacer multi-gráficas de grafías independientes en una sola gráfica.

* **`df.iplot()`** : Armara la gráfica según los parámetros.
  * **`Gráfico de Lineas`** : Se suele utilizar cuando hay tiempos involucrados, con el fin de ver la evolución de los tiempo (Fechas).

  ```python
      df_population.iplot(kind="line", title="Year vs Population" ,xTitle="Country", yTitle="Population",mode='lines+markers'
      ,fill=True,,error_y=5, error_type='percent')    
    
      
      # ,mode='lines+markers' : Genera puntos a lo largo de la linea
  ```
  * **`Gráfico de Lineas Diferencial`** : Este es un Gráfico de Lineas normal con la adición que en el pie del gráfico se añade un gráfica que muestra la diferencia entre las gráficas bien sea en una diferencia numérica (`spread`) o proporcional (`ratio`). Esta mas orientado a la comparación entre 2 campos.
  
  ```python
      #Diferencia en Valor Numérico.
      df_population[["United States","Brazil"]].iplot(kind='spread',mode="lines+markers",size=10, title="Diferencia Numérica")      

      #Diferencia en Proporción.
      df_population[["United States","Brazil"]].iplot(kind='ratio',mode="lines+markers",size=10, title="Diferencia Numérica")      

  ```
  
  * **`Gráfico de Dispersión (ScatterPlot)`** : Se suele utilizar para hacer comparación entre 2 variables. 

  ```python
      df_population.iplot(kind="scatter", mode="markers", xTitle="Year", yTitle="Population", title="Year vs Population",
                                size=12,symbol=['x-open',"circle-open","square-open","diamond-open","triangle-up-open"]))

      # scatter o line da gráfica de lineas por ello usamos mode="markers para forzar los puntos de dispersion. 
      # size=12 : Tamaño de los puntos.

      #Para exploración de datos "Scatter" se dimensiona la gráfica en un cuadrado. 

      data2.iplot(kind='scatter', x='TotalBsmtSF', y='SalePrice', mode='markers', size=9, xTitle='TotalBsmtSF', yTitle='SalePrice',
                  title='TotalBsmtSF vs SalePrice', bestfit=True, bestfit_colors=["red"], dimensions=[700,700], legend=False)

  ```
  * **`Gráfico de Barras (BarPlot)`** : Se puede usar para casi todo tipo de "kind" de contexto.<br>
  
    * **Barras verticales** se suele usar cuando las etiquetas caben.
    * **Barras Horizontales** se usan cuando el titulo no cabe (es grande). También cuando queremos presentar elementos de un conjunto. Ejm: ventas por Categoria

  ```python
      #Gráfica de Barras Verticales
      df_population_2020.iplot(kind="bar", title="Year vs Population" ,xTitle="Countries", yTitle="Population", colors="red")
      
      # ,kind="barh" = Barras Horizontales.
      # ,bargap = .2 : Afecta la anchura de las barras.
      # ,barmode="stack" : Crea barras segmentadas por categorías y las va sumando unas con otras poniendo a las mas pequeñas en el tope, es un acumulado. "overlay" : Similar a "stacks" pero no las suma sino da su valor tal cual (comparativo con otros) las presenta ordenada según estén las categorías en la leyenda.
  ```
  * **`Gráfico de Cajas (BoxPlot)`** : Es usado para encontrar los valores estadísticos como la media, mediana los qurtines (Q1,Q3) Q1 se refieren menor igual al 75% de datos y Q3 al 25%.

  ```python
      df_population.iplot(kind="box",boxpoints="all", title="Year vs Population",dimensions=[1200,500])
      
      #boxpoints="all" : Muestra la concentración de datos en forma scatter a lado del boxplot.

      #Sacando 2 columnas del df, para ordenar por columna - valor y saque un boxplot por calidad. 
      data3.pivot(columns="OverallQual",values="SalePrice").iplot(kind="box",
             title="OverallQual vs SalePrice", dimensions=[1100,600]) #no muestra etiqueta X ni Y.

  ```
  * **`Gráfico Histograma (Histogram)`** : Sirve para ver la Distribución de data Numérica. Para ejecutar este gráfico es necesario tener los campos (incluido el index) ordenado por columnas y solo tener valores en las filas.

  ```python
      df_population[["United States","Indonesia"]].iplot(kind="hist",title="Year vs Population",xTitle="Countries", yTitle="Population",orientation='v',bins=0, subplots=False,sortbars=False, barmode="overlay")

      # ,barmode="stack" : Da el Acumulado; "overlay" (por defecto) da su valor tal cual para comparación. "group" Agrupa por pares de barras.
      # ,bins=50  : Aumentara el número de barras (sample) para el gráfico.
      # ,orientation='h' : Hará un histograma con barras Horizontales.
      # ,histnorm= "percent", "probability", "density", "probability density". Establece el tipo de "kind" de normalización para una traza de histograma. Por defecto la altura de cada barra muestra la frecuencia de ocurrencia, es decir, el número de veces que se encontró este valor en el recipiente correspondiente
      #,histfunc= "count", "sum", "avg", "min", "max". Establece la función de agrupación utilizada para un seguimiento de histograma.
                                  
  ```
  
  * **`Gráfico de Pie (PieChart)`** : Se puede usar esta gráfica hasta con 4 variables. No es muy usada, es mas visual. Es necesario usar un index numérico de no tenerlo reseteamos el index con `reset_index(inplace=True)`

  ```python
      df_population.reset_index(inplace=True)

      df_population_2020.iplot(kind="pie",labels="country", values="2020", title="Population 2020")
  ```

  * **`Gráfico Mapa de Calor (Heatmap)`** : Mapa de calor.
  
  ```python
      df.corr().iplot(kind="heatmap",title="Correlation Matrix",colorscale="reds",dimensions=[1200,1000])
      #columns vs columns; Toma una escala de colores automatica con respecto a la data hallada.
      #,center_scale= 0  : Indicamos el valor central de la escala de colores con su valor numerico.
      #,zmin= -1 ,zmax= 1 : Indicamos el valor minimo y maximo de la escala de colores con su valor numerico.
  
  ```

* **`Multi-Gráficas`** : Subplot's varias gráficas.

  ```python
    # Creación de 4 DataFrames.
    df1=cf.datagen.heatmap()
    df2=cf.datagen.heatmap()
    df3=cf.datagen.heatmap()
    df4=cf.datagen.heatmap()

    #Ponemos cada una de las gráficas -> df.figure(kind=,) dentro de un cf.subplots().
    figs = cf.subplots([df1.figure(kind='bar',),df2.figure(kind='bar'),df3.figure(kind='bar'),df4.figure(kind='bar')], 
                  subplot_titles=['Bar 1','Bar 2','Bar 3','Bar 4'],
                  vertical_spacing=.20,horizontal_spacing=.05)

    #Formato y Titulo
    figs['layout'].update(showlegend=True, title='Subplots Graphics Comparasion', font=dict(color='#ffffff'))

    #Imprimimos la gráfica.
    cf.iplot(figs)

  ```

### Parámetros:

* **`,kind=`** : "line", "scatter", "bar", "box", "hist", "pie".
* **`,legend=False`** : Quita las Leyendas.
* **`,categories="Columna"`** :(groupby) Agrupa demanera distinct_count las columnas categoricas y las coloca en la leyenda del grafico.
* **`,colors=`** : para más de un dato en el gráfico usamos.
* 
    ```python
      ,color=["blue","red","yellow"]
    ```
* **`,colorscale=`** : Escalas de color: "accent", "-accent", "blues", "-blues", "reds", "-reds", "purples","-purples", "paired", "-paired", "spectral", "-spectral", "brbg", "-brbg"

* **`,rangeslider=True`** : Añade es un desplazador de rango al pie del gráfico para desplazarse en la visualización de datos.
* **`,annotations`** : Añade una marca con respecto al index(date) , con su respecto formato de texto. Ejemplo Series de tiempo en el index.
  
    ```python
      ,annotations={'2015-02-02':'Market Crash', '2015-03-01':'Recovery'}, textangle=-70,fontsize=13,fontcolor='grey'
    ```
  
* **`,error_y=5, error_type='percent'`** : Añade un rango de error de la data graficada de +- 5%. Error_type puede ser también `error_type='continuous_percent'`.

* **`,sortbars=True`** : Ordena las gráficas de Barras manera descendente.
  
* **`,subplots=True`** : Crea una gráfica por cada Categoria del DataFrame, desglosa la grafica en varias mini-graficas. En el caso de tener varios gráficos de un tipo de "kind" alojadas en una variable podemos hacer muchas minigráficas.<br>
El parametro ``shape=(fila,Columna)`` indicara la forma en la que se ordenaran las graficas separadas, este solo funciona cuando `,subplots=True`.
  ```python
    cf.datagen.histogram(4).iplot(kind='histogram',subplots=True,bins=30)

    cf.datagen.lines(4).iplot(subplots=True,shape=(4,1),shared_xaxes=True,vertical_spacing=.02,fill=True)
    #son 2 mosaicos de gráficas independientes
  ```
* **`subplot_titles : bool or list`** : Podran en la cabezera de cada grafico su titulo o podemos cambiarselo usando una lista para indicar los titulos de cada grafico.
  
* **`,asFigure=True`** : Compilas las gráficas creadas por `subplot()` en una gran gráfica sin mezclarlas.
  
* **`,shape=(5,1)`** : Según el nº de gráficas o categorías que tengamos ejecutara una gráfica aparte. Hace lo mismo que ``,asFigure=True``

* **`,shared_xaxes=True, shared_yaxes=True`** : Fuerza el mostrar etiquetas en eje X & Y. Amplia la vision sea que activemos en X o Y (mejora las multigraficas subplot=True).
  
* **`,hline or vline`** : Inserta una linea bien en horizontal `hline` o vertical `vline` en la grafica de cualquier tipo de "kind". Sirve para intervalos de confianza en estadistica cuya aplicacion podria darse en controles de calidad por ejemplo.
  ```python
      df.iplot(kind="line",hline=[2,4],vline=['2015-02-10'])
  ```
  
* **`,hspan or vspan`** : Inserta un rango sombreado bien en horizontal `hspan` o vertical `vspan` en la grafica de cualquier tipo de "kind". Para dar formato a cada linea insertada es necesario Enlistar cada diccionario el cual representa una cada lineacon el fin de darle formato a la linea dibujada con los parametros: `color`, `width`, `dash`

    ```python
      #Data
      df=cf.datagen.lines(3,columns=['a','b','c'])

      cf.datagen.lines(3).iplot(kind="line", hspan=dict(y0=-1,y1=2,color='orange',fill=True,opacity=.4))

      #Tipo intervalos de confianza con media en el centro
      df.iplot(hline=[dict(y=-1,color='blue',width=4),dict(y=1,color='pink',dash='dash'),
                      dict(y=2,color='blue', width=4)])
        # width=4 : Ancho de la linea a graficar
        # dash="dash" : Indica tipo de linea de trazos "- - - -"
    ```
* **`,fill=True`** : Activa sombreado, ejemplo en el gráfico de linea se vera sombreado todo el area bajo la linea.
  
* **`,bestfit=True`** : Genera una regresión lineal de los datos implicados en la gráfica.
  
  ```python
      ,bestfit=True,bestfit_colors=['pink','blue']
      #podemos asignarle un color a la regresión. El color también es tomado por el gráfico.
  ```
* **`,dimensions=[Ancho,Alto]`** : Modificara las dimensiones de la gráfica, para temas de exploración de datos es recomendable tener una medida de figura cuadrado y quitar las leyendas. Ejm: ,dimensions=[700,700]
  
  ```python
      data2.iplot(kind='scatter', x='TotalBsmtSF', y='SalePrice', mode='markers', size=9, xTitle='TotalBsmtSF', yTitle='SalePrice',
                  title='TotalBsmtSF vs SalePrice', bestfit=True, bestfit_colors=["red"], dimensions=[700,700], legend=False)
  ```
* **`yrange=[min,max], xrange = [min,max]`** : Modificara la escala de la grafica tanto en Y como en X indicandole el valor minimo y maximo de la escala.

* **`,world_readable=True`** : Hace que el gráfico pueda compartirse de forma publica ¿?.

### plotly.express (px)

Es una librería parecida a `cufflinks` es de fácil uso y gráficos de alto nivel de visualización. En complemento con cufflinks para gráficos más avanzados.

[Fuente y guía Gráficos](https://plotly.com/python-api-reference/)

Se importa de la siguiente manera para hacer gráficas con pandas:
```python
    import plotly.express as px

    #Para mantener el tema oscuro:
    import plotly.io as pio
    pio.templates.default = "plotly_dark"
```

### Gráficas

* **`px.histogram`** : Gráfica de histograma muy personalizable. Cufflinks resumen el personalizar todo este código.

```python
  df = px.data.tips()
  fig = px.histogram(df, x="day", y="total_bill", color="sex",
            title="Receipts by Payer Gender and Day of Week vs Target",
            width=800, height=600,
            labels={"sex": "Payer Gender",  "day": "Day of Week", "total_bill": "Receipts"},
            category_orders={"day": ["Thur", "Fri", "Sat", "Sun"], "sex": ["Male", "Female"]},
            color_discrete_map={"Male": "RebeccaPurple", "Female": "MediumPurple"},
            template="plotly_dark"
            )

  # Añadir el símbolo de $ a los números del Eje "Y"
  fig.update_yaxes(tickprefix="$", showgrid=True)

  # Fuente de texto, y legenda centrado
  fig.update_layout(font_family="Rockwell",legend=dict(title=None, orientation="h", y=1, yanchor="bottom", x=0.5, xanchor="center"))

  # Añade la linea horizontal con respecto al valor de y0 - y1
  fig.add_shape(type="line", line_color="salmon", line_width=3, opacity=1, line_dash="dot",
                x0=0, x1=1, xref="paper", y0=950, y1=950, yref="y")

  # Añade un texto en la posición en el elemento "x" y posición "y" indicada
  fig.add_annotation(text="below target!", x="Fri", y=400, arrowhead=1, showarrow=True)

  fig.show()

```

* **`px.scatter_matrix`** : Gráfica la correlación entre las propias columnas del DataFrame, no requiere preparación de la data.
```python
  columns = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']

  fig = px.scatter_matrix(df[columns],title="Scatter Matrix",height=1200,width=1200)

  fig.show()
```

### Parámetros:

* **`,height=1200, width=1200`** : Modificara las dimensiones de la gráfica como lo haría ``,dimensions=[Ancho,Alto]`` en cufflinks, para temas de exploración de datos es recomendable tener una medida de figura cuadrado y quitar las leyendas. Ejm: ,dimensions=[700,700].
  
* **`,template="plotly_dark"`** : Modificara el color del tema de fondo de los gráficos.

## [11. collections ("*clt*")](#indice)
Este módulo implementa tipos de datos para contenedores especializados que proporcionan alternativas a los contenedores integrados de uso general de Python, dict, list, set, and tuple.

* **`namedtuple()`**: función factory para crear subclases de tuplas con campos con nombre

* **`deque`**: contenedor similar a una lista con appends y pops rápidos en ambos extremos

* **`ChainMap`**: clase similar a dict para crear una vista única de múltiples mapeados

* **`Counter`**: subclase de dict para contar objetos hashables. Convierte una lista de datos en diccionario con sus repeticiones como valor.

* **`OrderedDict`**: subclase de dict que recuerda las entradas de la orden que se agregaron

* **`defaultdict`**: subclase de dict que llama a una función de factory para suministrar valores faltantes

* **`UserDict`**: envoltura alrededor de los objetos de diccionario para facilitar subclasificaciones dict

* **`UserList`**: envoltura alrededor de los objetos de lista para facilitar la subclasificación de un list

* **`UserString`**: envoltura alrededor de objetos de cadena para facilitar la subclasificación de string

## [12. selenium](#indice)
Esta Libreria permite poder hacer "**Web Scraping**" para jalar datos de paginas en internet. Para ello es necesario descargar el **ChromeDriver** (es un ejecutable ) para que python pueda conectarse por chrome. [ChomeDriver](https://chromedriver.chromium.org/downloads).<br>

Esta libreria tambien puede operar en paginas web desarrolladas con **JavaScript**.<br>
Considerar que esta libreria no es tan rapida a la hora de operar como si lo es la libreria **`Scrapy`**.

*pip install selenium*

```python
from selenium import webdriver
```
___
### Módulo:
Importamos el archivo "**chomedriver.exe**"

```python
    driver = webdriver.Chrome("C:/Users/Foster-PC/Downloads/Instaladores/Data Science/ChromeDriver (Web Scraping)/chromedriver.exe")
```
___
### Métodos:
Se divide en 2 a la hora de referenciar un elemento en la pagina: `.find_element` y `.find_elements`, traera 1 o mas resultados segun el que usemos.

* **``get.("URL")``** : Cargará la página en el **"chromedriver.exe"**.
* **``driver.quit()``** : Cerrará el **"chromedriver.exe"**.

* **``driver.find_element_by_xpath()``** : Buscara elementos por su "**XPath**":
```python
          ('//tag[@atributo="Valor"]')
```
* **``driver.find_element_by_class_name()``** : Buscara elementos por su tag "**Class**".
* **``driver.find_element_by_id()``** : Buscara elementos por su tag "**id**".
* **``driver.find_element_by_tag_name()``** : Buscara elementos por su tag. Ejm: "**tr**" (Table Rows).
* **``...click()``** : Ejecuta un click en el elemento previamente encontrado.
___
### Propiedades:
Las propiedades en python trabajan con **[ ]**.
* ``.page_source`` = Muestra todo el codigo `html` de la página en formato de texto sin respetar las indentaciones. Podemos asignar esto a una variable para posteriormente trabajarlo en BeautifulSoup.
___
Para Seleccionar Listas desplegables es necesario la clase **`Select`**:

```python
    from selenium.webdriver.support.ui import Select
```
```python
    box = driver.find_element_by_class_name("panel-body")
    #En este caso el atributo es un "id" usamos "element_by_id"
    dropdown = Select(box.find_element_by_id("country")) #Encontrar 1 solo Element.
    #selecciona segun el texto que aparece en la lista despegable.
    #tambien puede ser por indice con "select_by_index("0")" Contando desde el "0"
    dropdown.select_by_visible_text("Spain") # 1 solo Element.
```
___
Para no tener errores con respecto a los tiempos de carga de la pagina tras las acciones que realizamos usamos:

```python
    import time
    
    time.sleep(5)
    #Con esto nos aseguramos que despues del click duerma 5 segundos hasta
    #recibir el proximo comando.
```

## [13. BeautifulSoup ("*bs*")](#indice)
Esta Libreria permite interpretar los datos y leer el código fuente de los datos extraido de la pagina web.<br>
**``BeautifulSoup``** parsea el codigo extraido del html en formato de texto, por lo tanto siempre nos devolvera el html en texto, por ello si queremos interactuar con el codigo html se sugiere usar la libreria **`request`**.<br>
Considerar que esta libreria no opera con paginas web desarrolladas con **JavaScript**<br><br>
*pip install beautifulsoup4*

* A la hora de extraer elementos de los HTML es importante respetar la jerarquía de estos:

  1. ID 
  2. Class name
  3. Tag name, CSS Selector
  4. Xpath

```python
from bs4 import BeautifulSoup as bs
```
___
### Clase:
* **`bs()`** : Pondremos dentro del paréntesis la **variable** donde previamente tenemos asignado el código fuente de la página web.
Es necesario usarlo con `Request` para que extraiga el URL en formato de texto, junto con el uso de un **Parse** ("lxml").
```python
        soup = bs(requests.get("URL").text, 'lxml')
        
```
___
### Métodos:

* **`.find_all()`** : Buscará y Traerá más de un elemento de la web, todo lo que le indiquemos.Para traer todos los links de la página usamos ("a"). En html asi se llama a los links con "a".<br>
*("tipo de "kind" de elemento, atributo={"":""}<-"class=postingCardInfo")*
```python
        soup.find_all("div", attrs={"class":"postingCardInfo"})
```
* **`.find()`** : Buscará y Traerá de la web un elemento que le especifiquemos. (Una linea de código).
* **`.prettify()`** : Muestra el código HTML de manera mas legible respetando sus Indentaciones.
* **`.get_text()`** : Convertirá el resultado a formato de texto (string). Modifica a la variable contenedora. Hace lo mismo que la Propiedad "`.text`".

 ```python
        .get_text(strip=True, separator=' ')
        #Elimina los saltos de linea y separa el texto en espacios en blanco.
 ```
* **`.find_next_sibling(Tag)`** : Se usa cuando no hay una referencia (nodo) o ancla que anteceda a nuestro nodo. Buscará un nodo  teniendo como antecesor a un nodo hermano al mismo nivel dentro del código HTML. Para ello necesitamos un ancla a definir de la siguiente manera:

 ```python
        ancla = soup.find('div', id='product_description') 
        parrafo = ancla.find_next_sibling('p').get_text(strip=True)

 ```
___
### Propiedad:
* **`.text`**: Convertirá el resultado a formato de texto (string). Modifica a la variable contenedora.

___
### Tags:
* **`.title`**: Mostrar Titulo de la pagina web.


## [14. scrapy ("*scp*")](#indice)
Esta no es una libreria, es un **FrameWork** el cual es la herramienta más completa para **WebScraping**. Es más rapida debido que puede ejecutar solicitudes a paginas web en paralelo (spiders).

Esta libreria puede operar con paginas web desarrolladas con **JavaScript**.
Considerar que requiere un curva de aprendizaje. <br><br>
*pip install scrapy* <br>

```python
import scrapy
```
___
### HTML:
Es un lenguaje de marcado, define la estructura y el significado de una pagina web.
Para indagar elementos en la Página Web podemos usar **Ctrl + f**.
Ciertos elementos claves para indagar en el código HTML.<br><br>

Tiene la siguiente forma:<br> 
    ``<(tag) (atributo)=(valor atributo) > (contenido) <(tag)>``

```html
          <h1 class="title"> Titanic (1997) </h1>
```
* Tags:
  * **``<head>``** : Cabeza del html.
  * **``<body>``** : cuerpo del html.
  * **``<header>``** : Contiene una introduccion al contenido y suele ir en la parte superior del cuerpo.
  * **``<article>``** : Contiene articulos.
  * **``<button>``** : Refiere a un boton en la pagina para hacer "**click**"
  * **``<p>``** : Contiene parrafos de articulos, noticias, etc.
  * **``<h1>``** : Contiene al titulo del html.
  * **``<div>``** : Se refiere a divisor. Es un contenedor genérico.
  * **``<nav>``** : Se refiere a navegación. Contiene a la barra de paginación Ejm: pag 1,2,3,...99.
  * **``<li>``** : Contiene elementos de una lista.
  * **``<a>``** : Se refiere a ancla (anchor: enlace). Este contiene al atributo `href` que son enlaces.
  ```html
          <a href="http://example.com"> Texto </a>
  ```
  * **``<table>``** : Representa a una tabla que contiene filas y columnas con la data para ser extraida. Esta contiene 2 elementos:
    * **``<tr>``** : Table row. Son las filas de la tabla.
      * **``<td>``** : Table Data. Son los datos de la tabla.
 
  * **``<ul>``** : Representa a una lista desordenada.
  * **``<iframe>``** : Permite colocar una página dentro de otra página. Hace complicado hacer Web Scraping debido a que tenemos que cambiar de un `<iframe>` a otro, no es muy común encontrar este tag.
<br><br>
* Formato de Búsqueda:
Contiene la siguiente leyenda para filtrar la informacion:

  * **``/``** = Se refiere a un nodo Ejm: <"div">; <"ul">; <"label">; va de uno en uno cuando ruteamos ('/head/h1').
  * **``//``** = Indica saltar nodos hasta llegar a la coincidencia. 
  * **``[]``** = busca un elemento dentro del nodo indicado.
  * **``@``** = indica atributo como "Class" que esta igualado a algo.

En consola de html:
  ```html
          $x('//article[@class="Etiquetas"]//h3/a["ejemplo"]/text()').map(x=>x.wholeText)
  ```
Para obtener las direcciones href filtramos del siguiente modo:
Usamos el atributo al final /@href y en map buscamos por valor (value).
___
  ```html
          $x('//ul[@class="nav nav-list"]/li/ul/li/a/@href').map(x=>x.value)
  ```
* Estado de Respuestas:
Los códigos de estado de respuesta del HTTP indican si se ha completado satisfactoriamente una solicitud HTTP. Se agrupan en 5 clases.
  
  1. Respuestas Informativas: (100-199)
  2. Respuestas Satisfactoria: (200-299)
  3. Redirecciones: (300-399) 
  4. Error del Cliente: (400-499) puede el servidor negar la solicitud.
  5. Error del Servidor: (500-599)



### Terminal:
Escribimos `scrapy` en el terminal mostrara sus comandos. Para ejecutar cada uno de ellos es necesario escribir: **``scrapy commando [option] [args]``**

* Comandos:
  * **``bench:``** Hace un Bench para probar el rendimiento de scrapy.
  * **``fetch:``** Obtiene el marcado HTML de una página Web.
  * **``genspider:``** Crea un nuevo spider dentro de una plantilla predefinida.
  * **``runspider:``** 
  * **``settings:``** Muestra la configuracion por defecto de scrapy.
  * **``shell:``** Es un entorno de pruebas para testear nuestro código sin ejecutar a nuestro spider o crear un nuevo Spider.
  * **``startproject:``** Crea un nuevo proyecto en scrapy con sus carpetas necesarias. Ejm: ´scrapy startproject nombre_del_archivo´.   
  * **``version:``** Indica la version de scrapy.
  * **``view:``**
<br><br>

* Objetos:
  * **`response`** : Sirve para encontrar elementos (es como un Soup). Este solo puede utilizar los métodos: **``.xpath()``** y **``.css()``** para encontrar elementos den el HTML.
  * **`yield`** : Sirve para devolver valores como lo hace `return`. 
<br><br>

* Métodos:
  * **``.xpath()``** : Busca elementos dentro del HTML. Entiende el siguiente parametro.
  ```
          response.xpath('//tag[@atributo="Valor"]')
  ```
  * **``.css()``**   :
  * **``.get()``**   : Obtendra un elemento. Ya sea de la ruta del ``.xpath()`` o ``.css()``
  * **``.getall()``**   : Obtendra una lista con los elementos. Para ellos necesitaremos tener de antemano una **lista vacia** para llenarla con un ciclo `for`.
<br><br>

* Propiedades:
  * **``.body``** : Muestra el codigo HTML previamente extraido. 
<br><br>

* Parámetro:
  * **``/text()``** : Este parametro seleccion solo el texto de un nodo de codigo HTML. Para extraerlo usaremos `.get()` o `.getall()` según sea conveniente.
  ```
          response.xpath('//h1/text()').get()
  ```
___

### * startproject:
Para crear un nuevo proyecto escribiremos lo siguiente en el terminal:
```
          scrapy startproject nombre_del_archivo
```

Creara una carpeta que contendrá los siguientes archivos:

- **``scrapy.cfg``** : Este archivo correrá comandos de scrapy. Ejecutara lo que hallamos escrito dentro de los demas archivos "**py**" incluyendo el **spider**.
- **``items.py``** : Ayuda a estructurar mejor la data que extraemos. Puede ser reemplazada con la palabra clave "``yield``" para devolver elementos de la página según su estructura por defecto.
- **``middleware.py``** : Podemos añadir funcionalidades personalizadas para procesar las solicitudes y respuestas. Contiene un "spider-middleware" y un "downloader-middleware".
- **``pipelines.py``** : Almacena la data que extraemos en una base de datos (MongoDB, SQlite).
- **``settings.py``** : Podemos añadir configuraciones extras a nuestro proyecto.

Aparecera en consola 2 lineas una para situarse en el directorio que se creo la carpeta que nombramos, y la otra linea es para crear el spider

```python
    cd scrapy_tutorial #situa en la carpeta
    scrapy genspider example example.com #aqui pegamos nuestro URL de esta manera:
```
`scrapy genspider nombre_spider www.abc.com/halo` (Sin "http" y sin "/" al final).

Esto creará nuestro spider que nombramos dentro de una carpeta **spiders**.
Los **spiders** se dividen en 2 tipos:

  - scrapy-Spider : Se personaliza para extraer data de las webs.
  - CrawlSpider   : Este es para hacer **Clouding** sirve para seguir links.

___
### * shell:
Probamos una solicitud a la página web. Para salir del modo shell escribimos "`exit`".
```
          r = scrapy.Request(url='https://www.pegamoslink.com/')
```
Luego usamos el comando "`fetch`" para ver la respues de nuestra solicitud.
```
          fetch(r)
          response.body (verificamos que tenemos el codigo HTML)
```
___
### * Ejecutar "scrapy.cfg":

Para ejecutar nuestro codigo guardado en los archivos **``py``** escribimos en el terminal:

```
          scrapy crawl nombre_del_spider
```

Para Ejecutar y guardar en un archivo ``.csv`` o ``json`` escribimos el siguiente código:

```
          scrapy crawl nombre_del_spider -o nombre.csv.json
```
___
### Tags:



## [15. request](#indice)
Esta Librería permite hacer solicitudes a las paginas web. Es usado para la técnica de **Web Scraping**.<br><br>

```python
import request 
```

___
### Clase, Funcion:
* **`()`** :
___
### Métodos:
* **`.get("URL")`** : Extrae el código HTML (crudo). 
* **`.content.decode("utf-8")`** : Codifica el contenido (código HTML) en "utf-8". Codifica mejor los valores "ñ" , tildes y caracteres especiales para evitar futuros problemas.

___
### Propiedad:
* **`.status_code`** : Hace una solicitud de prueba a la página regresando el **estado de respuesta** del HTTP.

## [16. scipy (stats)](#indice)
Esta Libreria permite trabajar con regresion lineal. De esta usaremos el módulo **``stats``**<br><br>

```python
from scipy import stats
```

___
### Clase, Funcion:
* **`linregress()`**: Crea una regresion lineal de 2 variables.
  ```python
        reglin = stats.linregress(x=kchouse.AreaFt, y=kchouse["P.V"])
  ```
* **`st.ttest_1samp()`**: Prueba T de un solo objeto. Tiene un parametro llamado **`,alternative=`** que de no indicar nada significa que tomara las 2 colas, **`,alternative='greater'`** para cola derecha y **`,alternative='less'`** para cola izquierda.<br>
Nos arroja como resultado el Statistic = nivel de dignificación (alfa) y pvalue.<br> 
n_significancia > p-valor = Rechaza Hipotesis nula y se Acepta H. alternativa.<br>
**st.ttest_1samp(df.colm, hipotesis_nula,alternative="")**
  ```python
    mu_0 = 170 #media, para prueba de H.
    st.ttest_1samp(df.Estatura, mu_0, alternative="")
    #,alternative= 'greater' : Cola derecha
    #,alternative= 'less' : Cola izquierda

  ```

* **`st.ttest_ind()`**: Prueba t de dos colas para dos poblaciones independientes. Esta prueba contrasta la hipótesis de que las medias de dos poblaciones, supuestas independientes, son iguales. Dicho de otra forma, contrasta la hipótesis de que la diferencia de medias es 0.<br>
Aplica lo mismo para el parametro **`,alternative=`**<br>
**st.ttest_1samp(Poblacion_1, Poblacion_2,alternative="")**
  ```python
    #¿Serán distintas las estaturas de mujeres y hombres?. Ambas colas
    st.ttest_ind(df_F.Estatura, df_M.Estatura)

    #¿Mayor la de las mujeres que la de los hombres? ,alternative= 'greater'
    #¿Mayor la de los hombres que la de las mujeres? ,alternative= 'less'
  ```

___
### Métodos:
___
### Propiedad:
* **`.slope`**: Arroja el valor de la pendiente."m"
* **`.intercept`**: Arroja el valor de la interseccion. Cuando "x es 0" Valor "b"
* **`.rvalue`**: Arroja el valor de la correlacion de los datos. Este evalua la precision de un modelo.

## [17. TextBlob ("*tb*")](#indice)
Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural** como análisis morfológico, extracción de entidades, análisis de opinión, traducción automática, análisis de sentimientos, etc.

 Esta librería ha sido entrenada con textos en ingles debido a ello está más optimizada para ese idioma más que otros.<br><br>
*pip install textblob*

Descargaremos el cuerpo de textos con el que `textblob` funciona. Esto incluye el poder separar las palabra por adjetivos , sustantivos; tambien separar frases y oraciones; separa oraciones del parrafo y tambien trae definicion de sinonimos y antonimos. Para ello necesitamos escribir en la consola o terminal.<br><br>
*ipython -m textblob.download_corpora*

Para importar TextBlob:

```python
    from textblob import TextBlob
```

Para cambiar el algoritmo por defecto **(pattern)** de analisis de sentimientos a otro algoritmo **(NaiveBayesAnalyzer)**.

```python
    from textblob.sentiments import NaiveBayesAnalyzer
    #Ejm:
    blob3 = TextBlob(texto, analyzer = NaiveBayesAnalyzer())
```

Para la **singularidad y pluralidad de palabras**; **Correccion de palabras mal escritas** importamos la libreria "**`Word`**" de "**``TextBlob``**".
```python
    from textblob import Word
```
___
### Clase, Funcion:
* **`TextBlob()`**: Crea o convierte un string a tipo de "kind" TextBlob.

___
### Métodos:

* **`detect_language()`** : Indica el tipo de "kind" de idioma del lenguaje analizado.
* **`translate(to="")`** : Traducira el texto al idioma que le especifiquemos. "es", "en", etc

#### Métodos de la Sub-Libreria Word:

* **`pluralize()`** : Arrojara el plural de la "palabra" indicada.
* **`singularize()`** : Arrojara el singular de la palabra indicada.
* **`spellcheck()`** : Sugiere mediante una lita de palabras con sus probabilidades la posible corrección de la palabra mal escrita.
* **`correct()`** : Arroja la corrección de oraciones de palabras mal escritas. 
* **`word_counts()`** : Arroja la cantidad de veces que la palabra indicada es encontrada.
___
### Propiedad:

* **`.sentences`** : Arroja cada una de las oraciones hasta donde terminan con un punto inicia otra. Segmenta por oraciones.
* **`.words`** : Arroja cada una de las palabras únicas del texto. Segmenta por palabras únicas
* **`.word_counts`** : Arroja un diccionario con cada una de las palabras únicas como **Key** y la cantidad de veces que aparecen estas palabras como **Values**.
* **`.tags`** : Arroja una lista con cada palabra segmentada indicando su tipo de "kind" (Sustantivos, Pronombres, Verbos, Adjetivos, Conjunciones, Adverbios, Interjecciones, Preposiciones)
* **`.noun_phrases`** : Arroja las frases-sustantivos encontrados en el texto.

#### Propiedades de la Sub-Libreria Word:

* **`.definitions`** : Arroja el significado de la palabra en cuestion.
* **`.synsets`** : Arroja una lista de posibles sinonimos para la palabra.
___
### Parámetro:

* **`,analyzer=`** : Podemos cambiar el analizador de sentimientos a la funcion `TextBlob` . Ejm: NaiveBayesAnalyzer 
```python
    blob3 = TextBlob(texto, analyzer = NaiveBayesAnalyzer())
```

## [18. nltk](#indice)
**Natural Language Toolkit** Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural**. No requiere "pip install" python ya lo trae.<br>

Para importar nltk:

```python
    import nltk
```

Para usar "**Stop Words**" descargamos el siguiente paquete para "nltk".

```python
    nltk.download("stopwords")
```
Ahora importamos el "**stopwords**"
```python
    from nltk.corpus import stopwords
```
Para segmentar por palabras y  asignar las "**stopwords**" a una variable:
```python
    stops = stopwords.words("english")
    print(stops)
    #Podemos usar idioma "spanish" o cualquier otro.
```

## [19. spacy](#indice)
Es una librería de **Procesamiento del Texto** para Python que permite realizar tareas de **Procesamiento del Lenguaje Natural**.<br>
*pip install spacy*<br><br>

Para trabajar con el idioma ingles es necesario descargar por consola:<br>
*py -m spacy download en*

Para importar spacy:

```python
    import spacy
```
Cargamos un paquete de spacy en lenguaje ingles entrenado con paginas web; tambien podemos usar cualquier otro paquete por Ejm: Textos en español entrenados con noticias ("es_core_news_sm"). A nuestra variable "nlp" (Natural Language Processing):
```python
    nlp = spacy.load("en_core_web_sm") 
    #cargamos el lenguaje ingles ("en_core_web_sm") a nuestra variable.
```
Asignamos un nlp con texto a una variable:
```python
    documento = nlp("TEXTO")
```
Para sacar de un texto sus entidades y tipos:
```python
    for entity in documento.ents:
        print(f"{entity.text}:{entity.label_}")
```
### Propiedades:

* **`.ents`** : Arroja las entidades halladas en el texto. 
* **`.text`** : Arroja el texto  en formato de string con sus comillas ('').

## [20. sklearn](#indice)
**Scikit Learn** Es una librería para **Machine Learning**.<br>
Esta libreria ofrece un catálogo de modelos ya predefinidos en el que podremos configurar sus hiper-parámetros para ajustar más su precición. Para la mayoria de modelos son suficientes lo hiper-parámetros excepto para Redes neuronales el cual podremos usar `tensorflow.keras` para con figurar el modelo de redes a nivel de capas para una mejor personalizacion y precicion.

*pip install sklearn*<br>


### > sklearn.datasets:

Son bases de datos que ya incorpora la librería de `sklearn` y las podemos cargar llamándolas de la propia librería de sklearn. Estos Datasets vienen la data repartida en 2 tomas `.data` y `.target`.

#### load_digits:
Esta librería trabaja con un dataset de reconocimiento óptico de digitos (números) escritos a mano.
Con un pixel de 8x8 y un rango de (0,16) valores diferentes por pixel. Este dataset funciona con arrays de 2 dimensiones.

Para importar esta librería:<br>
```python
    from sklearn.datasets import load_digits
```
Para ver la información de lo que hace el dataset importado:
```python
    digist = load_digits()
    print(digist.DESCR)
```
#### load_iris:

Son un dataset de 3 tipos de flores con 4 tipos de medidas para determinar que tipo de "kind" de flor pertenecen. Vienen con sus respectivas Etiquetas.

```python
    from sklearn.datasets import load_iris
```

##### Propiedades:

* **`.target`** : Son las **etiquetas** con el resultado de lo que debe arrojar el modelo. Posteriormente lo usará para corroboración con lo arrojado por el modelo.
* **`.data`** : Se refiere a toda la lista de **datos en cuestion a aprender**. En este caso son imagenes en formato array (1x64) con las que se ha entrenado el modelo para corroborar con su respectivo target (Etiqueta).
* **`.images`** : Contiene la **data** en arrays de 8x8 **listas para ser representadas** en imagenes.
* **`.shape`** : Mostrara ca cantidad de (filas,columnas), bien sea de un target, data, etc.

#### fetch_california_housing:

Dataset de precios de las casas del estado de California en 1990. Los métodos, y comandos usados en el dataset anterior funcionan para todos los datasets. 

```python
    from sklearn.datasets import fetch_california_housing
```
##### Propiedades:

  * **``dataset.feature_names``** = Muestra el nombre de las columnas del dataset para train.
  * **``dataset.target_names``** = Muestra el nombre de las columnas del target para test.
___

#### from sklearn import preprocessing:

En el caso de trabajar con datos que no sean propios de `sklearn.datasets` es necesario preprocesar los valores de los datos para eliminar o controlar valores atipicos o otorgar a nuestros datos distribuciones lo cual ayudan a que el modelo entrenado no tenga sesgos por datos inapropiados.

Es modulo esta orientado a la estandarización de los datos para una correcta distribucion de los datos, con el fin de posteriormente evaluar si la data posee una distribucion normal. Tener encuenta que el output de todos estos metodos son vectores - arrays de numpy el cual debemos volver a converti a dataframe si asi lo queremos.

  * **``preprocessing.MinMaxScaler()``** : Escala en función del mínimo y máximo. Toma el valor maximo como 1 y el valo minimo como 0 y el resto de valores en proporcion entre 1 y 0. Tener en cuenta que los valores atipicos afectan al resultado de este metodo.
  ```python
    #datos es un df con 2 columnas (ingreso,	carros)
    datos_min_max = preprocessing.MinMaxScaler().fit_transform(datos) #datos = variable de 2 Dimensiones.

    datos_min_max
  
  * **``preprocessing.Normalizer()``** : No se refiere a distribucion normal sino que normaliza en función de la Norma del Vector. Con esto aseguramos de que ningun elemento tenga una magnitud mayor a la que tendria todo el elemento. Por defecto usa la normal L2.<br>
  Itera en cada valor de las columnas sacando la raiz cuadradada de la suma de cada valor al cuadrado. Al igual que `MinMaxScaler()` da como resultado valores positivos.<br>
  Sin embargo `.Normalizer()` no estan utilizado debido a que comprime demasiado los datos dejandolo casi lineal.
  ```python
    #Es necesario pasar las columnas a filas para ejecutar la funcion por ello lo transponemos.
    datos_normalizer = preprocessing.Normalizer().transform(datos.T)
    datos_normalizer = datos_normalizer.T #volvemos a transponer para mantener nuestro formato df.
    # normalizado = X / raíz_cuadrada( X_1^2 + X_2^2 + X_3^2 + ...)
  
  ```
  * **``preprocessing.StandardScaler()``** : Estandariza (desv_std = 1, media = 0). Cambia la distribucion de nuestros datos para que tengas una medio = 0 y desv= 1. Se busca tener este resultado debido a que las tecnicas de aprendizaje de maquina operan con datos que posean **distribucion normal**. Es un homogenizador.<br>
  Este metodo a deferencia de los 2 anteriores si nos dara un rango entre valores negativos y positivos.<br>
  Para lograr esto hay 2 metodos: ``StandardScaler()`` y ``RobustScaler()``, la diferencia radica en que RobustScaler() usa el rango intercuatilico (Q1,Q3), el cual quita los datos atipicos, por lo que hace que nuestros datos esten menos distribuidos por ellos es uno de los mas usado.<br><br>

  Otros de los usos de ``.RobustScaler()`` es usarlo junto a `.MinMaxScaler()` para que una vez teniendo una distribucion normal de nuestros datos, escalarlo de forma 0 a 1.s
  
  ```python
    # Ejemplo con .StandardScaler()
    datos_standard_scaler = preprocessing.StandardScaler().fit_transform(datos)
    # estandarizado = (X - media) / std

    # Ejemplo con .RobustScaler()
    datos_robust_scaler = preprocessing.RobustScaler().fit_transform(datos)
    # estandarizado = (X - rango_intercuartílico) / std

    datos_standard_scaler, datos_robust_scaler


    # Otro ejemplo con .StandardScaler()
    saleprice_scaled = StandardScaler().fit_transform(df[['SalePrice']]) #Transformamos el valor de "SalePrice" a 2D.
    saleprice_scaled #Nos arroja la distribución de los datos ya ordenado de menor (-) a mayor (+).

    low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10] #Nos arroja los 10 valores mas bajos.
    high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:] #Nos arroja los 10 valores mas altos.
    print('outer range (low) of the distribution:')
    print(low_range)
    print('\nouter range (high) of the distribution:')
    print(high_range)
  
  ```
  * **``preprocessing.OrdinalEncoder()``** : Sirve para Categorizar Variables Ordinales las cuales son categorizaciones que estan relacionadas a su proximidad entre ellas como: "Alto, Medio, Bajo". Convertira estos datos situados en el df a [0.0, 1.0, 2.0].<br>
  A diferencia del **OneHotEncoder** no nos categoriza en 0 y 1, es por ello que necesitamos escalarlo bien sea con Max-Min, o cualquier otra tecnica de Escalamiento para evitar ruido en el entrenamiento del modelo.
  ```python
    from sklearn.preprocessing import OrdinalEncoder 

    Categorias_Servicio = ["Alto", "Medio", "Bajo"]
    Categorias_Calidad = ["Malo", "Medio", "Bueno"]

    #Indicamos que categorize bajo los criterios de las listas creadas. 
    codificador = OrdinalEncoder(categories = [Categorias_Servicio, Categorias_Calidad])

    #Podemos Crear un nuevo df o reemplazar el existente aplicando el "fit_transform" al df convirtiendo las categorias en valor numerico.
    df_categorizado = pd.DataFrame(codificador.fit_transform(df), columns=["Servicio", "Calidad"])
  
  ```
  
### > Estimadores (Modelos):
Son los que ejecutan las pruebas a los datasets(train, test) arrojando resultados de predicción.

#### Métodos para los modelos:

* **`model.fit()`** : Carga la data para entrena el modelo en base a la data previamente separa para el entrenamiento (train). Devuelve como resultado una expresión que indica que ha cargado el modelo para entrenar pero aún no lo ha ejecutado.

```python
    model.fit(X=X_train, y=Y_train) #Aprende del Train (1347 datos)
    #Asignamos el dataset train para el entrenamiento.
    #Epochs=1 Indica el numero de veces que debe procesar todo el set de datos.
```
* **`model.fit_transform()`** : Ajusta el X en un espacio incrustado y devuelva esa salida transformada. Usa en los datos de **entrenamiento** para que podamos escalar los datos de entrenamiento y también aprender los parámetros de escala de esos datos. Se usa para aprendizaje no Supervisado.
* **`model.transform()`** : Este método ajusta y transforma los datos de entrada al mismo tiempo y convierte los puntos de datos. Si usamos ajuste y transformación separados cuando necesitamos ambos, disminuirá la eficiencia del modelo, por lo que usamos fit_transform() que hará el trabajo de ambos.
```python
    datos_reducidos = tsne.fit_transform(digits.data)
    #Usa la Data para entrenarse sin el Target.
```
* **`model.predict()`** : Ejecutara la predicción de nuestra data que previamente separamos para esta función

```python
    prediccion = model.predict(X=X_test)
    #predice el Test (450 datos)
```
* **`model.score()`** : Mostrará que tan preciso fue nuestra predicción (X_test) con respecto al resultado que debería haber arrojado.

```python
    print(f'{model.score(X_test,Y_test):.2%}')
    #97.78%, se evaluá los arrays de imágenes (X_test) con sus targets.
```
#### 1.- sklearn.neighbors:

```python
    from sklearn.neighbors import KNeighborsClassifier
```
Esta clase usará el algoritmo de los K números mas cercanos para aprender.
Es necesario guardar la clase en una variable para usar sus métodos.

```python
    knc = KNeighborsClassifier()
```
#### 2.- sklearn.svm:

```python
    from sklearn.svm import SVC
```
El objetivo de un SVC lineal (clasificador de vectores de soporte) es ajustarse a los datos que proporciona, devolviendo un hiperplano de "mejor ajuste" que divide o categoriza sus datos. ... Además de los paquetes de visualización que estamos usando, solo necesitará importar svm de sklearn y numpy para la conversión de matriz.

```python
    svc = SVC(gamma="scale")
```
#### 3.- sklearn.naive_bayes:

```python
    from sklearn.naive_bayes import GaussianNB
```
Un algoritmo Gaussian Naive Bayes es un tipo de "kind" especial de algoritmo NB. Se usa específicamente cuando las características tienen valores continuos. También se supone que todas las características siguen una distribución gaussiana, es decir, una distribución normal

```python
    GNB = GaussianNB()
```
#### 4.- sklearn.linear_model:

  ```python
      from sklearn.linear_model import LinearRegression
  ```
  Regresión lineal con sklearn:

  * **``LinearRegression()``**

    ```python
        model_rl = LinearRegression()
        model_rl.fit(X=X_train, y=y_train) #Previo "train_test_split"

        predice = (lambda x:model_rl.coef_*x+model_rl.intercept_)
    ```
    * Propiedades:
        * **``.coef_``**       : "m"
        * **``.intercept_``**  : "b"
    <br><br>
  * **``ElasticNet()``**
  * **``Lasso()``**
  * **``Ridge()``**

#### 5.- sklearn.manifold :

  ```python
      from sklearn.manifold import TSNE
  ```
T-Distributed Stochastic Neighbor Embedding
```python
    tsne = TSNE(n_components=2, random_state=11)
    datos_reducidos = tsne.fit_transform(digits.data)
```
#### 6.- sklearn.cluster:

```python
        from sklearn.cluster import KMeans
```
Para Aprendizaje de maquina no supervisado:
  * DBSCAN():
  
    ```python
            from sklearn.cluster import DBSCAN
    ```
  * MeanShift():
  
    ```python
            from sklearn.cluster import MeanShift
    ```
  * SpectralClustering():
  
    ```python
            from sklearn.cluster import SpectralClustering
    ```
  * AgglomerativeClustering():
  
    ```python
            from sklearn.cluster import AgglomerativeClustering
    ```
    
#### 7.- sklearn.decomposition:
```python
        from sklearn.decomposition import PCA
```
___
### > Comandos sklearn:

#### from sklearn.model_selection:

* **train_test_split**

    ```python
            from sklearn.model_selection import train_test_split
    ```

  * **`train_test_split()`** : Esta función nos permite dividir un dataset en 2 bloques; uno destinado al aprendizaje (**entrenamiento**) y otro para hacer las **pruebas**. Por defecto esta division le asigna 75% de los datos a  **entrenamiento** y 25% a **pruebas**.

      **Variables:** <br>

      * X_train = Hace referencia a la data en cuestión para el **entrenamiento**
      * X_test  = Hace referencia a la data en cuestión para las  **pruebas** 
      * Y_train = Hace referencia a el target de cada data en cuestión de: **entrenamiento**
      * Y_test  = Hace referencia a el target de cada data en cuestión de: **pruebas**

      ```python
          X_train,X_test,Y_train,Y_test = train_test_split(digist.data,digist.target,random_state=11)
          #(X,y,random_state=11)
          #(dataset,Etiquetas,semilla:11)
      ```
* **KFold**

    ```python
            from sklearn.model_selection import KFold
    ```

    Esta clase Segmenta la data en k partes el cual usará una de las partes para ``test`` y las otras para ``train`` hasta haber terminado de conjugar con cada una de las segmentaciones previamente dada logrando asi aumentar el entrenamiento del modelo para mejorar su precision.

    Cuando haya mucha cantidad de datos o no haya patrones difíciles por descubrir `k` será más pequeño. Este es un método de validación, el modelo final será entrenado con toda la tabla.

    ```python
        kf = KFold(n_splits= 10 , random_state= 11, shuffle= True)
        #n_splits = nº de dobleces (De acuerdo a la repartición de los datos)
        #random_state = nº de semilla
        #shuffle = barajeara los datos antes de ponerlos en los dobleces.
    ```
* **cross_val_score**

    ```python
            from sklearn.model_selection import cross_val_score
    ```
    Validación Cruzada.

    ```python
        puntuacion = cross_val_score(estimator= knc, X=digits.data, y=digits.target,cv=kf)
        #estimator = Modelo de estimador.
        # X = data (train y test)
        # y = targets (train y test)
        # cv = generador de validación cruzada (como se repartirá la data para train y test)
    ```



#### from sklearn.metrics:

* **metrics: Funciones**

  * **``metrics.r2_score()``** : Saca el r2. Esta se mide de 1- a 1 mientras más cercana a 1 indica absoluta precision del modelo en cambio con -1 es inversamente proporcional. Valores cercanos a 0 indica que no hay relación.
  ```python
                      metrics.r2_score(esperado,prediccion)
  ```
  * **``metrics.mean_squared_error()``** : Saca el Mean Squared Error (Promedio de Errores Cuadrados). Mientras más cerca de "0" indica mayor precision del modelo.
  ```python
                    metrics.mean_squared_error(esperado,prediccion)  
  ```
* **confusion_matrix**

    Muestra en un array los datos acertados y erróneos del la prueba.

    ```python
        from sklearn.metrics import confusion_matrix

        confusion = confusion_matrix(y_true=esperado,y_pred=prediccion)
    ```
* **classification_report:**

    Parecido a la matriz de confusion muestra métricas de la comparación de prueba y target.

    - Precision: Verdaderos positivos/ vp + falsos positivos
    - recall (Sensitividad): Verdaderos positivos/ total positivos
    - f1-score : promedio entre Precision y recall.
    - support : Cantidad de valores reales que tenia para cada dígito para el ejemplo.

    ```python
        from sklearn.metrics import classification_report

        nombres = [str(digit) for digit in digits.target_names]
        #Pasamos los targets de formato array a string.
        print(classification_report(esperado,prediccion,target_names= nombres)) #En ese orden (espera,prediccion,targetnames=)
    ```

#### from sklearn.feature_extraction.text:

Esta Librería esta orientada a la extracción y tokenización de palabras de un texto.

* **CountVectorizer:**
  
    ```python
        from sklearn.feature_extraction.text import CountVectorizer
    ```

  Es el constructor que recibe los comandos que indiquemos para la tokenización.

  * **.fit_transform():** Este método extrae y tokeniza las palabras del texto a indicar.

    ```python
        sample_data = ["Hello Word","Hello Hello Word", "Word Word Word"] 

        vectorizer = CountVectorizer()
        x = vectorizer.fit_transform(sample_data)

    ```

  * **.get_feature_names():** Este método mostrara las palabras tokenizadas.

    ```python
        print(vectorizer.get_feature_names())
    ```

## [21. TensorFlow](#indice)
Es una librería para **Depp Learning**.<br>
*pip install tensorflow*<br>

```python
    import tensorflow as tf
```
Para tensorflow las dimensiones de los diferentes tipos de datos son indicados de las siguiente manera:

* **``Scalar``** : Referencia a un solo valor.
* **``Vector``** : Referencia a una lista tupla o Array (columna en df).
* **``Matriz``** : Referencia a una matriz de 2D en numpy o tabla de 2 columnas en pandas. Imagen a blanco y Negro.
* **``Tensor``** : Referencia a una matriz de 3D a más dimensiones en numpy o tabla de n columnas en pandas.
  * **``Tensor 3D``** : Imagen a color (R,G,B)
  * **``Tensor 4D``** : Coleccion de imagenes a color (fps) el cual se usa para representar los videos.
  * **``Tensor 5D``** : Coleccion de videos.
  

### Keras:

Es un FrameWork encima de TensorFlow que ayuda a la creación del modelo de manera mas simplificada. Esto es debido a que el diseño del modelo de la Red lo haremos a nivel de capas ya establecidas como Dense, convolucionales ,etc.

```python
    from tensorflow import keras
```

#### Datasets:

Keras trae sus propios datasets para trabajar y entrenar un modelo:

```python
    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
    #X_train : Contiene 60k matrices de 28x28 representando a los pixeles para entrenar.
    #y_train : Son las Etiquetas 0-9 para cada imagen o matriz de X_train.
    #X_test y y_test : Son los datos de prueba para evaluar el aprendizaje del modelo. 10k
```
#### utils:

Son un paquete de Funciones para el pre-procesamiento, preparacion de datos y graficos.

```python
from tensorflow.keras.utils import to_categorical

```
* **`to_categorical()`** : (One Hot Encoding) Expresa los valores numericos en un array donde cada columna es un "0" hasta contar al numero indicado donde lo indicara como un "1". Para una lista tomara el valor maximo para definir la longitud o tamaño de la lista a generar rellenando de 0 hasta indicar con un 1 cada valor de la lista contando las posiciones del array desde el index 0.<br>
Devuelve una matriz de 2D en donde cada array tendra el valor maximo de la lista hallada, contando desde la posicion 0 hasta el valor maximo hallado. Ejemplo si el valor maximo de nuestra lista es 9 tendremos Arrays de 10 elementos en nuestra matriz. **(nº de elementos, valor maximo de la lista)**
 *  **`,num_classes=`** : Podemos indicarle la longitud del array a devolver para que llene de 0's siempre y cuando indiquemos un nùmero mayor al maximo valor de la lista dada.
 *  **`,dtype=`** : Indicamos el tipo de dato de la lista de entrada. Por defecto pasa los valores a "float32".

```python
    a = to_categorical([1, 3, 4], num_classes=9, dtype=float32)
    print(a)
#   array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],
#          [0., 0., 0., 1., 0., 0., 0., 0., 0.],
#          [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float32)
    # (nº de elementos, valor maximo de la lista ó num_classes siempre y cuando sea mayor al max valor de la lista)

    y_train = to_categorical(y_train) #Indicara de manera categorica todos los valores numericos del Array de 1D.
``` 

##### Tipos de Modelos:

* **`Red Neuronal Convolucional (CNN) `** : 
* **`Red Neuronal Recurrente (RNN) `** : 
* **`Red de Densidad Mixta (MDN) `** : 
* **`Auto Encoder Variacional (VAE) `** : 
* **`Algoritmos Genéticos (GA) `** : 
* **`Estrategias Evolutivas (ES) `** : 

##### Models:

Eligiremos el modelo según el tipo de "kind" de modelo que queramos construir teniendo en cuenta nuestros datos y target. Según el tipo de "kind" de modelo que elijamos se suele guardar dentro de una variable llamada **model**.

* **`model = keras.models.Sequential([])`** : Para armar nuestras capas siempre partimos con `.Sequential`

Métodos para nuestra variable model:

* **`model.summary() `** : Nos arrojara un resumen de todas las capas y tipo de "kind" de capas que hemos ingresado a nuestro `.Sequential` o model.

    ```python
        model.summary()

        # Visualizar Gráficamente en archivo png.

        from tensorflow.keras.utils import plot_model
        from IPython.display import Image

        plot_model(cnn, to_file="covnet.png", show_shapes=True,show_layer_names=True) #Guarda la imagen .png
        Image(filename="covnet.png") #muestra la imagen .png en la notebook.

        #Es necesario descargar e instalar el "graphviz".
    
    ```
* **`model.compile() `** : Compilaremos el modelo indicando el optimizador, función de error y métrica de presición. El tipo de función de error lo definira el tipo de output que tegnamos en el modelo:
  
    ```python
        model.compile(optimizer="adam", #Es el más balanceado de los Optimizadores
                      loss="binary_crossentropy", #Para Clasificacion Binaria
                      metrics=["accuracy"])
        
    ```
* **`model.fit() `** : Tras asignar nuestro `.compile` entrenará el modelo.

    ```python
        history = model.fit(train_features, train_labels, validation_split=0.2, ephocs=100)
        #Entrenaremos un modelo con 100 ciclos.
        #batch_size : Indica el numero de muestras a procesar durante un "epochs" (evita el Overfitting). Indica cada cuantos registros procesados recalculara los pesos hasta terminar una epoca.
        #ephocs= : Desde el incio hasta el final de los registros es una epoca.
        #validation_split : Indica que deje el 20% de los datos para validación
    ```

* **`model.evaluate() `** : Nos arrojara la presición ("accuracy") del modelo entrenado.

    ```python
        loss, accuracy = model.evaluate(train_features, train_labels, verborse=2)
        # verbose = es la elección de cómo desea ver la salida de su Red Neuronal mientras se está entrenando. valores [0,1,2]
    ```

* **`model.save() `** : Guardara nuestro modelo entrenado en un archivo `.h5`

    ```python
        model.save("modelo_1_binario.h5") #El formato es ".h5"
    ```

* **`model.load_model() `** : Cargara nuestro modelo de un archivo `.h5`

    ```python
        from tensorflow.keras.models import load_model

        model = load_model("modelo_1_binario.h5")

    ```



##### Layers:

Son los tipos de capas que van dentro de los modelos

* **`keras.layers.Input(())** : **Capa de Entrada**. En el caso de trabajar con los DataFrames nuestra entrada seria el **nº de columnas** (features) que tendrá en cuenta el modelo para predecir.<br>
**keras.layers.Input((None,nºColumnas_df valor "X",nºfilas_df valor "Y"))**
  
      ```python
        keras.layers.Input((None,18,)) #Dejamos la coma en "Y" indicando que puede tomar todos los valores de la fila.
        
    ```
  
* **`keras.layers.Dense()** : **Capas Ocultas**. Son tipo de capa "Fully-Connected" lo que significa que son capas de coneccion entre la capa anterior y las que indicaremos en esta capa. Aquí podremos elegir el **nº de capas** con el **nº de neuronas** indicando su respectiva **función de activación** por capa (RELU suele ser la más usada).

    ```python
        keras.layers.Dense(64, activation="relu")
        
    ```
 * Con este mismo método indicaremos nuestra capa de salida: Referenciamos nuestro target, 1 neurona por columna a predecir. Por ejemplo si nuestro objetivo era predecir el valor de una columna tendrá 1 sola neurona como resultado y si ese resultado es binario (1=si, 0=no) eligiremos nuestra **función de activación SIGMOIDE **
    
    ```python
        keras.layers.Dense(1, activation="sigmoid") #En caso de una salida binaria.
        
    ```

* **`keras.layers.Dropout()** : **Elimina Neuronas**. Indica el % de neuronas que se eliminaran al saltar de una capa oculta a otra. Mantendrá los valores del % de las neuronas con el fin de restar la probabilidad de Overfitting.

    ```python
        keras.layers.Dense(64, activation="relu")
        keras.layers.Dropout(0.2) #20% de eliminación de neuronas.
        keras.layers.Dense(128, activation="relu") #Por ello aumentamos el nº de neuronas
        
    ```
  

#### Losses:

Hace referencia a la Función de Error a elegir. El cual indicará el comportamiento del modelo, como aprendió el modelo. Sirve para ver si hubo overfeeding o algún otro problema a la hora del aprendizaje.<br>

Esta es la etapa de compilación del modelo por eso usamos el método `.compile`:

```python
model.compile(optimizer="adam", #Es el más balanceado de los Optimizadores
              loss=,
              metrics=["accuracy"])
```

* **``"binary_crossentropy"``** : Para modelos de **Clasificacion Binaria** "0", "1". Dará como resultado 2 posibles valores para una columna ejemplo, sobrevive o no sobrevive.

```python
    model.compile(optimizer="adam",
                  loss="binary_crossentropy",
                  metrics=["accuracy"])
```
* **``"sparse_categorical_crossentropy"``** : Para modelo de **clasificacion Multiclase** "Entropía cruzada categórica con objetivos enteros". Normalmente se usa cuando tenemos más de 2 categorias ejemplo para 1 de 10 posibles Categorias. Dara como resultado un % asignado a cada una de las 10 categorias señalando con la mayor probabilidad la respuesta.
```python
        model.compile(optimizer="adam", 
                      loss="sparse_categorical_crossentropy", 
                      metrics=["sparse_categorical_accuracy"])
```

* **``"mean_square_error"``** : Para modelo de **Regresion Escalar**. Este modelo busca predecir un valor continuo como un precio, temperatura de un dia en particular, etc. La arquitectura son capas Relu con una salida Relu o sin F.Activacion.
```python
        model.compile(optimizer="adam", 
                      loss="mean_square_error", 
                      metrics=["accuracy"])
```

#### Entrenamiento del Modelo:

Guaramos en una variable el entrenamiento del modelo, para ejecutarlo usaremos el método `.fit()`. El objetivo es que aprenda el modelo para que con una tabla de features nueva pueda predecir los valores de la columna target.<br>
Guardamos este entrenamiento del modelo en una variable **history** para luego usar sus métodos `.history.keys()`.
**model.fit(Tabla de las columnas del df a entrenar, Tabla con la columna target, validation_split=0.2, ephocs=100)**

```python
    history = model.fit(train_features, train_labels, validation_split=0.2, ephocs=100)
    #Entrenaremos un modelo con 100 ciclos.
    #batch_size : Indica el numero de muestras a procesar durante un "epochs"
    #validation_split : Indica que deje el 20% de los datos para validación
```
___
### Evaluación del Modelo:

Nos arrojara la presición ("accuracy") del modelo entrenado.

```python
    loss, accuracy = model.evaluate(train_features, train_labels, verborse=2)
    # verbose = es la elección de cómo desea ver la salida de su Red Neuronal mientras se está entrenando. valores [0,1,2]
```

### Gráfica del aprendizaje:

Evaluaremos gráficamente como aprendió nuestro modelo. Verificaremos que el "model accuracy" este lo mas cercano a 1 y en contra parte el "model loss" tiene que estar lo mas cercano a 0. Teniendo como referencia el accuracy que nos arrojo el método ``model.evaluate()``.

El método "fit" nos devuelve un diccionario que guardamos en nuestra variable **"history"** con los resultados de la entrenamiento. El **``"val_loss"``** y el **``"val_accuracy"``** son para los datos de **validación** que hemos indicado con el parámetro de **``validation_split=0.2``**, el cual usa el 20% de los mismos datos para evaluar y arrojarnos el **``loss``** y **``accuracy``**.

Con esto se busca evitar el **overfitting** (Sobre-Entrenamiento) para que el modelo no memorize los resultado, sino aprenda. Esto suele suceder cuando cargamos con demasiados **Ephocs** al entrenamiento. En caso contrario de poner muy poco **Ephocs** tendríamos lo que se conoce como **underfitting**

```python
    history.history.keys() #arroja el nombre de las columnas de la evaluación: (["loss","accuracy","val_loss","val_accuracy"])
    import matplotlib.pyplot as plt

    #Model Accuracy
    plt.plot(history.history["accuracy"])
    plt.plot(history.history["val_accuracy"])
    plt.title("Model Accuracy")
    plt.ylabel("Accuracy")
    plt.xlabel("Epoch")
    plt.legend(["Train","test"], loc="upper left")
    plt.show()

     #Model Loss
    plt.plot(history.history["loss"])
    plt.plot(history.history["val_loss"])
    plt.title("Model Loss")
    plt.ylabel("Loss")
    plt.xlabel("Epoch")
    plt.legend(["Train","test"], loc="upper left")
    plt.show()

```
### Predicción

Tras tener nuestro modelo entrenado podemos cargarle nuevos datos al modelo según los campos de entrenamiento para que nos prediga nuestra columna target.

```python
    predictions = model.predict(test_features) #variable con nuevos valores de los campos de entrenamiento.

```

### Exportar Modelo

Guarda la topologia (arquitectura) del modelo de la red neuronal.

```python
    model_json = model.to_json()  #Guardamos en una variable el modelo en formato "json"
    with open ("fashion_model.json", "w") as json_file:  #Nombramos el archivo json y elegimos modo "w".
      json_file.write(model_json)

```

Exportara los pesos de la red neuronal (Red Weights) entrenado en un archivo `.h5` con el fin de no volver a gastar recursos para volver a entrenar.

```python
    model.save("modelo_1_binario.h5") #El formato es ".h5"
```
Para cargar un modelo:

```python
    from tensorflow.keras.models import load_model

    model = load_model("modelo_1_binario.h5")

```

## [22. camelot](#indice)

Es una libreria para extraer cuadros de los PDF.

**pip install camelot**

En caso falte alguna libreria podemos instalarlar como en el siguiente ejemplo.

**pip install opencv-python** 

```python
import camelot
```

### Metodos

* **``".read_pdf"``** : Carga un archivo PDF a la variable que le asignemos.
  ```python
    table = camelot.read_pdf("archivo.pdf", pages="1", flavor="lattice")
      #flavor= Indica el metodos que usara para extraer las tablas, por defecto usa "lattice"
  ```

* **``".export"``** : Exporta la tabla en la variable que guardamos la tabla.

  ```python
      table.export("archivo.csv", f="csv", compress=True) #Guarda el CSV en la variable
      table[0].to_csv("archivo.csv") #Exportamos a un Archivo csv en nuestro directorio.
  ```